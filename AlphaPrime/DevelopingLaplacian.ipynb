{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddabf73c",
   "metadata": {},
   "source": [
    "# Training a NN for metric on CICY with homog\n",
    "\n",
    "## Import the required packages/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5790c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#disable GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3cde382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/lib/python38.zip',\n",
       " '/usr/lib/python3.8',\n",
       " '/usr/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/home/f/fraser-talientec/cymetricTESTING217/lib/python3.8/site-packages',\n",
       " 'home/f/fraser-talientec/cymetricap/AlphaPrime']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a9cc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-11 20:30:24.719702: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-11 20:30:28.815146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-08-11 20:30:28.815185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: tplxdt127\n",
      "2024-08-11 20:30:28.815196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: tplxdt127\n",
      "2024-08-11 20:30:28.815269: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 555.42.6\n",
      "2024-08-11 20:30:28.815302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 555.42.6\n",
      "2024-08-11 20:30:28.815310: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 555.42.6\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'laplacian_funcs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcymetric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SigmaLoss, KaehlerLoss, TransitionLoss, RicciLoss, VolkLoss, TotalLoss\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#from NewCustomMetrics import *\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlaplacian_funcs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#from generate_and_train_all_nnsHOLO import *\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcustom_networks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'laplacian_funcs'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import pickle\n",
    "import sys\n",
    "#sys.path.append(\"/Users/kit/Documents/Phys_Working/MF metric\")\n",
    "#sys.path.append(\"/home/f/fraser-talientec/PhysicalYukawas\")\n",
    "sys.path.append(\"/home/f/fraser-talientec/cymetricap/AlphaPrime\")\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout)\n",
    "\n",
    "from cymetric.pointgen.pointgen import PointGenerator\n",
    "from cymetric.pointgen.nphelper import prepare_dataset, prepare_basis_pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "from cymetric.models.tfmodels import PhiFSModel, MultFSModel, FreeModel, MatrixFSModel, AddFSModel, PhiFSModelToric, MatrixFSModelToric\n",
    "from cymetric.models.tfhelper import prepare_tf_basis, train_model\n",
    "from cymetric.models.callbacks import SigmaCallback, KaehlerCallback, TransitionCallback, RicciCallback, VolkCallback, AlphaCallback\n",
    "from cymetric.models.metrics import SigmaLoss, KaehlerLoss, TransitionLoss, RicciLoss, VolkLoss, TotalLoss\n",
    "\n",
    "#from NewCustomMetrics import *\n",
    "from laplacian_funcs import *\n",
    "#from generate_and_train_all_nnsHOLO import *\n",
    "from custom_networks import *\n",
    "import sys\n",
    "import importlib\n",
    "from AlphaPrimeModel import *\n",
    "#reimport ALphaPrimeModel\n",
    "importlib.reload(sys.modules['AlphaPrimeModel'])\n",
    "from AlphaPrimeModel import *\n",
    "#reimport custom_networks\n",
    "importlib.reload(sys.modules['custom_networks'])\n",
    "from custom_networks import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d21c24a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'AlphaPrime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mAlphaPrime\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'AlphaPrime'"
     ]
    }
   ],
   "source": [
    "import AlphaPrime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d00ecc1-f6f4-47d4-a2bb-f7830e164093",
   "metadata": {},
   "source": [
    "## Point Cloud Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ba80cc",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d1dd93a",
   "metadata": {},
   "source": [
    "Set the properties of the defining polynomial. And the point in Kahler Moduli space\n",
    "\n",
    "If correct, this should be for the following defining polynomial\n",
    "$0.44 x_{1,0}^2 x_{3,0}^2 x_{4,0}^2 x_{2,0}^2+0.88 x_{1,1}^2 x_{3,0}^2 x_{4,0}^2 x_{2,0}^2+0.88 x_{1,0}^2 x_{3,1}^2 x_{4,0}^2 x_{2,0}^2+0.88 x_{1,1}^2 x_{3,1}^2 x_{4,0}^2 x_{2,0}^2-0.03 x_{1,0} x_{1,1} x_{3,0} x_{3,1} x_{4,0}^2 x_{2,0}^2+0.44 x_{1,0}^2 x_{3,0}^2 x_{4,1}^2 x_{2,0}^2+0.44 x_{1,1}^2 x_{3,0}^2 x_{4,1}^2 x_{2,0}^2+0.88 x_{1,0}^2 x_{3,1}^2 x_{4,1}^2 x_{2,0}^2+0.44 x_{1,1}^2 x_{3,1}^2 x_{4,1}^2 x_{2,0}^2-0.41 x_{1,0} x_{1,1} x_{3,0} x_{3,1} x_{4,1}^2 x_{2,0}^2-0.41 x_{1,0} x_{1,1} x_{3,0}^2 x_{4,0} x_{4,1} x_{2,0}^2-0.03 x_{1,0} x_{1,1} x_{3,1}^2 x_{4,0} x_{4,1} x_{2,0}^2+0.62 x_{1,0}^2 x_{3,0} x_{3,1} x_{4,0} x_{4,1} x_{2,0}^2+0.62 x_{1,1}^2 x_{3,0} x_{3,1} x_{4,0} x_{4,1} x_{2,0}^2-0.62 x_{1,0} x_{1,1} x_{2,1} x_{3,0}^2 x_{4,0}^2 x_{2,0}-0.62 x_{1,0} x_{1,1} x_{2,1} x_{3,1}^2 x_{4,0}^2 x_{2,0}+0.41 x_{1,0}^2 x_{2,1} x_{3,0} x_{3,1} x_{4,0}^2 x_{2,0}+0.03 x_{1,1}^2 x_{2,1} x_{3,0} x_{3,1} x_{4,0}^2 x_{2,0}-0.62 x_{1,0} x_{1,1} x_{2,1} x_{3,0}^2 x_{4,1}^2 x_{2,0}-0.62 x_{1,0} x_{1,1} x_{2,1} x_{3,1}^2 x_{4,1}^2 x_{2,0}+0.03 x_{1,0}^2 x_{2,1} x_{3,0} x_{3,1} x_{4,1}^2 x_{2,0}+0.41 x_{1,1}^2 x_{2,1} x_{3,0} x_{3,1} x_{4,1}^2 x_{2,0}+0.41 x_{1,0}^2 x_{2,1} x_{3,0}^2 x_{4,0} x_{4,1} x_{2,0}+0.03 x_{1,1}^2 x_{2,1} x_{3,0}^2 x_{4,0} x_{4,1} x_{2,0}+0.03 x_{1,0}^2 x_{2,1} x_{3,1}^2 x_{4,0} x_{4,1} x_{2,0}+0.41 x_{1,1}^2 x_{2,1} x_{3,1}^2 x_{4,0} x_{4,1} x_{2,0}+0.9 x_{1,0} x_{1,1} x_{2,1} x_{3,0} x_{3,1} x_{4,0} x_{4,1} x_{2,0}+0.44 x_{1,0}^2 x_{2,1}^2 x_{3,0}^2 x_{4,0}^2+0.88 x_{1,1}^2 x_{2,1}^2 x_{3,0}^2 x_{4,0}^2+0.44 x_{1,0}^2 x_{2,1}^2 x_{3,1}^2 x_{4,0}^2+0.44 x_{1,1}^2 x_{2,1}^2 x_{3,1}^2 x_{4,0}^2-0.41 x_{1,0} x_{1,1} x_{2,1}^2 x_{3,0} x_{3,1} x_{4,0}^2+0.88 x_{1,0}^2 x_{2,1}^2 x_{3,0}^2 x_{4,1}^2+0.88 x_{1,1}^2 x_{2,1}^2 x_{3,0}^2 x_{4,1}^2+0.88 x_{1,0}^2 x_{2,1}^2 x_{3,1}^2 x_{4,1}^2+0.44 x_{1,1}^2 x_{2,1}^2 x_{3,1}^2 x_{4,1}^2-0.03 x_{1,0} x_{1,1} x_{2,1}^2 x_{3,0} x_{3,1} x_{4,1}^2-0.03 x_{1,0} x_{1,1} x_{2,1}^2 x_{3,0}^2 x_{4,0} x_{4,1}-0.41 x_{1,0} x_{1,1} x_{2,1}^2 x_{3,1}^2 x_{4,0} x_{4,1}+0.62 x_{1,0}^2 x_{2,1}^2 x_{3,0} x_{3,1} x_{4,0} x_{4,1}+0.62 x_{1,1}^2 x_{2,1}^2 x_{3,0} x_{3,1} x_{4,0} x_{4,1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe185432",
   "metadata": {},
   "outputs": [],
   "source": [
    "monomialsTQ = 5*np.eye(5, dtype=np.int64)\n",
    "coefficientsTQ = np.ones(5)\n",
    "kmoduliTQ = np.ones(1)\n",
    "ambientTQ = np.array([4])\n",
    "nameofmanifold=\"Quintic\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_points_and_save_using_defaults(free_coefficient,number_points,force_generate=False,seed_set=0):\n",
    "   coefficients=coefficientsTQ\n",
    "   # coefficients=np.array([1, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, \\\n",
    "   # 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, free_coefficient, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 1, 0, 2, 0, 0, 0, 2, 0, 1])\n",
    "   monomials=monomialsTQ\n",
    "   kmoduli=kmoduliTQ\n",
    "   ambient=ambientTQ\n",
    "   # monomials = 5*np.eye(5, dtype=np.int64)\n",
    "   # coefficients = np.ones(5)\n",
    "   \n",
    "   # kmoduli = np.ones(1)\n",
    "   # ambient = np.array([4])\n",
    "   pg = PointGenerator(monomials, coefficients, kmoduli, ambient)\n",
    "   pg._set_seed(seed_set)\n",
    "   dirname = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   print(\"dirname: \" + dirname)\n",
    "   #test if the directory exists, if not, create it\n",
    "   if force_generate or (not os.path.exists(dirname)):\n",
    "      print(\"Generating: forced? \" + str(force_generate))\n",
    "      kappa = pg.prepare_dataset(number_points, dirname)\n",
    "      pg.prepare_basis(dirname, kappa=kappa)\n",
    "   elif os.path.exists(dirname):\n",
    "      try:\n",
    "         print(\"loading prexisting dataset\")\n",
    "         data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "         if (len(data['X_train'])+len(data['X_val']))!=number_points:\n",
    "            print(\"wrong length - generating anyway\")\n",
    "            kappa = pg.prepare_dataset(number_points, dirname)\n",
    "            pg.prepare_basis(dirname, kappa=kappa)\n",
    "      except:\n",
    "         print(\"error loading - generating anyway\")\n",
    "         kappa = pg.prepare_dataset(number_points, dirname)\n",
    "         pg.prepare_basis(dirname, kappa=kappa)\n",
    "   \n",
    "\n",
    "def getcallbacksandmetrics(data):\n",
    "   #rcb = RicciCallback((data['X_val'], data['y_val']), data['val_pullbacks'])\n",
    "   scb = SigmaCallback((data['X_val'], data['y_val']))\n",
    "   volkcb = VolkCallback((data['X_val'], data['y_val']))\n",
    "   kcb = KaehlerCallback((data['X_val'], data['y_val']))\n",
    "   tcb = TransitionCallback((data['X_val'], data['y_val']))\n",
    "   #cb_list = [rcb, scb, kcb, tcb, volkcb]\n",
    "   cb_list = [ scb, kcb, tcb, volkcb]\n",
    "   cmetrics = [TotalLoss(), SigmaLoss(), KaehlerLoss(), TransitionLoss(), VolkLoss()]#, RicciLoss()]\n",
    "   return cb_list, cmetrics\n",
    "\n",
    "def train_and_save_nn(free_coefficient,nlayer=3,nHidden=128,nEpochs=50,stddev=0.1,bSizes=[192,50000],lRate=0.001,use_zero_network=False):\n",
    "   dirname = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   name = 'phimodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(bSizes[1]) + 's' + str(nlayer) + 'x' +str(nHidden)\n",
    "   print('dirname: ' + dirname)\n",
    "   print('name: ' + name)\n",
    "   \n",
    "   data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirname, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetrics(data)\n",
    "\n",
    "   act = 'gelu'\n",
    "   alpha = [1., 1., 30., 1., 2.] # 1 AND 3??\n",
    "   ambient=tf.cast(tf.math.abs(BASIS['AMBIENT']),tf.int32)\n",
    "\n",
    "   #nfirstlayer=tf.reduce_prod(2*(np.array(ambient)+1)).numpy().item()\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   #nn_phi = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_phi=make_nn(10,1,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_phi_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   # print(nn_phi_zero(tf.cast(data['X_val'][0:2],tf.float32)))\n",
    "   nn_phi_zero=make_nn(10,1,nlayer,nHidden,act,use_zero_network=True)\n",
    "   phimodel = PhiFSModel(nn_phi, BASIS, alpha=alpha)\n",
    "   phimodelzero = PhiFSModel(nn_phi_zero, BASIS, alpha=alpha)\n",
    "\n",
    "   #Note, currently running legacy due to ongoing tf issue with M1/M2. \n",
    "   #Use the commented line instead if not on an M1/M2 machine\n",
    "   opt = tfk.optimizers.Adam(learning_rate=lRate)\n",
    "   #opt = tfk.optimizers.legacy.Adam(learning_rate=lRate)\n",
    "   # compile so we can test on validation set before training\n",
    "   phimodel.compile(custom_metrics=cmetrics)\n",
    "   phimodelzero.compile(custom_metrics=cmetrics)\n",
    "\n",
    "   ## compare validation loss before training for zero network and nonzero network\n",
    "   datacasted=[tf.cast(data['X_val'],tf.float32),tf.cast(data['y_val'],tf.float32)]\n",
    "   #need to re-enable learning, in case there's been a problem:\n",
    "   phimodel.learn_transition = False\n",
    "   phimodelzero.learn_transition = False\n",
    "   phimodel.learn_volk = True\n",
    "   phimodelzero.learn_volk = True\n",
    "   #phimodel.learn_ricci_val= True\n",
    "   #phimodelzero.learn_ricci_val= True\n",
    "   valzero=phimodelzero.test_step(datacasted)\n",
    "   valraw=phimodel.test_step(datacasted)\n",
    "   # phimodel.learn_ricci_val=False \n",
    "   # phimodelzero.learn_ricci_val=False \n",
    "   valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   valraw = {key: value.numpy() for key, value in valraw.items()}\n",
    "\n",
    "   phimodel, training_history = train_model(phimodel, data, optimizer=opt, epochs=nEpochs, batch_sizes=bSizes, \n",
    "                                       verbose=1, custom_metrics=cmetrics, callbacks=cb_list)\n",
    "   return phimodel,training_history\n",
    "   print(\"finished training\\n\")\n",
    "   phimodel.model.save(os.path.join(dirname, name))\n",
    "   np.savez_compressed(os.path.join(dirname, 'trainingHistory-' + name),training_history)\n",
    "   #now print the initial losses and final losses for each metric\n",
    "   # first_metrics = {key: value[0] for key, value in training_history.items()}\n",
    "   # lastometrics = {key: value[-1] for key, value in training_history.items()}\n",
    "   phimodel.learn_transition = True\n",
    "   phimodel.learn_volk = True\n",
    "   #phimodel.learn_ricci_val= True\n",
    "   valfinal=phimodel.test_step(datacasted)\n",
    "   valfinal = {key: value.numpy() for key, value in valfinal.items()}\n",
    "   #phimodel.learn_ricci_val=False \n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for raw network: \")\n",
    "   print(valraw)\n",
    "   print(\"validation loss for final network: \")\n",
    "   print(valfinal)\n",
    "   print(\"ratio of final to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valfinal.items()}))\n",
    "   print(\"ratio of final to raw: \" + str({key + \" ratio\": value/(valraw[key]+1e-8) for key, value in valfinal.items()}))\n",
    "\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(phimodel,tf.cast(data[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   #IMPLEMENT THE FOLLOWING\n",
    "   #meanfailuretosolveequation,_,_=measure_laplacian_failure(phimodel,data)\n",
    "   print(\"\\n\\n\")\n",
    "   return phimodel,training_history\n",
    "\n",
    "def load_nn_phimodel(free_coefficient,nlayer=3,nHidden=128,nEpochs=50,bSizes=[192,50000],stddev=0.1,lRate=0.001,set_weights_to_zero=False):\n",
    "   dirname = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   name = 'phimodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(bSizes[1]) + 's' + str(nlayer) + 'x' +str(nHidden)\n",
    "   print(dirname)\n",
    "   print(name)\n",
    "   \n",
    "   data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirname, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetrics(data)\n",
    "\n",
    "\n",
    "   act = 'gelu'\n",
    "\n",
    "   alpha = [1., 1., 30., 1., 2.] # 1 AND 3??\n",
    " \n",
    "\n",
    "\n",
    "   ambient=tf.cast(BASIS['AMBIENT'],tf.int32)\n",
    "\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   nn_phi = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_phi_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "\n",
    "\n",
    "#    nn_phi = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "#    nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "   phimodel = PhiFSModel(nn_phi, BASIS, alpha=alpha)\n",
    "   phimodelzero = PhiFSModel(nn_phi_zero, BASIS, alpha=alpha)\n",
    "\n",
    "   if set_weights_to_zero:\n",
    "      training_history=0\n",
    "   else:\n",
    "      phimodel.model=tf.keras.models.load_model(os.path.join(dirname,name))\n",
    "      training_history=np.load(os.path.join(dirname, 'trainingHistory-' + name +'.npz'),allow_pickle=True)['arr_0'].item()\n",
    "\n",
    "   phimodel.compile(custom_metrics=cmetrics)\n",
    "   phimodelzero.compile(custom_metrics=cmetrics)\n",
    "\n",
    "   # compare validation loss before training for zero network and nonzero network\n",
    "   datacasted=[tf.cast(data['X_val'],tf.float32),tf.cast(data['y_val'],tf.float32)]\n",
    "   #need to re-enable learning, in case there's been a problem:\n",
    "   phimodel.learn_transition = True\n",
    "   phimodelzero.learn_transition = True\n",
    "   phimodel.learn_volk = True\n",
    "   phimodelzero.learn_volk = True\n",
    "   #phimodel.learn_ricci_val= True\n",
    "   #phimodelzero.learn_ricci_val= True\n",
    "   valzero=phimodelzero.evaluate(datacasted[0],datacasted[1],return_dict=True)\n",
    "   valtrained=phimodel.evaluate(datacasted[0],datacasted[1],return_dict=True)\n",
    "   #metricsnames=phimodel.metrics_names\n",
    "   # phimodel.learn_ricci_val=False \n",
    "   # phimodelzero.learn_ricci_val=False \n",
    "   #valzero = {metricsnames[i]: valzero[i] for i in range(len(valzero))}\n",
    "   #valtrained= {metricsnames[i]: valtrained[i] for i in range(len(valtrained))}\n",
    "\n",
    "   #valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   #valtrained = {key: value.numpy() for key, value in valtrained.items()}\n",
    "\n",
    "   #valtrained = {key: value.numpy() for key, value in valtrained.items()}\n",
    "\n",
    "   phimodel.learn_transition = True\n",
    "   phimodel.learn_volk = True\n",
    "\n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for final network: \")\n",
    "   print(valtrained)\n",
    "   print(\"ratio of trained to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valtrained.items()}))\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(phimodel,tf.cast(data[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   print(\"\\n\\n\")\n",
    "   #IMPLEMENT THE FOLLOWING\n",
    "   #meanfailuretosolveequation,_,_=measure_laplacian_failure(phimodel,data)\n",
    "   #print(\"\\n\\n\")\n",
    "   #print(\"mean of difference/mean of absolute value of source, weighted by sqrt(g): \" + str(meanfailuretosolveequation))\n",
    "   return phimodel,training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "848e584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_points_and_save_using_defaultsAlpha(free_coefficient,phimodel,euler_char,force_generate=False,seed_set=0):\n",
    "   coefficients=coefficientsTQ\n",
    "   # coefficients=np.array([1, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, \\\n",
    "   # 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, free_coefficient, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 1, 0, 2, 0, 0, 0, 2, 0, 1])\n",
    "   monomials=monomialsTQ\n",
    "   kmoduli=kmoduliTQ\n",
    "   ambient=ambientTQ\n",
    "   # monomials = 5*np.eye(5, dtype=np.int64)\n",
    "   # coefficients = np.ones(5)\n",
    "   \n",
    "   # kmoduli = np.ones(1)\n",
    "   # ambient = np.array([4])\n",
    "   pg = PointGenerator(monomials, coefficients, kmoduli, ambient)\n",
    "   pg._set_seed(seed_set)\n",
    "\n",
    "\n",
    "   dirnameForMetric = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   dirnameAlpha= 'dataAlphaP/'+nameofmanifold+ \"Alpha_pg_with_\" + str(free_coefficient) \n",
    "   #dirnameAlpha = 'dataAlphaP/tetraquadricAlpha_pg_with_'+str(free_coefficient)+'forLB_'+lbstring\n",
    "   #dirnameForMetric = 'dataAlphaP/tetraquadric_pg_with_'+str(free_coefficient)\n",
    "   print(\"dirname for alpha: \" + dirnameForMetric)\n",
    "   print(\"dirname for alpha: \" + dirnameAlpha)\n",
    "\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirnameForMetric, 'basis.pickle'), allow_pickle=True))\n",
    "   \n",
    "   data=np.load(os.path.join(dirnameForMetric, 'dataset.npz'))\n",
    "\n",
    "   if force_generate or (not os.path.exists(dirnameAlpha)):\n",
    "      print(\"Generating: forced? \" + str(force_generate))\n",
    "      kappaAlpha = prepare_dataset_Alpha(pg,data,dirnameAlpha,phimodel,euler_char,BASIS,normalize_to_vol_j=True);\n",
    "   elif os.path.exists(dirnameAlpha):\n",
    "      try:\n",
    "         print(\"loading prexisting dataset\")\n",
    "         data = np.load(os.path.join(dirnameAlpha, 'dataset.npz'))\n",
    "      except:\n",
    "         print(\"problem loading data - generating anyway\")\n",
    "         kappaAlpha = prepare_dataset_Alpha(pg,data, dirnameAlpha,phimodel,euler_char,BASIS,normalize_to_vol_j=True);\n",
    "      \n",
    "   \n",
    "\n",
    "def getcallbacksandmetricsAlpha(dataalpha):\n",
    "   dataalpha_val_dict=dict(list(dict(dataalpha).items())[len(dict(dataalpha))//2:])\n",
    "   tcb = TransitionCallback((dataalpha['X_val'], dataalpha['y_val']))\n",
    "   lplcb = LaplacianCallback(dataalpha_val_dict)\n",
    "   # lplcb = LaplacianCallback(data_val)\n",
    "   cb_list = [lplcb,tcb]\n",
    "   cmetrics = [TotalLoss(), LaplacianLoss(), TransitionLoss()]\n",
    "   return cb_list, cmetrics\n",
    "\n",
    "   \n",
    "def train_and_save_nn_Alpha(free_coefficient,phimodel,euler_char,alphaprime,nlayer=3,nHidden=128,nEpochs=30,bSizes=[192,50000],stddev=0.1,lRate=0.001,alpha=[1,1],load_network=False,use_zero_network=False):\n",
    "   \n",
    "   dirnameForMetric = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   dirnameAlpha= 'dataAlphaP/'+nameofmanifold+ \"Alpha_pg_with_\" + str(free_coefficient) \n",
    "\n",
    "   #data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirnameForMetric, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "\n",
    "   dataalpha = np.load(os.path.join(dirnameAlpha, 'dataset.npz'))\n",
    "   dataalpha_train=tf.data.Dataset.from_tensor_slices(dict(list(dict(dataalpha).items())[:len(dict(dataalpha))//2]))\n",
    "   dataalpha_val_dict=dict(list(dict(dataalpha).items())[len(dict(dataalpha))//2:])\n",
    "   dataalpha_val=tf.data.Dataset.from_tensor_slices(dataalpha_val_dict)\n",
    "   # batch_sizes=[64,10000]\n",
    "   dataalpha_train=dataalpha_train.shuffle(buffer_size=1024).batch(bSizes[0])\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetricsAlpha(dataalpha)\n",
    "\n",
    "   #nlayer = 3\n",
    "   #nHidden = 128\n",
    "   act = 'gelu'\n",
    "   #nEpochs = 30\n",
    "   #bSizes = [192, 150000]\n",
    "   #alpha = [1., 1.] # 1 AND 3??\n",
    "   nfold = 3\n",
    "   n_in = 2*8\n",
    "   n_out = 1\n",
    "   #lRate = 0.001\n",
    "   name = 'alphamodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(nlayer) + 'x' +str(nHidden)\n",
    "   print(\"name: \" + name)\n",
    "\n",
    "   ambient=tf.cast(BASIS['AMBIENT'],tf.int32)\n",
    "\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   #nfirstlayer=tf.reduce_sum(((np.array(ambient)+1)**2)).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=stddev)\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   activ=tf.square\n",
    "   #activ=tfk.activations.gelu\n",
    "   #nn_alpha = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #copie from phi above\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "\n",
    "   #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.2)\n",
    "   #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)#note we don't need a last bias (flat direction)\n",
    "   #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network,kernel_initializer=initializer)#note we don't need a last bias (flat direction)\n",
    "   #nn_alpha_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)#note we don't need a last bias (flat direction)\n",
    "   \n",
    "   alphamodel= AlphaPrimeModel(nn_alpha,BASIS, phimodel,alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "   if load_network:\n",
    "      print(\"loading network\")\n",
    "      alphamodel.model=tf.keras.models.load_model(os.path.join(dirnameAlpha,name))\n",
    "      print(\"network loaded\")\n",
    "\n",
    "   alphamodelzero= AlphaPrimeModel(nn_alpha_zero,BASIS,phimodel, alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "\n",
    "   #Note, currently running legacy due to ongoing tf issue with M1/M2. \n",
    "   #Use the commented line instead if not on an M1/M2 machine\n",
    "   opt = tfk.optimizers.Adam(learning_rate=lRate)\n",
    "   #opt = tfk.optimizers.legacy.Adam(learning_rate=lRate)\n",
    "   # compile so we can test on validation set before training\n",
    "   alphamodel.compile(custom_metrics=cmetrics)\n",
    "   alphamodelzero.compile(custom_metrics=cmetrics)\n",
    "   \n",
    "   #datacasted=[tf.cast(data['X_val'],tf.float32),tf.cast(data['y_val'],tf.float32)]\n",
    "   valzero=alphamodelzero.test_step(dataalpha_val_dict)\n",
    "   valraw=alphamodel.test_step(dataalpha_val_dict)\n",
    "   valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   valraw = {key: value.numpy() for key, value in valraw.items()}\n",
    "   \n",
    "   training_historyAlpha={'transition_loss': [10**(-8)],'laplacian_loss': [1000000000000000]}\n",
    "   i=0\n",
    "   newLR=lRate\n",
    "   #while (training_historyAlpha['transition_loss'][-1]<10**(-5)) or (training_historyAlpha['laplacian_loss'][-1]>1.):\n",
    "   # continue looping if >10 or is nan\n",
    "   while i==0:#(training_historyAlpha['laplacian_loss'][-1]>10000000000000.) or (np.isnan( training_historyAlpha['laplacian_loss'][-1])):\n",
    "      print(\"trying iteration of training \"+str(i))\n",
    "      if i >0:\n",
    "\n",
    "         print('trying again laplacian_loss too big')\n",
    "         #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.2)\n",
    "         #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network,kernel_initializer=initializer)#note we don't need a last bias (flat direction)\n",
    "         #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         #nn_alpha = BiholoModelFuncGENERALforAlphainv2(shapeofnetwork,BASIS,activation=tfk.activations.gelu,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         #nn_alpha = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=False)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)#note we don't need a last bias (flat direction)\n",
    "         if newLR>0.0002:\n",
    "             newLR=newLR/2\n",
    "             print(\"new LR \" + str(newLR))\n",
    "         opt = tfk.optimizers.legacy.Adam(learning_rate=newLR)\n",
    "         alphamodel= AlphaPrimeModel(nn_alpha,BASIS, phimodel,alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "         cb_list, cmetrics = getcallbacksandmetricsAlpha(dataalpha)\n",
    "         alphamodel.compile(custom_metrics=cmetrics)\n",
    "      alphamodel, training_historyAlpha= train_modelalpha(alphamodel, dataalpha_train, optimizer=opt, epochs=nEpochs, batch_sizes=bSizes, \n",
    "                                        verbose=1, custom_metrics=cmetrics, callbacks=cb_list)\n",
    "      i+=1\n",
    "   print(\"finished training\\n\")\n",
    "   alphamodel.model.save(os.path.join(dirnameAlpha, name))\n",
    "   np.savez_compressed(os.path.join(dirnameAlpha, 'trainingHistory-' + name),training_historyAlpha)\n",
    "   valfinal =alphamodel.test_step(dataalpha_val_dict)\n",
    "   valfinal = {key: value.numpy() for key, value in valfinal.items()}\n",
    "   #return training_historyAlpha\n",
    "   #now print the initial losses and final losses for each metric, by taking the first element of each key in the dictionary\n",
    "   #first_metrics = {key: value[0] for key, value in training_historyAlpha.items()}\n",
    "   #last_metrics = {key: value[-1] for key, value in training_historyAlpha.items()}\n",
    "\n",
    "   #print(\"initial losses\")\n",
    "   #print(first_metrics)\n",
    "   #print(\"final losses\")\n",
    "   #print(last_metrics)\n",
    "\n",
    "\n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for raw network: \")\n",
    "   print(valraw)\n",
    "   print(\"validation loss for final network: \")\n",
    "   print(valfinal)\n",
    "   print(\"ratio of final to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valfinal.items()}))\n",
    "   print(\"ratio of final to raw: \" + str({key + \" ratio\": value/(valraw[key]+1e-8) for key, value in valfinal.items()}))\n",
    "\n",
    "\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(alphamodel,tf.cast(dataalpha[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   meanfailuretosolveequation,_,_=HYM_measure_val(alphamodel,dataalpha)\n",
    "   print(\"mean of difference/mean of absolute value of source, weighted by sqrt(g): \" + str(meanfailuretosolveequation))\n",
    "   print(\"\\n\\n\")\n",
    "   tf.keras.backend.clear_session()\n",
    "   return alphamodel,training_historyAlpha\n",
    "\n",
    "def load_nn_Alpha(free_coefficient,phimodel,euler_char,alphaprime,nlayer=3,nHidden=128,nEpochs=30,bSizes=[192,50000],stddev=0.1,lRate=0.001,alpha=[1,1],set_weights_to_zero=False):\n",
    "   dirnameForMetric = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   dirnameAlpha= 'dataAlphaP/'+nameofmanifold+ \"Alpha_pg_with_\" + str(free_coefficient) \n",
    "\n",
    "   name = 'alphamodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(nlayer) + 'x' +str(nHidden)\n",
    "   print(\"name: \" + name)\n",
    "\n",
    "   #data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirnameForMetric, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "\n",
    "   dataalpha = np.load(os.path.join(dirnameAlpha, 'dataset.npz'))\n",
    "   dataalpha_train=tf.data.Dataset.from_tensor_slices(dict(list(dict(dataalpha).items())[:len(dict(dataalpha))//2]))\n",
    "   dataalpha_val_dict=dict(list(dict(dataalpha).items())[len(dict(dataalpha))//2:])\n",
    "   dataalpha_val=tf.data.Dataset.from_tensor_slices(dataalpha_val_dict)\n",
    "   # batch_sizes=[64,10000]\n",
    "   dataalpha_train=dataalpha_train.shuffle(buffer_size=1024).batch(bSizes[0])\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetricsAlpha(dataalpha)\n",
    "\n",
    "   #nlayer = 3\n",
    "   #nHidden = 128\n",
    "   act = 'gelu'\n",
    "   #nEpochs = 30\n",
    "   #bSizes = [192, 150000]\n",
    "   alpha = [1., 1.] # 1 AND 3??\n",
    "   nfold = 3\n",
    "   n_in = 2*8\n",
    "   n_out = 1\n",
    "   #lRate = 0.001\n",
    "   ambient=tf.cast(BASIS['AMBIENT'],tf.int32)\n",
    "\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=stddev)\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   activ=tf.square\n",
    "   nn_alpha = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,stddev=stddev,use_zero_network=set_weights_to_zero)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_alpha_zero = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #copie from phi above\n",
    "   #nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "   \n",
    "   #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)#note we don't need a last bias (flat direction)\n",
    "   #nn_alpha_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)#note we don't need a last bias (flat direction)\n",
    "   alphamodel= AlphaPrimeModel(nn_alpha,BASIS, phimodel,alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "   alphamodelzero= AlphaPrimeModel(nn_alpha_zero,BASIS,phimodel, alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "\n",
    "   alphamodel(dataalpha['X_train'][0:1])\n",
    "   alphamodelzero(dataalpha['X_train'][0:1])\n",
    "\n",
    "   if set_weights_to_zero:\n",
    "      training_historyAlpha=0\n",
    "   else:\n",
    "      alphamodel.model=tf.keras.models.load_model(os.path.join(dirnameAlpha,name))\n",
    "      training_historyAlpha=np.load(os.path.join(dirnameAlpha, 'trainingHistory-' + name +'.npz'),allow_pickle=True)['arr_0'].item()\n",
    "\n",
    "   alphamodel.compile(custom_metrics=cmetrics)\n",
    "   alphamodelzero.compile(custom_metrics=cmetrics)\n",
    "   \n",
    "   valzero=alphamodelzero.evaluate(dataalpha_val_dict,return_dict=True)\n",
    "   valtrained=alphamodel.evaluate(dataalpha_val_dict,return_dict=True)\n",
    "   #valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   #valtrained= {key: value.numpy() for key, value in valtrained.items()}\n",
    "\n",
    "\n",
    "   #metricsnames=alphamodel.metrics_names\n",
    "\n",
    "   #valtrained= {metricsnames[i]: valtrained[i] for i in range(len(valtrained))}\n",
    "   ##valzero = {metricsnames[i]: valzero[i] for i in range(len(valzero))}\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for trained network: \")\n",
    "   print(valtrained)\n",
    "   print(\"ratio of trained to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valtrained.items()}))\n",
    "\n",
    "\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(alphamodel,tf.cast(dataalpha[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   meanfailuretosolveequation,_,_=HYM_measure_val(alphamodel,dataalpha)\n",
    "   print(\"mean of difference/mean of absolute value of source, weighted by sqrt(g): \" + str(meanfailuretosolveequation))\n",
    "   print(\"\\n\\n\")\n",
    "   return alphamodel,training_historyAlpha\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb10fbce",
   "metadata": {},
   "source": [
    "Now generate example points with a point generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a2a6fe4",
   "metadata": {},
   "source": [
    "Geneate the point cloud for our NN training - note that this will take a few mins\n",
    "\n",
    "\n",
    "Note that \"free_coefficient\" is just a label for this particular quintic - for the TQ it was psi. Here, it just lets you have different runs not overwrite each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a21f4a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sympy' has no attribute 'utilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m widthPhi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m\u001b[38;5;66;03m#128 4 in the 1.0s\u001b[39;00m\n\u001b[1;32m     21\u001b[0m train_phi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mgenerate_points_and_save_using_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfree_coefficient\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnPoints\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 32\u001b[0m, in \u001b[0;36mgenerate_points_and_save_using_defaults\u001b[0;34m(free_coefficient, number_points, force_generate, seed_set)\u001b[0m\n\u001b[1;32m     30\u001b[0m kmoduli\u001b[38;5;241m=\u001b[39mkmoduliTQ\n\u001b[1;32m     31\u001b[0m ambient\u001b[38;5;241m=\u001b[39mambientTQ\n\u001b[0;32m---> 32\u001b[0m pg \u001b[38;5;241m=\u001b[39m \u001b[43mPointGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonomials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoefficients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkmoduli\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m pg\u001b[38;5;241m.\u001b[39m_set_seed(seed_set)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#dirname = 'data/tetraquadric_pg_wit#h_'+str(free_coefficient)\u001b[39;00m\n",
      "File \u001b[0;32m~/cymetricTESTING217/lib/python3.8/site-packages/cymetric/pointgen/pointgen.py:109\u001b[0m, in \u001b[0;36mPointGenerator.__init__\u001b[0;34m(self, monomials, coefficients, kmoduli, ambient, vol_j_norm, verbose, backend)\u001b[0m\n\u001b[1;32m    107\u001b[0m old_monoms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonomials\u001b[38;5;241m.\u001b[39mcopy()  \u001b[38;5;66;03m# make dimensions consistent with CICY case\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonomials \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonomials]) \n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintersection_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_intersection_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonomials \u001b[38;5;241m=\u001b[39m old_monoms  \u001b[38;5;66;03m# undo change\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvol_j_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_volume_from_intersections(np\u001b[38;5;241m.\u001b[39mones_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkmoduli)) \u001b[38;5;28;01mif\u001b[39;00m vol_j_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m vol_j_norm\n",
      "File \u001b[0;32m~/cymetricTESTING217/lib/python3.8/site-packages/cymetric/pointgen/pointgen.py:272\u001b[0m, in \u001b[0;36mPointGenerator._generate_intersection_tensor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    270\u001b[0m d \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkmoduli)] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfold), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m comb:\n\u001b[0;32m--> 272\u001b[0m     d_int \u001b[38;5;241m=\u001b[39m \u001b[43mget_int\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m     entries \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mpermutations(x, \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfold))\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;66;03m# there will be some redundant elements, but they will only have to be calculated once.\u001b[39;00m\n",
      "File \u001b[0;32m~/cymetricTESTING217/lib/python3.8/site-packages/cymetric/pointgen/pointgen.py:553\u001b[0m, in \u001b[0;36mPointGenerator._drst\u001b[0;34m(self, r, s, t)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m# the combinations of mu grow exponentially with len(self.monomials) and the number of ambient spaces\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# Check, when the number of multiset_permutations become to large to handle\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonomials) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m8\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(combination)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m6\u001b[39m:\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;66;03m# Hence, for large K and small len(self.kmoduli), this might take really long.\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m     mu \u001b[38;5;241m=\u001b[39m \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutilities\u001b[49m\u001b[38;5;241m.\u001b[39miterables\u001b[38;5;241m.\u001b[39mmultiset_permutations(combination)\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;66;03m# (len(self.monomials))!/(#x_1!*...*#x_n!)\u001b[39;00m\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m mu:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sympy' has no attribute 'utilities'"
     ]
    }
   ],
   "source": [
    "monomialsTQ = 5*np.eye(5, dtype=np.int64)\n",
    "coefficientsTQ = np.ones(5)\n",
    "kmoduliTQ = np.ones(1)\n",
    "ambientTQ = np.array([4])\n",
    "nameofmanifold=\"Quintic\"\n",
    "\n",
    "nPoints=500000\n",
    "\n",
    "free_coefficient = 1.9#float(sys.argv[1])\n",
    "free_coefficient=2.342351\n",
    "free_coefficient=2.342343234\n",
    "free_coefficient=0.00000000000001\n",
    "#free_coefficient=1.# when the coefficient is 1, ensure that it's 1., not 1 for the sake of the filename\n",
    "#nEpochsPhi=100\n",
    "nEpochsPhi=10\n",
    "\n",
    "depthPhi=4\n",
    "widthPhi=64#128 4 in the 1.0s\n",
    "\n",
    "\n",
    "train_phi=False\n",
    "generate_points_and_save_using_defaults(free_coefficient,nPoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4e14dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sympy\n",
      "Version: 1.13.2\n",
      "Summary: Computer algebra system (CAS) in Python\n",
      "Home-page: https://sympy.org\n",
      "Author: SymPy development team\n",
      "Author-email: sympy@googlegroups.com\n",
      "License: BSD\n",
      "Location: /home/f/fraser-talientec/cymetricTESTING217/lib/python3.8/site-packages\n",
      "Requires: mpmath\n",
      "Required-by: cymetric\n"
     ]
    }
   ],
   "source": [
    "!pip show sympy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01658c4c",
   "metadata": {},
   "source": [
    "## Training the NN\n",
    "\n",
    "Now we can start preperation for training the NN\n",
    "\n",
    "Begin by loading in the required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0f3caf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataAlphaP/Quintic_pg_with_1e-14\n",
      "phimodel_for_10_64_50000s4x64\n",
      "network shape: [25, 64, 64, 64, 64, 1]\n",
      "nns made\n",
      "compiling\n",
      "1563/1563 [==============================] - 6s 2ms/step - loss: 0.4989 - sigma_loss: 0.4989 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 2.6878e-07\n",
      "1563/1563 [==============================] - 6s 2ms/step - loss: 0.5476 - sigma_loss: 0.0022 - kaehler_loss: 0.0000e+00 - transition_loss: 6.9553e-10 - ricci_loss: 0.0000e+00 - volk_loss: 0.2727\n",
      "zero network validation loss: \n",
      "{'loss': 0.5725029110908508, 'sigma_loss': 0.5725029110908508, 'kaehler_loss': 0.0, 'transition_loss': 0.0, 'ricci_loss': 0.0, 'volk_loss': 1.173486197103557e-08}\n",
      "validation loss for final network: \n",
      "{'loss': 0.5150485634803772, 'sigma_loss': 0.0020871683955192566, 'kaehler_loss': 0.0, 'transition_loss': 6.273492281216875e-10, 'ricci_loss': 0.0, 'volk_loss': 0.25648069381713867}\n",
      "ratio of trained to zero: {'loss ratio': 0.8996435555280665, 'sigma_loss ratio': 0.003645690386247029, 'kaehler_loss ratio': 0.0, 'transition_loss ratio': 0.06273492281216875, 'ricci_loss ratio': 0.0, 'volk_loss ratio': 11800428.921928806}\n",
      "average transition discrepancy in standard deviations: tf.Tensor(1.0658346e-06, shape=(), dtype=float32)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# phimodel2,training_history=load_nn_phimodel(free_coefficient,depthPhi,widthPhi,nEpochsPhi,[64,50000],set_weights_to_zero=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd5dbc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirname: dataAlphaP/Quintic_pg_with_1e-14\n",
      "name: phimodel_for_10_64_50000s4x64\n",
      "network shape: [25, 64, 64, 64, 64, 1]\n",
      "testing\n",
      "tested\n",
      "\n",
      "Epoch  1/10\n",
      "7031/7031 [==============================] - 37s 4ms/step - loss: 0.0252 - sigma_loss: 0.0252 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.0077 - sigma_loss: 0.0077 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Sigma measure val:      0.0056\n",
      "9/9 [==============================] - 17s 2s/step - loss: 0.0075 - sigma_loss: 0.0075 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - sigma_val: 0.0056\n",
      "\n",
      "Epoch  2/10\n",
      "7031/7031 [==============================] - 30s 4ms/step - loss: 0.0079 - sigma_loss: 0.0079 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.0070 - sigma_loss: 0.0070 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Sigma measure val:      0.0047\n",
      "9/9 [==============================] - 15s 2s/step - loss: 0.0068 - sigma_loss: 0.0068 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - sigma_val: 0.0047\n",
      "\n",
      "Epoch  3/10\n",
      "7031/7031 [==============================] - 30s 4ms/step - loss: 0.0065 - sigma_loss: 0.0065 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.0047 - sigma_loss: 0.0047 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Sigma measure val:      0.0031\n",
      "9/9 [==============================] - 15s 2s/step - loss: 0.0046 - sigma_loss: 0.0046 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - sigma_val: 0.0031\n",
      "\n",
      "Epoch  4/10\n",
      "7031/7031 [==============================] - 30s 4ms/step - loss: 0.0056 - sigma_loss: 0.0056 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.0043 - sigma_loss: 0.0043 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Sigma measure val:      0.0028\n",
      "9/9 [==============================] - 15s 2s/step - loss: 0.0042 - sigma_loss: 0.0042 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - sigma_val: 0.0028\n",
      "\n",
      "Epoch  5/10\n",
      "7031/7031 [==============================] - 30s 4ms/step - loss: 0.0051 - sigma_loss: 0.0051 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.0053 - sigma_loss: 0.0053 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Sigma measure val:      0.0036\n",
      "9/9 [==============================] - 15s 2s/step - loss: 0.0052 - sigma_loss: 0.0052 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - sigma_val: 0.0036\n",
      "\n",
      "Epoch  6/10\n",
      "7031/7031 [==============================] - 30s 4ms/step - loss: 0.0047 - sigma_loss: 0.0047 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.0046 - sigma_loss: 0.0046 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Sigma measure val:      0.0029\n",
      "9/9 [==============================] - 15s 2s/step - loss: 0.0044 - sigma_loss: 0.0044 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - sigma_val: 0.0029\n",
      "\n",
      "Epoch  7/10\n",
      "7031/7031 [==============================] - 30s 4ms/step - loss: 0.0044 - sigma_loss: 0.0044 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.0038 - sigma_loss: 0.0038 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Sigma measure val:      0.0027\n",
      "9/9 [==============================] - 15s 2s/step - loss: 0.0037 - sigma_loss: 0.0037 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - sigma_val: 0.0027\n",
      "\n",
      "Epoch  8/10\n",
      "7031/7031 [==============================] - 30s 4ms/step - loss: 0.0041 - sigma_loss: 0.0041 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.0031 - sigma_loss: 0.0031 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Sigma measure val:      0.0024\n",
      "9/9 [==============================] - 15s 2s/step - loss: 0.0030 - sigma_loss: 0.0030 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - sigma_val: 0.0024\n",
      "\n",
      "Epoch  9/10\n",
      "7031/7031 [==============================] - 31s 4ms/step - loss: 0.0039 - sigma_loss: 0.0039 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.0034 - sigma_loss: 0.0034 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Sigma measure val:      0.0026\n",
      "9/9 [==============================] - 15s 2s/step - loss: 0.0034 - sigma_loss: 0.0034 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - sigma_val: 0.0026\n",
      "\n",
      "Epoch 10/10\n",
      "7031/7031 [==============================] - 30s 4ms/step - loss: 0.0037 - sigma_loss: 0.0037 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.0029 - sigma_loss: 0.0029 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Sigma measure val:      0.0021\n",
      "9/9 [==============================] - 15s 2s/step - loss: 0.0029 - sigma_loss: 0.0029 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - sigma_val: 0.0021\n",
      "finished training\n",
      "\n",
      "zero network validation loss: \n",
      "{'loss': 0.49886194, 'sigma_loss': 0.49886194, 'kaehler_loss': 0.0, 'transition_loss': 0.0, 'ricci_loss': 0.0, 'volk_loss': 4.251413e-11}\n",
      "validation loss for raw network: \n",
      "{'loss': 0.57518685, 'sigma_loss': 0.5685474, 'kaehler_loss': 0.0, 'transition_loss': 0.0, 'ricci_loss': 0.0, 'volk_loss': 0.0033197403}\n",
      "validation loss for final network: \n",
      "{'loss': 0.007263125, 'sigma_loss': 0.002180979, 'kaehler_loss': 0.0, 'transition_loss': 6.813912e-10, 'ricci_loss': 0.0, 'volk_loss': 0.0025410652}\n",
      "ratio of final to zero: {'loss ratio': 0.014559388502444192, 'sigma_loss ratio': 0.004371908689896581, 'kaehler_loss ratio': 0.0, 'transition_loss ratio': 0.06813912212244588, 'ricci_loss ratio': 0.0, 'volk_loss ratio': 253030.7832173289}\n",
      "ratio of final to raw: {'loss ratio': 0.012627418049145816, 'sigma_loss ratio': 0.003836054380341823, 'kaehler_loss ratio': 0.0, 'transition_loss ratio': 0.06813912212244588, 'ricci_loss ratio': 0.0, 'volk_loss ratio': 0.7654386595213793}\n",
      "average transition discrepancy in standard deviations: tf.Tensor(1.0658346e-06, shape=(), dtype=float32)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if train_phi:\n",
    "    phimodel1,training_history=train_and_save_nn(free_coefficient,depthPhi,widthPhi,nEpochsPhi,stddev=0.05,bSizes=[64,50000],lRate=0.1) \n",
    "else:\n",
    "    phimodel1,training_history=load_nn_phimodel(free_coefficient,depthPhi,widthPhi,nEpochsPhi,[64,50000],set_weights_to_zero=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f06c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=np.load('dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) + '/dataset.npz')\n",
    "# X_total=data['X_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5cbe52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #compute ricci scalar\n",
    "# from cymetric.models.measures import ricci_scalar_fn\n",
    "# R_scalar_cymetric_train=ricci_scalar_fn(phimodel1,tf.cast(X_total[0:100],np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f32fd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-26.42 -15.88 8.34 37.44 -21.00 58.78 -37.50 -12.50 55.89 -14.08 14.36\n",
      " -31.41 14.55 3.01 -17.73 -37.55 -36.73 12.50 -32.13 -26.40 -2.15 -34.69\n",
      " -26.19 -34.96 22.06 4.58 -1.72 5.24 64.00 48.70 -11.64 6.86 -25.40 -8.96\n",
      " 24.06 -31.53 21.98 64.09 -34.84 13.21 -10.32 52.61 13.37 6.64 4.61 -18.74\n",
      " -14.62 -1.76 5.37 -34.79 -11.98 -35.05 -2.36 -10.76 -34.16 -33.27 -17.06\n",
      " 11.13 -29.20 22.80 10.76 53.24 -15.38 -1.14 18.03 19.18 23.92 -34.45\n",
      " -31.54 -32.06 -36.48 22.89 38.67 -4.93 16.31 1.46 13.80 -36.13 49.92\n",
      " -35.97 50.06 5.98 2.76 -15.94 -15.63 -32.80 -11.93 -31.42 -15.20 -34.02\n",
      " -3.66 40.01 4.99 16.00 -3.19 -33.77 -5.83 -3.56 14.31 14.54]\n"
     ]
    }
   ],
   "source": [
    "# R_scalar_cymetric_train[0]\n",
    "# #round to 2dp, and print in standard format\n",
    "# print(np.array2string(np.round(R_scalar_cymetric_train[0], 2), formatter={'float_kind':lambda x: f\"{x:.2f}\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1eb5fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirname for alpha: dataAlphaP/Quintic_pg_with_1e-14\n",
      "dirname for alpha: dataAlphaP/QuinticAlpha_pg_with_1e-14\n",
      "Generating: forced? False\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prepare_dataset_Alpha' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_points_and_save_using_defaultsAlpha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfree_coefficient\u001b[49m\u001b[43m,\u001b[49m\u001b[43mphimodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m200.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mforce_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mseed_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 32\u001b[0m, in \u001b[0;36mgenerate_points_and_save_using_defaultsAlpha\u001b[0;34m(free_coefficient, phimodel, euler_char, force_generate, seed_set)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_generate \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dirnameAlpha)):\n\u001b[1;32m     31\u001b[0m    \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating: forced? \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(force_generate))\n\u001b[0;32m---> 32\u001b[0m    kappaAlpha \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_dataset_Alpha\u001b[49m(pg,data,dirnameAlpha,phimodel,euler_char,BASIS,normalize_to_vol_j\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m);\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dirnameAlpha):\n\u001b[1;32m     34\u001b[0m    \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prepare_dataset_Alpha' is not defined"
     ]
    }
   ],
   "source": [
    "generate_points_and_save_using_defaultsAlpha(free_coefficient,phimodel1,-200.,force_generate=False,seed_set=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a058210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphaprime=1\n",
    "euler_char=-200\n",
    "depthAlpha=4\n",
    "widthAlpha=64\n",
    "nEpochsAlpha=10\n",
    "\n",
    "\n",
    "train_alpha=True\n",
    "if train_alpha:\n",
    "    AlphaModel1,training_historyAlpha=train_and_save_nn_Alpha(free_coefficient,phimodel1,euler_char,alphaprime,depthAlpha,widthAlpha,nEpochsAlpha,bSizes=[64,50000],stddev=0.05,lRate=0.1,use_zero_network=False,alpha=[1.,1.],load_network=False)\n",
    "else:\n",
    "    AlphaModel1,training_historyAlpha=load_nn_Alpha(free_coefficient,phimodel1,euler_char,alphaprime,depthPhi,widthPhi,nEpochsPhi,[64,50000],set_weights_to_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f584aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.load('dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) + '/dataset.npz')\n",
    "# AlphaModel1.model(data['X_train'][0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2954e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reimport custom_networks\n",
    "importlib.reload(sys.modules['custom_networks'])\n",
    "from custom_networks import *\n",
    "#reimport laplacian_funcs\n",
    "importlib.reload(sys.modules['laplacian_funcs'])\n",
    "from laplacian_funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bc49d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(self,kphi,linebundleindices,ambient_env_var,n_projective,kmoduli)\n",
    "n_0=0\n",
    "n_p= 500000\n",
    "ptdata=tf.cast(data['X_train'],tf.float32)[n_0:n_p]\n",
    "#cptdata=point_vec_to_complex(ptdata)\n",
    "#cptdata2=cptdata/cptdata[:,0:1]\n",
    "flat_measure_weights=tf.cast(((data['y_train'][:,0]/(data['y_train'][:,1]))*(1/(6)))[n_0:n_p],tf.float32)\n",
    "pullbacks=AlphaModel1.pullbacks(ptdata)#[n_0:n_p]\n",
    "kphi = np.array([3])\n",
    "#get_sections_cpts_func=get_degree_kphiandMmonomials_general(kphi,[0],ambientTQ,len(ambientTQ),kmoduliTQ)\n",
    "# get_sections_cpts_func(cptdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b5b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tf.function this, but for fixed arguments 0,2 and tensor arguments 1,3,4\n",
    "\n",
    "def compute_matrices_for_pts(rpoints,metric,flat_measure_weights,pullbacks,get_sections_cpts_func):\n",
    "    get_sections_rpts_func= lambda x: get_sections_cpts_func(point_vec_to_complex(x))\n",
    "    metrics = metric(rpoints)\n",
    "    inversemetrics=tf.linalg.inv(metrics)\n",
    "    dets = tf.linalg.det(metrics)\n",
    "    #k_phi=get_sections_rpts_func(cpoints)\n",
    "    djsigma=extder_j_for_sigma(rpoints,get_sections_rpts_func)\n",
    "    djbarsigma=extder_jbar_for_sigma(rpoints,get_sections_rpts_func)\n",
    "    dasigma = tf.einsum('xai,xCi->xCa',pullbacks,djsigma)\n",
    "    dbbarsigma = tf.einsum('xbj,xCj->xCb',tf.math.conj(pullbacks),djbarsigma)\n",
    "    #b is barred, a is unbarred\n",
    "    dbarfbar_df = tf.einsum('xba,xCb,xDa->xCD',inversemetrics,tf.math.conj(dasigma),dasigma)\n",
    "    dfbar_dbarf = tf.einsum('xba,xCa,xDb->xCD',inversemetrics,tf.math.conj(dbbarsigma),dbbarsigma)\n",
    "    #factor of 2 form the laplacian!\n",
    "    integrand_CD = tf.einsum('xCD,x->xCD',2*(dbarfbar_df + dfbar_dbarf),dets*tf.cast(flat_measure_weights,tf.complex64))\n",
    "    integrand_mean=tf.reduce_mean(integrand_CD,axis=0)\n",
    "    print(\"vol: \" + str(tf.reduce_mean(dets*tf.cast(flat_measure_weights,tf.complex64))))\n",
    "    return integrand_mean\n",
    "    \n",
    "get_sections_cpts_func=get_degree_kphiandMmonomials_general(kphi,kphi*0,ambientTQ,len(ambientTQ),kmoduliTQ)\n",
    "#mats=compute_matrices_for_pts(kphi,ptdata,AlphaModel1,flat_measure_weights,pullbacks,get_sections_cpts_func)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf56f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normalising_matrices_for_pts(rpoints,metric,flat_measure_weights,get_sections_cpts_func):\n",
    "    get_sections_rpts_func= lambda x: get_sections_cpts_func(point_vec_to_complex(x))\n",
    "    metrics = metric(rpoints)\n",
    "    dets = tf.linalg.det(metrics)\n",
    "    sectionsB    = get_sections_rpts_func(rpoints)\n",
    "    sectionsConjA   = tf.math.conj(sectionsB)\n",
    "\n",
    "    #k_phi=get_sections_rpts_func(cpoints)\n",
    "    integrand_CD = tf.einsum('xA,xB,x->AB',sectionsConjA,sectionsB,dets*tf.cast(flat_measure_weights,tf.complex64))/tf.cast(tf.shape(flat_measure_weights)[-1],tf.complex64)\n",
    "    print(\"vol: \" + str(tf.reduce_mean(dets*tf.cast(flat_measure_weights,tf.complex64))))\n",
    "    return integrand_CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47cc20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "metric_using=phimodel1\n",
    "\n",
    "\n",
    "#create compiled versions\n",
    "@tf.function\n",
    "def compute_matrices_for_pts_fixed(rpoints, flat_measure_weights, pullbacks):\n",
    "    #kphi = tf.constant([1])  # Fixed argument\n",
    "    return compute_matrices_for_pts(rpoints, metric_using, flat_measure_weights, pullbacks,get_sections_cpts_func)\n",
    "\n",
    "# Get the concrete function\n",
    "concrete_func = compute_matrices_for_pts_fixed.get_concrete_function(\n",
    "    tf.TensorSpec(shape=(batch_size,2*metric_using.ncoords), dtype=tf.float32),  # rpoints\n",
    "    tf.TensorSpec(shape=(batch_size), dtype=tf.float32),  # flat_measure_weights\n",
    "    tf.TensorSpec(shape=(batch_size, metric_using.nfold,metric_using.ncoords), dtype=tf.complex64)  # pullbacks\n",
    ")\n",
    "\n",
    "@tf.function\n",
    "def compute_normalising_matrices_for_pts_fixed(rpoints, flat_measure_weights):\n",
    "    #kphi = tf.constant([1])  # Fixed argument\n",
    "    return compute_normalising_matrices_for_pts(rpoints, metric_using, flat_measure_weights,get_sections_cpts_func)\n",
    "\n",
    "# Get the concrete function for the normalising matrix\n",
    "concrete_func_normalising = compute_normalising_matrices_for_pts_fixed.get_concrete_function(\n",
    "    tf.TensorSpec(shape=(batch_size,2*metric_using.ncoords), dtype=tf.float32),  # rpoints\n",
    "    tf.TensorSpec(shape=(batch_size), dtype=tf.float32),  # flat_measure_weights\n",
    ")\n",
    "\n",
    "#compute_matrices_for_pts_fixed(ptdata, flat_measure_weights, pullbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf64f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test runtime!\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((ptdata, flat_measure_weights, pullbacks))\n",
    "batched_dataset = dataset.batch(batch_size)\n",
    "import time\n",
    "start = time.time()\n",
    "for batch in batched_dataset.take(4):\n",
    "    batch_rpoints, batch_flat_measure_weights, batch_pullbacks = batch\n",
    "    batch_result = compute_matrices_for_pts_fixed(batch_rpoints, batch_flat_measure_weights, batch_pullbacks)\n",
    "    #oresults.append(batch_result)\n",
    "    print('done, timing: ' + str(time.time()-start))\n",
    "    start=time.time()\n",
    "\n",
    "print(\"\\n\\n concrete\" )\n",
    "import time\n",
    "start = time.time()\n",
    "for batch in batched_dataset.take(4):\n",
    "    batch_rpoints, batch_flat_measure_weights, batch_pullbacks = batch\n",
    "    batch_result = concrete_func(batch_rpoints, batch_flat_measure_weights, batch_pullbacks)\n",
    "    #oresults.append(batch_result)\n",
    "    print('done, timing: ' + str(time.time()-start))\n",
    "    start=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbb7f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the concrete_func is defined as in the previous example\n",
    "\n",
    "# Create a dataset from your large number of points\n",
    "dataset = tf.data.Dataset.from_tensor_slices((ptdata, flat_measure_weights, pullbacks))\n",
    "# Adjust based on your memory constraints and performance needs\n",
    "\n",
    "\n",
    "# Batch the dataset\n",
    "batched_dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Process batches\n",
    "results = []\n",
    "for batch in batched_dataset:\n",
    "    batch_rpoints, batch_flat_measure_weights, batch_pullbacks = batch\n",
    "    batch_result = concrete_func(batch_rpoints, batch_flat_measure_weights, batch_pullbacks)\n",
    "    results.append(batch_result)\n",
    "\n",
    "\n",
    "# Combine results if needed\n",
    "matrix_laplacian_requires_mean = np.array(results)\n",
    "\n",
    "print(f\"Processed {num_points} points. Shape of final result: {final_result.shape}\")\n",
    "\n",
    "results_norm = []\n",
    "for batch in batched_dataset:\n",
    "    batch_rpoints, batch_flat_measure_weights,_ = batch\n",
    "    batch_result_norm = concrete_func_normalising(batch_rpoints, batch_flat_measure_weights)\n",
    "    results_norm.append(batch_result_norm)\n",
    "\n",
    "matrix_normalising_requires_mean = np.array(results_norm)\n",
    "\n",
    "print(f\"Processed {num_points} points. Shape of final result: {final_result_norm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "#take eigs of final_result\n",
    "\n",
    "\n",
    "#log distribute over 1 to 100000\n",
    "number_approx=6\n",
    "\n",
    "numbers_to_take= np.unique((batch_size*np.round(np.logspace(np.log10(10000),np.log10(len(ptdata)),number_approx)/batch_size)).astype('int'))[0:]\n",
    "indices_to_take=tf.cast(numbers_to_take/batch_size,tf.int32).numpy()\n",
    "indices_to_take[0]\n",
    "print(numbers_to_take)\n",
    "\n",
    "eigvals_all=[]\n",
    "for i in indices_to_take:\n",
    "    matrix=tf.reduce_mean(np.array(results)[0:i],axis=0)\n",
    "    matrix_norm=tf.reduce_mean(np.array(results_norm)[0:i],axis=0)\n",
    "    eigvals=tf.math.real(scipy.linalg.eigvals(matrix,matrix_norm))\n",
    "    eigvals_all.append(np.sort(eigvals))\n",
    "    #round and print to 2dp\n",
    "    #eigvals_rounded=np.round(eigvals,2)\n",
    "    #print(tf.math.real(eigvals_rounded))\n",
    "eigvals_all=np.array(eigvals_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7793a9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#sort_eigvals_rounded=np.sort(eigvals_rounded)[0:200]\n",
    "#plt.scatter(range(len(sort_eigvals_rounded)),sort_eigvals_rounded)\n",
    "numbers_for_plotting=tf.repeat(tf.expand_dims(numbers_to_take,axis=-1),tf.shape(eigvals_all)[-1],axis=-1)\n",
    "#join each eigval as hte number of points increase with a dotted line\n",
    "#change the plot to a line plot\n",
    "#change all the line colours to red and thin, with a small red x at each point\n",
    "#fade the lines from blue to red corresponding to their index in the first axis of the array\n",
    "\n",
    "#plt.plot(numbers_for_plotting,eigvals_all,marker='x',linestyle='dotted',color='red',linewidth=0.5,markersize=1)\n",
    "\n",
    "\n",
    "#change the x axis to log scale, labelled at the endpoints with one point in the middle\n",
    "plt.xscale('log')\n",
    "middle_pt=((numbers_to_take[-1]-numbers_to_take[0])//(5000*2))*5000 + numbers_to_take[0]\n",
    "middle_pt2=((numbers_to_take[-1]-numbers_to_take[0])//(5000*5))*5000 + numbers_to_take[0]\n",
    "ticks = tf.cast(np.concatenate((numbers_to_take[[0,-1]],np.array([middle_pt,middle_pt2])),axis=0),tf.int32).numpy()\n",
    "plt.xticks(ticks,ticks)\n",
    "#label the x axis as number of points n_p\n",
    "#label the y axis as eigenvalue\n",
    "plt.xlabel('number of points n_p')\n",
    "plt.ylabel('eigenvalue')\n",
    "#title the plot as eigenvalues of the laplacian as number of points increases\n",
    "plt.title('eigenvalues of the laplacian vs integration dataset size')\n",
    "plt.ylim(0,300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d691905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Create a custom colormap from blue to red\n",
    "colors = [(0, 0, 1), (1, 0, 0)]  # R -> G -> B\n",
    "n_bins = eigvals_all.shape[1]  # Number of color bins (1225 eigenvalues)\n",
    "cmap = LinearSegmentedColormap.from_list('red_to_blue', colors, N=n_bins)\n",
    "\n",
    "# Plot setup\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot each eigenvalue evolution with a different color\n",
    "for i in range(n_bins):\n",
    "    color = cmap(i / (n_bins - 1))\n",
    "    ax.plot(numbers_for_plotting[:,i], eigvals_all[:,i],\n",
    "            marker='x', linestyle='dotted', color=color, linewidth=0.5, markersize=1)\n",
    "\n",
    "# Set x-axis to log scale\n",
    "ax.set_xscale('log')\n",
    "\n",
    "# Set x-axis ticks\n",
    "middle_pt = ((numbers_to_take[-1]-numbers_to_take[0])//(5000*2))*5000 + numbers_to_take[0]\n",
    "middle_pt2 = ((numbers_to_take[-1]-numbers_to_take[0])//(5000*5))*5000 + numbers_to_take[0]\n",
    "ticks = np.concatenate((numbers_to_take[[0,-1]], np.array([middle_pt, middle_pt2])))\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_xticklabels(ticks)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('number of points n_p')\n",
    "ax.set_ylabel('eigenvalue')\n",
    "ax.set_title('eigenvalues of the laplacian vs integration dataset size')\n",
    "\n",
    "# Set y-axis limit\n",
    "# `ax.set_ylim(0, 300)`\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c172119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_adam_optimizer_with_decay(initial_learning_rate, nEpochs, final_lr_factor=10):\n",
    "    \"\"\"\n",
    "    Creates an Adam optimizer with exponential learning rate decay.\n",
    "\n",
    "    Parameters:\n",
    "    - initial_learning_rate: The starting learning rate\n",
    "    - nEpochs: The number of epochs over which to decay the learning rate\n",
    "    - final_lr_factor: The factor by which to reduce the learning rate (default: 10)\n",
    "\n",
    "    Returns:\n",
    "    - An Adam optimizer with the specified learning rate decay\n",
    "    \"\"\"\n",
    "    decay_rate = (1/final_lr_factor) ** (1/nEpochs)\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=1,\n",
    "        decay_rate=decay_rate,\n",
    "        staircase=False\n",
    "    )\n",
    "\n",
    "    return tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "def generate_points_and_save_using_defaults(free_coefficient,number_points,force_generate=False,seed_set=0):\n",
    "   coefficients=coefficientsTQ\n",
    "   # coefficients=np.array([1, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, \\\n",
    "   # 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, free_coefficient, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 1, 0, 2, 0, 0, 0, 2, 0, 1])\n",
    "   monomials=monomialsTQ\n",
    "   kmoduli=kmoduliTQ\n",
    "   ambient=ambientTQ\n",
    "   pg = PointGenerator(monomials, coefficients, kmoduli, ambient)\n",
    "   pg._set_seed(seed_set)\n",
    "   #dirname = 'data/tetraquadric_pg_wit#h_'+str(free_coefficient)\n",
    "   dirname = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   print(\"dirname: \" + dirname)\n",
    "   #test if the directory exists, if not, create it\n",
    "   if force_generate or (not os.path.exists(dirname)):\n",
    "      print(\"Generating: forced? \" + str(force_generate))\n",
    "      kappa = pg.prepare_dataset(number_points, dirname)\n",
    "      pg.prepare_basis(dirname, kappa=kappa)\n",
    "   elif os.path.exists(dirname):\n",
    "      try:\n",
    "         print(\"loading prexisting dataset\")\n",
    "         data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "         if (len(data['X_train'])+len(data['X_val']))!=number_points:\n",
    "            print(\"wrong length - generating anyway\")\n",
    "            kappa = pg.prepare_dataset(number_points, dirname)\n",
    "            pg.prepare_basis(dirname, kappa=kappa)\n",
    "      except:\n",
    "         print(\"error loading - generating anyway\")\n",
    "         kappa = pg.prepare_dataset(number_points, dirname)\n",
    "         pg.prepare_basis(dirname, kappa=kappa)\n",
    "\n",
    "\n",
    "def getcallbacksandmetrics(data):\n",
    "   #rcb = RicciCallback((data['X_val'], data['y_val']), data['val_pullbacks'])\n",
    "   scb = SigmaCallback((data['X_val'], data['y_val']))\n",
    "   volkcb = VolkCallback((data['X_val'], data['y_val']))\n",
    "   kcb = KaehlerCallback((data['X_val'], data['y_val']))\n",
    "   tcb = TransitionCallback((data['X_val'], data['y_val']))\n",
    "   #cb_list = [rcb, scb, kcb, tcb, volkcb]\n",
    "   #cb_list = [ scb, kcb, tcb, volkcb]\n",
    "   #cmetrics = [TotalLoss(), SigmaLoss(), KaehlerLoss(), TransitionLoss(), VolkLoss()]#, RicciLoss()]\n",
    "   cb_list = [ scb ]\n",
    "   cmetrics = [TotalLoss(), SigmaLoss()]#, RicciLoss()]\n",
    "   return cb_list, cmetrics\n",
    "\n",
    "def train_and_save_nn(free_coefficient,nlayer=3,nHidden=128,nEpochs=50,stddev=0.1,bSizes=[192,50000],lRate=0.001,use_zero_network=False):\n",
    "   #dirname = 'data/tetraquadric_pg_with_'+str(free_coefficient)\n",
    "   dirname = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   name = 'phimodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(bSizes[1]) + 's' + str(nlayer) + 'x' +str(nHidden)\n",
    "   print('dirname: ' + dirname)\n",
    "   print('name: ' + name)\n",
    "\n",
    "   data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirname, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetrics(data)\n",
    "\n",
    "   #nlayer = 3\n",
    "   #nHidden = 128\n",
    "   act = 'gelu'\n",
    "   #nEpochs = 100\n",
    "   #bSizes = [192, 150000]\n",
    "   alpha = [1., 1., 30., 1., 2.] # 1 AND 3??\n",
    "   #nfold = 3\n",
    "   #n_in = 2*5\n",
    "   #n_out = 1\n",
    "   #lRate = 0.001\n",
    "   ambient=tf.cast(tf.math.abs(BASIS['AMBIENT']),tf.int32)\n",
    "\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   nn_phi = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_phi_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "   phimodel = PhiFSModel(nn_phi, BASIS, alpha=alpha)\n",
    "   phimodelzero = PhiFSModel(nn_phi_zero, BASIS, alpha=alpha)\n",
    "   #print('nn_phi ' )\n",
    "   #print('nn_phi ' + str(phimodel(data['X_train'][0:10])))\n",
    "\n",
    "   #Note, currently running legacy due to ongoing tf issue with M1/M2.\n",
    "   #Use the commented line instead if not on an M1/M2 machine\n",
    "   #opt = tfk.optimizers.Adam(learning_rate=lRate)\n",
    "   #opt = tfk.optimizers.legacy.Adam(learning_rate=lRate)\n",
    "   opt='adam'\n",
    "   #opt = create_adam_optimizer_with_decay(\n",
    "   # initial_learning_rate=lRate,\n",
    "   # nEpochs=nEpochs*(len(data['X_train'])//bSizes[0]),\n",
    "   # final_lr_factor=10  # This will decay to lRate/10\n",
    "   #)\n",
    "   # compile so we can test on validation set before training\n",
    "   phimodel.compile(custom_metrics=cmetrics)\n",
    "   phimodelzero.compile(custom_metrics=cmetrics)\n",
    "\n",
    "   ## compare validation loss before training for zero network and nonzero network\n",
    "   datacasted=[tf.cast(data['X_val'],tf.float32),tf.cast(data['y_val'],tf.float32)]\n",
    "   #need to re-enable learning, in case there's been a problem:\n",
    "   phimodel.learn_transition = False\n",
    "   phimodelzero.learn_transition = False\n",
    "   phimodel.learn_volk = True\n",
    "   phimodelzero.learn_volk = True\n",
    "   #phimodel.learn_ricci_val= True\n",
    "   #phimodelzero.learn_ricci_val= True\n",
    "   print(\"testing\")\n",
    "   valzero=phimodelzero.test_step(datacasted)\n",
    "   valraw=phimodel.test_step(datacasted)\n",
    "   print('tested')\n",
    "   # phimodel.learn_ricci_val=False\n",
    "   # phimodelzero.learn_ricci_val=False\n",
    "   valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   valraw = {key: value.numpy() for key, value in valraw.items()}\n",
    "\n",
    "   #### maybe remove\n",
    "   phimodel.learn_volk = False\n",
    "   phimodel.learn_transition=False\n",
    "   phimodel, training_history = train_model(phimodel, data, optimizer=opt, epochs=nEpochs, batch_sizes=bSizes,\n",
    "                                       verbose=1, custom_metrics=cmetrics, callbacks=cb_list)\n",
    "   print(\"finished training\\n\")\n",
    "   phimodel.model.save_weights(os.path.join(dirname, name) + '.weights.h5')\n",
    "   np.savez_compressed(os.path.join(dirname, 'trainingHistory-' + name),training_history)\n",
    "   #now print the initial losses and final losses for each metric\n",
    "   # first_metrics = {key: value[0] for key, value in training_history.items()}\n",
    "   # lastometrics = {key: value[-1] for key, value in training_history.items()}\n",
    "   phimodel.learn_transition = True\n",
    "   phimodel.learn_volk = True\n",
    "   #phimodel.learn_ricci_val= True\n",
    "   valfinal=phimodel.test_step(datacasted)\n",
    "   valfinal = {key: value.numpy() for key, value in valfinal.items()}\n",
    "   #phimodel.learn_ricci_val=False\n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for raw network: \")\n",
    "   print(valraw)\n",
    "   print(\"validation loss for final network: \")\n",
    "   print(valfinal)\n",
    "   print(\"ratio of final to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valfinal.items()}))\n",
    "   print(\"ratio of final to raw: \" + str({key + \" ratio\": value/(valraw[key]+1e-8) for key, value in valfinal.items()}))\n",
    "\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(phimodel,tf.cast(data[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   #IMPLEMENT THE FOLLOWING\n",
    "   #meanfailuretosolveequation,_,_=measure_laplacian_failure(phimodel,data)\n",
    "   print(\"\\n\\n\")\n",
    "   return phimodel,training_history\n",
    "\n",
    "def load_nn_phimodel(free_coefficient,nlayer=3,nHidden=128,nEpochs=50,bSizes=[192,50000],stddev=0.1,lRate=0.001,set_weights_to_zero=False):\n",
    "   dirname = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   name = 'phimodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(bSizes[1]) + 's' + str(nlayer) + 'x' +str(nHidden)\n",
    "   print(dirname)\n",
    "   print(name)\n",
    "\n",
    "   data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirname, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetrics(data)\n",
    "\n",
    "   #nlayer = 3\n",
    "   #nHidden = 128\n",
    "   act = 'gelu'\n",
    "   #nEpochs = 100\n",
    "   #bSizes = [192, 150000]\n",
    "   alpha = [1., 1., 30., 1., 2.] # 1 AND 3??\n",
    "   nfold = 3\n",
    "   n_in = 2*8\n",
    "   n_out = 1\n",
    "   #lRate = 0.001\n",
    "\n",
    "\n",
    "   ambient=tf.cast(BASIS['AMBIENT'],tf.int32)\n",
    "\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   nn_phi = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_phi_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "   print(\"nns made\")\n",
    "\n",
    "   datacasted=[tf.cast(data['X_val'],tf.float32),tf.cast(data['y_val'],tf.float32)]\n",
    "\n",
    "   #    nn_phi = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "   #    nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "   phimodel = PhiFSModel(nn_phi, BASIS, alpha=alpha)\n",
    "   phimodelzero = PhiFSModel(nn_phi_zero, BASIS, alpha=alpha)\n",
    "   #initialise weights\n",
    "   phimodel(datacasted[0][0:1])\n",
    "   phimodelzero(datacasted[0][0:1])\n",
    "\n",
    "\n",
    "   if set_weights_to_zero:\n",
    "      training_history=0\n",
    "   else:\n",
    "      #phimodel.model=tf.keras.layers.TFSMLayer(os.path.join(dirname,name),call_endpoint=\"serving_default\")\n",
    "      #phimodel.model=tf.keras.models.load_model(os.path.join(dirname, name) + \".keras\")\n",
    "      #print_sample_params(phimodel.model)\n",
    "      phimodel.model.load_weights(os.path.join(dirname, name) + '.weights.h5')\n",
    "      #print_sample_params(phimodel.model)\n",
    "      training_history=np.load(os.path.join(dirname, 'trainingHistory-' + name +'.npz'),allow_pickle=True)['arr_0'].item()\n",
    "\n",
    "   print(\"compiling\")\n",
    "   phimodel.compile(custom_metrics=cmetrics)\n",
    "   phimodelzero.compile(custom_metrics=cmetrics)\n",
    "\n",
    "   # compare validation loss before training for zero network and nonzero network\n",
    "   #need to re-enable learning, in case there's been a problem:\n",
    "   phimodel.learn_transition = True\n",
    "   phimodelzero.learn_transition = True\n",
    "   phimodel.learn_volk = True\n",
    "   phimodelzero.learn_volk = True\n",
    "\n",
    "   #problem - metricsnames aren't defined unless model has been trained, not just evaluated? SOlution - return_dict\n",
    "   valzero=phimodelzero.evaluate(datacasted[0],datacasted[1],return_dict=True)\n",
    "   valtrained=phimodel.evaluate(datacasted[0],datacasted[1],return_dict=True)\n",
    "\n",
    "\n",
    "   phimodel.learn_transition = True\n",
    "   phimodel.learn_volk = True\n",
    "\n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for final network: \")\n",
    "   print(valtrained)\n",
    "   print(\"ratio of trained to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valtrained.items()}))\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(phimodel,tf.cast(data[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   print(\"\\n\\n\")\n",
    "   #IMPLEMENT THE FOLLOWING\n",
    "   #meanfailuretosolveequation,_,_=measure_laplacian_failure(phimodel,data)\n",
    "   #print(\"\\n\\n\")\n",
    "   #print(\"mean of difference/mean of absolute value of source, weighted by sqrt(g): \" + str(meanfailuretosolveequation))\n",
    "   tf.keras.backend.clear_session()\n",
    "   return phimodel,training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42307dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_points_and_save_using_defaultsAlpha(free_coefficient,phimodel,euler_char,force_generate=False,seed_set=0):\n",
    "   coefficients=coefficientsTQ\n",
    "   # coefficients=np.array([1, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, \\\n",
    "   # 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, free_coefficient, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 1, 0, 2, 0, 0, 0, 2, 0, 1])\n",
    "   monomials=monomialsTQ\n",
    "   kmoduli=kmoduliTQ\n",
    "   ambient=ambientTQ\n",
    "   # monomials = 5*np.eye(5, dtype=np.int64)\n",
    "   # coefficients = np.ones(5)\n",
    "   \n",
    "   # kmoduli = np.ones(1)\n",
    "   # ambient = np.array([4])\n",
    "   pg = PointGenerator(monomials, coefficients, kmoduli, ambient)\n",
    "   pg._set_seed(seed_set)\n",
    "\n",
    "\n",
    "   dirnameForMetric = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   dirnameAlpha= 'dataAlphaP/'+nameofmanifold+ \"Alpha_pg_with_\" + str(free_coefficient) \n",
    "   #dirnameAlpha = 'dataAlphaP/tetraquadricAlpha_pg_with_'+str(free_coefficient)+'forLB_'+lbstring\n",
    "   #dirnameForMetric = 'dataAlphaP/tetraquadric_pg_with_'+str(free_coefficient)\n",
    "   print(\"dirname for alpha: \" + dirnameForMetric)\n",
    "   print(\"dirname for alpha: \" + dirnameAlpha)\n",
    "\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirnameForMetric, 'basis.pickle'), allow_pickle=True))\n",
    "   \n",
    "   data=np.load(os.path.join(dirnameForMetric, 'dataset.npz'))\n",
    "\n",
    "   if force_generate or (not os.path.exists(dirnameAlpha)):\n",
    "      print(\"Generating: forced? \" + str(force_generate))\n",
    "      kappaAlpha = prepare_dataset_Alpha(pg,data,dirnameAlpha,phimodel,euler_char,BASIS,normalize_to_vol_j=True);\n",
    "   elif os.path.exists(dirnameAlpha):\n",
    "      try:\n",
    "         print(\"loading prexisting dataset\")\n",
    "         data = np.load(os.path.join(dirnameAlpha, 'dataset.npz'))\n",
    "      except:\n",
    "         print(\"problem loading data - generating anyway\")\n",
    "         kappaAlpha = prepare_dataset_Alpha(pg,data, dirnameAlpha,phimodel,euler_char,BASIS,normalize_to_vol_j=True);\n",
    "      \n",
    "   \n",
    "\n",
    "def getcallbacksandmetricsAlpha(dataalpha):\n",
    "   dataalpha_val_dict=dict(list(dict(dataalpha).items())[len(dict(dataalpha))//2:])\n",
    "   tcb = TransitionCallback((dataalpha['X_val'], dataalpha['y_val']))\n",
    "   lplcb = LaplacianCallback(dataalpha_val_dict)\n",
    "   # lplcb = LaplacianCallback(data_val)\n",
    "   cb_list = [lplcb,tcb]\n",
    "   cmetrics = [TotalLoss(), LaplacianLoss(), TransitionLoss()]\n",
    "   return cb_list, cmetrics\n",
    "\n",
    "   \n",
    "def train_and_save_nn_Alpha(free_coefficient,phimodel,euler_char,alphaprime,nlayer=3,nHidden=128,nEpochs=30,bSizes=[192,50000],stddev=0.1,lRate=0.001,alpha=[1,1],load_network=False,use_zero_network=False):\n",
    "   \n",
    "   dirnameForMetric = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   dirnameAlpha= 'dataAlphaP/'+nameofmanifold+ \"Alpha_pg_with_\" + str(free_coefficient) \n",
    "\n",
    "   #data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirnameForMetric, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "\n",
    "   dataalpha = np.load(os.path.join(dirnameAlpha, 'dataset.npz'))\n",
    "   dataalpha_train=tf.data.Dataset.from_tensor_slices(dict(list(dict(dataalpha).items())[:len(dict(dataalpha))//2]))\n",
    "   dataalpha_val_dict=dict(list(dict(dataalpha).items())[len(dict(dataalpha))//2:])\n",
    "   dataalpha_val=tf.data.Dataset.from_tensor_slices(dataalpha_val_dict)\n",
    "   # batch_sizes=[64,10000]\n",
    "   dataalpha_train=dataalpha_train.shuffle(buffer_size=1024).batch(bSizes[0])\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetricsAlpha(dataalpha)\n",
    "\n",
    "   #nlayer = 3\n",
    "   #nHidden = 128\n",
    "   act = 'gelu'\n",
    "   #nEpochs = 30\n",
    "   #bSizes = [192, 150000]\n",
    "   #alpha = [1., 1.] # 1 AND 3??\n",
    "   nfold = 3\n",
    "   n_in = 2*8\n",
    "   n_out = 1\n",
    "   #lRate = 0.001\n",
    "   name = 'alphamodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(nlayer) + 'x' +str(nHidden)\n",
    "   print(\"name: \" + name)\n",
    "\n",
    "   ambient=tf.cast(BASIS['AMBIENT'],tf.int32)\n",
    "\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   #nfirstlayer=tf.reduce_sum(((np.array(ambient)+1)**2)).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=stddev)\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   activ=tf.square\n",
    "   #activ=tfk.activations.gelu\n",
    "   #nn_alpha = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #copie from phi above\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "\n",
    "   #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.2)\n",
    "   #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)#note we don't need a last bias (flat direction)\n",
    "   #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network,kernel_initializer=initializer)#note we don't need a last bias (flat direction)\n",
    "   #nn_alpha_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)#note we don't need a last bias (flat direction)\n",
    "   \n",
    "   alphamodel= AlphaPrimeModel(nn_alpha,BASIS, phimodel,alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "   if load_network:\n",
    "      print(\"loading network\")\n",
    "      alphamodel.model=tf.keras.models.load_model(os.path.join(dirnameAlpha,name))\n",
    "      print(\"network loaded\")\n",
    "\n",
    "   alphamodelzero= AlphaPrimeModel(nn_alpha_zero,BASIS,phimodel, alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "\n",
    "   #Note, currently running legacy due to ongoing tf issue with M1/M2. \n",
    "   #Use the commented line instead if not on an M1/M2 machine\n",
    "   opt = tfk.optimizers.Adam(learning_rate=lRate)\n",
    "   #opt = tfk.optimizers.legacy.Adam(learning_rate=lRate)\n",
    "   # compile so we can test on validation set before training\n",
    "   alphamodel.compile(custom_metrics=cmetrics)\n",
    "   alphamodelzero.compile(custom_metrics=cmetrics)\n",
    "   \n",
    "   #datacasted=[tf.cast(data['X_val'],tf.float32),tf.cast(data['y_val'],tf.float32)]\n",
    "   valzero=alphamodelzero.test_step(dataalpha_val_dict)\n",
    "   valraw=alphamodel.test_step(dataalpha_val_dict)\n",
    "   valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   valraw = {key: value.numpy() for key, value in valraw.items()}\n",
    "   \n",
    "   training_historyAlpha={'transition_loss': [10**(-8)],'laplacian_loss': [1000000000000000]}\n",
    "   i=0\n",
    "   newLR=lRate\n",
    "   #while (training_historyAlpha['transition_loss'][-1]<10**(-5)) or (training_historyAlpha['laplacian_loss'][-1]>1.):\n",
    "   # continue looping if >10 or is nan\n",
    "   while i==0:#(training_historyAlpha['laplacian_loss'][-1]>10000000000000.) or (np.isnan( training_historyAlpha['laplacian_loss'][-1])):\n",
    "      print(\"trying iteration of training \"+str(i))\n",
    "      if i >0:\n",
    "\n",
    "         print('trying again laplacian_loss too big')\n",
    "         #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.2)\n",
    "         #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network,kernel_initializer=initializer)#note we don't need a last bias (flat direction)\n",
    "         #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         #nn_alpha = BiholoModelFuncGENERALforAlphainv2(shapeofnetwork,BASIS,activation=tfk.activations.gelu,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         #nn_alpha = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=False)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)#note we don't need a last bias (flat direction)\n",
    "         if newLR>0.0002:\n",
    "             newLR=newLR/2\n",
    "             print(\"new LR \" + str(newLR))\n",
    "         opt = tfk.optimizers.legacy.Adam(learning_rate=newLR)\n",
    "         alphamodel= AlphaPrimeModel(nn_alpha,BASIS, phimodel,alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "         cb_list, cmetrics = getcallbacksandmetricsAlpha(dataalpha)\n",
    "         alphamodel.compile(custom_metrics=cmetrics)\n",
    "      alphamodel, training_historyAlpha= train_modelalpha(alphamodel, dataalpha_train, optimizer=opt, epochs=nEpochs, batch_sizes=bSizes, \n",
    "                                        verbose=1, custom_metrics=cmetrics, callbacks=cb_list)\n",
    "      i+=1\n",
    "   print(\"finished training\\n\")\n",
    "   #alphamodel.model.save(os.path.join(dirnameAlpha, name))\n",
    "   phimodel.model.save_weights(os.path.join(dirname, name) + '.weights.h5')\n",
    "   np.savez_compressed(os.path.join(dirnameAlpha, 'trainingHistory-' + name),training_historyAlpha)\n",
    "   valfinal =alphamodel.test_step(dataalpha_val_dict)\n",
    "   valfinal = {key: value.numpy() for key, value in valfinal.items()}\n",
    "   #return training_historyAlpha\n",
    "   #now print the initial losses and final losses for each metric, by taking the first element of each key in the dictionary\n",
    "   #first_metrics = {key: value[0] for key, value in training_historyAlpha.items()}\n",
    "   #last_metrics = {key: value[-1] for key, value in training_historyAlpha.items()}\n",
    "\n",
    "   #print(\"initial losses\")\n",
    "   #print(first_metrics)\n",
    "   #print(\"final losses\")\n",
    "   #print(last_metrics)\n",
    "\n",
    "\n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for raw network: \")\n",
    "   print(valraw)\n",
    "   print(\"validation loss for final network: \")\n",
    "   print(valfinal)\n",
    "   print(\"ratio of final to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valfinal.items()}))\n",
    "   print(\"ratio of final to raw: \" + str({key + \" ratio\": value/(valraw[key]+1e-8) for key, value in valfinal.items()}))\n",
    "\n",
    "\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(alphamodel,tf.cast(dataalpha[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   meanfailuretosolveequation,_,_=HYM_measure_val(alphamodel,dataalpha)\n",
    "   print(\"mean of difference/mean of absolute value of source, weighted by sqrt(g): \" + str(meanfailuretosolveequation))\n",
    "   print(\"\\n\\n\")\n",
    "   tf.keras.backend.clear_session()\n",
    "   return alphamodel,training_historyAlpha\n",
    "\n",
    "def load_nn_Alpha(free_coefficient,phimodel,euler_char,alphaprime,nlayer=3,nHidden=128,nEpochs=30,bSizes=[192,50000],stddev=0.1,lRate=0.001,alpha=[1,1],set_weights_to_zero=False):\n",
    "   dirnameForMetric = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   dirnameAlpha= 'dataAlphaP/'+nameofmanifold+ \"Alpha_pg_with_\" + str(free_coefficient) \n",
    "\n",
    "   name = 'alphamodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(nlayer) + 'x' +str(nHidden)\n",
    "   print(\"name: \" + name)\n",
    "\n",
    "   #data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirnameForMetric, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "\n",
    "   dataalpha = np.load(os.path.join(dirnameAlpha, 'dataset.npz'))\n",
    "   dataalpha_train=tf.data.Dataset.from_tensor_slices(dict(list(dict(dataalpha).items())[:len(dict(dataalpha))//2]))\n",
    "   dataalpha_val_dict=dict(list(dict(dataalpha).items())[len(dict(dataalpha))//2:])\n",
    "   dataalpha_val=tf.data.Dataset.from_tensor_slices(dataalpha_val_dict)\n",
    "   # batch_sizes=[64,10000]\n",
    "   dataalpha_train=dataalpha_train.shuffle(buffer_size=1024).batch(bSizes[0])\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetricsAlpha(dataalpha)\n",
    "\n",
    "   #nlayer = 3\n",
    "   #nHidden = 128\n",
    "   act = 'gelu'\n",
    "   #nEpochs = 30\n",
    "   #bSizes = [192, 150000]\n",
    "   alpha = [1., 1.] # 1 AND 3??\n",
    "   nfold = 3\n",
    "   n_in = 2*8\n",
    "   n_out = 1\n",
    "   #lRate = 0.001\n",
    "   ambient=tf.cast(BASIS['AMBIENT'],tf.int32)\n",
    "\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=stddev)\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   activ=tf.square\n",
    "   nn_alpha = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,stddev=stddev,use_zero_network=set_weights_to_zero)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_alpha_zero = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #copie from phi above\n",
    "   #nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "   \n",
    "   #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)#note we don't need a last bias (flat direction)\n",
    "   #nn_alpha_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)#note we don't need a last bias (flat direction)\n",
    "   alphamodel= AlphaPrimeModel(nn_alpha,BASIS, phimodel,alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "   alphamodelzero= AlphaPrimeModel(nn_alpha_zero,BASIS,phimodel, alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "\n",
    "   alphamodel(dataalpha['X_train'][0:1])\n",
    "   alphamodelzero(dataalpha['X_train'][0:1])\n",
    "\n",
    "\n",
    "   if set_weights_to_zero:\n",
    "      training_historyAlpha=0\n",
    "   else:\n",
    "      #alphamodel.model=tf.keras.models.load_model(os.path.join(dirnameAlpha,name))\n",
    "      alphamodel.model.load_weights(os.path.join(dirnameAlpha, name) + '.weights.h5')\n",
    "      training_historyAlpha=np.load(os.path.join(dirnameAlpha, 'trainingHistory-' + name +'.npz'),allow_pickle=True)['arr_0'].item()\n",
    "\n",
    "   alphamodel.compile(custom_metrics=cmetrics)\n",
    "   alphamodelzero.compile(custom_metrics=cmetrics)\n",
    "   \n",
    "   valzero=alphamodelzero.evaluate(dataalpha_val_dict,return_dict=True)\n",
    "   valtrained=alphamodel.evaluate(dataalpha_val_dict,return_dict=True)\n",
    "   #valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   #valtrained= {key: value.numpy() for key, value in valtrained.items()}\n",
    "\n",
    "\n",
    "   #metricsnames=alphamodel.metrics_names\n",
    "\n",
    "   #valtrained= {metricsnames[i]: valtrained[i] for i in range(len(valtrained))}\n",
    "   ##valzero = {metricsnames[i]: valzero[i] for i in range(len(valzero))}\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for trained network: \")\n",
    "   print(valtrained)\n",
    "   print(\"ratio of trained to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valtrained.items()}))\n",
    "\n",
    "\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(alphamodel,tf.cast(dataalpha[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   meanfailuretosolveequation,_,_=HYM_measure_val(alphamodel,dataalpha)\n",
    "   print(\"mean of difference/mean of absolute value of source, weighted by sqrt(g): \" + str(meanfailuretosolveequation))\n",
    "   print(\"\\n\\n\")\n",
    "   return alphamodel,training_historyAlpha\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d277c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cymetricTESTING217",
   "language": "python",
   "name": "cymetrictesting217"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
