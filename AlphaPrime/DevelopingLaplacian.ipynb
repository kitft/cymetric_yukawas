{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddabf73c",
   "metadata": {},
   "source": [
    "# Training a NN for metric on CICY with homog\n",
    "\n",
    "## Import the required packages/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5790c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#disable GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a9cc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 14:25:10.668634: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-12 14:25:14.837995: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-08-12 14:25:14.838031: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: tplxdt127\n",
      "2024-08-12 14:25:14.838040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: tplxdt127\n",
      "2024-08-12 14:25:14.838106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 555.42.6\n",
      "2024-08-12 14:25:14.838131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 555.42.6\n",
      "2024-08-12 14:25:14.838137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 555.42.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import pickle\n",
    "import sys\n",
    "#sys.path.append(\"/Users/kit/Documents/Phys_Working/MF metric\")\n",
    "#sys.path.append(\"/home/f/fraser-talientec/PhysicalYukawas\")\n",
    "sys.path.append(\"/home/f/fraser-talientec/cymetricap/AlphaPrime\")\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout)\n",
    "\n",
    "from cymetric.pointgen.pointgen import PointGenerator\n",
    "from cymetric.pointgen.nphelper import prepare_dataset, prepare_basis_pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "from cymetric.models.tfmodels import PhiFSModel, MultFSModel, FreeModel, MatrixFSModel, AddFSModel, PhiFSModelToric, MatrixFSModelToric\n",
    "from cymetric.models.tfhelper import prepare_tf_basis, train_model\n",
    "from cymetric.models.callbacks import SigmaCallback, KaehlerCallback, TransitionCallback, RicciCallback, VolkCallback, AlphaCallback\n",
    "from cymetric.models.metrics import SigmaLoss, KaehlerLoss, TransitionLoss, RicciLoss, VolkLoss, TotalLoss\n",
    "\n",
    "#from NewCustomMetrics import *\n",
    "from laplacian_funcs import *\n",
    "#from generate_and_train_all_nnsHOLO import *\n",
    "from custom_networks import *\n",
    "import sys\n",
    "import importlib\n",
    "from AlphaPrimeModel import *\n",
    "#reimport ALphaPrimeModel\n",
    "importlib.reload(sys.modules['AlphaPrimeModel'])\n",
    "from AlphaPrimeModel import *\n",
    "#reimport custom_networks\n",
    "importlib.reload(sys.modules['custom_networks'])\n",
    "from custom_networks import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d00ecc1-f6f4-47d4-a2bb-f7830e164093",
   "metadata": {},
   "source": [
    "## Point Cloud Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ba80cc",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d1dd93a",
   "metadata": {},
   "source": [
    "Set the properties of the defining polynomial. And the point in Kahler Moduli space\n",
    "\n",
    "If correct, this should be for the following defining polynomial\n",
    "$0.44 x_{1,0}^2 x_{3,0}^2 x_{4,0}^2 x_{2,0}^2+0.88 x_{1,1}^2 x_{3,0}^2 x_{4,0}^2 x_{2,0}^2+0.88 x_{1,0}^2 x_{3,1}^2 x_{4,0}^2 x_{2,0}^2+0.88 x_{1,1}^2 x_{3,1}^2 x_{4,0}^2 x_{2,0}^2-0.03 x_{1,0} x_{1,1} x_{3,0} x_{3,1} x_{4,0}^2 x_{2,0}^2+0.44 x_{1,0}^2 x_{3,0}^2 x_{4,1}^2 x_{2,0}^2+0.44 x_{1,1}^2 x_{3,0}^2 x_{4,1}^2 x_{2,0}^2+0.88 x_{1,0}^2 x_{3,1}^2 x_{4,1}^2 x_{2,0}^2+0.44 x_{1,1}^2 x_{3,1}^2 x_{4,1}^2 x_{2,0}^2-0.41 x_{1,0} x_{1,1} x_{3,0} x_{3,1} x_{4,1}^2 x_{2,0}^2-0.41 x_{1,0} x_{1,1} x_{3,0}^2 x_{4,0} x_{4,1} x_{2,0}^2-0.03 x_{1,0} x_{1,1} x_{3,1}^2 x_{4,0} x_{4,1} x_{2,0}^2+0.62 x_{1,0}^2 x_{3,0} x_{3,1} x_{4,0} x_{4,1} x_{2,0}^2+0.62 x_{1,1}^2 x_{3,0} x_{3,1} x_{4,0} x_{4,1} x_{2,0}^2-0.62 x_{1,0} x_{1,1} x_{2,1} x_{3,0}^2 x_{4,0}^2 x_{2,0}-0.62 x_{1,0} x_{1,1} x_{2,1} x_{3,1}^2 x_{4,0}^2 x_{2,0}+0.41 x_{1,0}^2 x_{2,1} x_{3,0} x_{3,1} x_{4,0}^2 x_{2,0}+0.03 x_{1,1}^2 x_{2,1} x_{3,0} x_{3,1} x_{4,0}^2 x_{2,0}-0.62 x_{1,0} x_{1,1} x_{2,1} x_{3,0}^2 x_{4,1}^2 x_{2,0}-0.62 x_{1,0} x_{1,1} x_{2,1} x_{3,1}^2 x_{4,1}^2 x_{2,0}+0.03 x_{1,0}^2 x_{2,1} x_{3,0} x_{3,1} x_{4,1}^2 x_{2,0}+0.41 x_{1,1}^2 x_{2,1} x_{3,0} x_{3,1} x_{4,1}^2 x_{2,0}+0.41 x_{1,0}^2 x_{2,1} x_{3,0}^2 x_{4,0} x_{4,1} x_{2,0}+0.03 x_{1,1}^2 x_{2,1} x_{3,0}^2 x_{4,0} x_{4,1} x_{2,0}+0.03 x_{1,0}^2 x_{2,1} x_{3,1}^2 x_{4,0} x_{4,1} x_{2,0}+0.41 x_{1,1}^2 x_{2,1} x_{3,1}^2 x_{4,0} x_{4,1} x_{2,0}+0.9 x_{1,0} x_{1,1} x_{2,1} x_{3,0} x_{3,1} x_{4,0} x_{4,1} x_{2,0}+0.44 x_{1,0}^2 x_{2,1}^2 x_{3,0}^2 x_{4,0}^2+0.88 x_{1,1}^2 x_{2,1}^2 x_{3,0}^2 x_{4,0}^2+0.44 x_{1,0}^2 x_{2,1}^2 x_{3,1}^2 x_{4,0}^2+0.44 x_{1,1}^2 x_{2,1}^2 x_{3,1}^2 x_{4,0}^2-0.41 x_{1,0} x_{1,1} x_{2,1}^2 x_{3,0} x_{3,1} x_{4,0}^2+0.88 x_{1,0}^2 x_{2,1}^2 x_{3,0}^2 x_{4,1}^2+0.88 x_{1,1}^2 x_{2,1}^2 x_{3,0}^2 x_{4,1}^2+0.88 x_{1,0}^2 x_{2,1}^2 x_{3,1}^2 x_{4,1}^2+0.44 x_{1,1}^2 x_{2,1}^2 x_{3,1}^2 x_{4,1}^2-0.03 x_{1,0} x_{1,1} x_{2,1}^2 x_{3,0} x_{3,1} x_{4,1}^2-0.03 x_{1,0} x_{1,1} x_{2,1}^2 x_{3,0}^2 x_{4,0} x_{4,1}-0.41 x_{1,0} x_{1,1} x_{2,1}^2 x_{3,1}^2 x_{4,0} x_{4,1}+0.62 x_{1,0}^2 x_{2,1}^2 x_{3,0} x_{3,1} x_{4,0} x_{4,1}+0.62 x_{1,1}^2 x_{2,1}^2 x_{3,0} x_{3,1} x_{4,0} x_{4,1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe185432",
   "metadata": {},
   "outputs": [],
   "source": [
    "monomialsTQ = 5*np.eye(5, dtype=np.int64)\n",
    "coefficientsTQ = np.ones(5)\n",
    "kmoduliTQ = np.ones(1)\n",
    "ambientTQ = np.array([4])\n",
    "nameofmanifold=\"Quintic\"\n",
    "\n",
    "\n",
    "def generate_points_and_save_using_defaults(free_coefficient,number_points,force_generate=False,seed_set=0):\n",
    "   coefficients=coefficientsTQ\n",
    "   # coefficients=np.array([1, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, \\\n",
    "   # 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, free_coefficient, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 1, 0, 2, 0, 0, 0, 2, 0, 1])\n",
    "   monomials=monomialsTQ\n",
    "   kmoduli=kmoduliTQ\n",
    "   ambient=ambientTQ\n",
    "   # monomials = 5*np.eye(5, dtype=np.int64)\n",
    "   # coefficients = np.ones(5)\n",
    "   \n",
    "   # kmoduli = np.ones(1)\n",
    "   # ambient = np.array([4])\n",
    "   pg = PointGenerator(monomials, coefficients, kmoduli, ambient)\n",
    "   pg._set_seed(seed_set)\n",
    "   dirname = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   print(\"dirname: \" + dirname)\n",
    "   #test if the directory exists, if not, create it\n",
    "   if force_generate or (not os.path.exists(dirname)):\n",
    "      print(\"Generating: forced? \" + str(force_generate))\n",
    "      kappa = pg.prepare_dataset(number_points, dirname)\n",
    "      pg.prepare_basis(dirname, kappa=kappa)\n",
    "   elif os.path.exists(dirname):\n",
    "      try:\n",
    "         print(\"loading prexisting dataset\")\n",
    "         data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "         if (len(data['X_train'])+len(data['X_val']))!=number_points:\n",
    "            print(\"wrong length - generating anyway\")\n",
    "            kappa = pg.prepare_dataset(number_points, dirname)\n",
    "            pg.prepare_basis(dirname, kappa=kappa)\n",
    "      except:\n",
    "         print(\"error loading - generating anyway\")\n",
    "         kappa = pg.prepare_dataset(number_points, dirname)\n",
    "         pg.prepare_basis(dirname, kappa=kappa)\n",
    "   \n",
    "\n",
    "def getcallbacksandmetrics(data):\n",
    "   #rcb = RicciCallback((data['X_val'], data['y_val']), data['val_pullbacks'])\n",
    "   scb = SigmaCallback((data['X_val'], data['y_val']))\n",
    "   volkcb = VolkCallback((data['X_val'], data['y_val']))\n",
    "   kcb = KaehlerCallback((data['X_val'], data['y_val']))\n",
    "   tcb = TransitionCallback((data['X_val'], data['y_val']))\n",
    "   #cb_list = [rcb, scb, kcb, tcb, volkcb]\n",
    "   cb_list = [ scb, kcb, tcb, volkcb]\n",
    "   cmetrics = [TotalLoss(), SigmaLoss(), KaehlerLoss(), TransitionLoss(), VolkLoss()]#, RicciLoss()]\n",
    "   return cb_list, cmetrics\n",
    "\n",
    "def train_and_save_nn(free_coefficient,nlayer=3,nHidden=128,nEpochs=50,stddev=0.1,bSizes=[192,50000],lRate=0.001,use_zero_network=False):\n",
    "   dirname = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   name = 'phimodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(bSizes[1]) + 's' + str(nlayer) + 'x' +str(nHidden)\n",
    "   print('dirname: ' + dirname)\n",
    "   print('name: ' + name)\n",
    "   \n",
    "   data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirname, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetrics(data)\n",
    "\n",
    "   act = 'gelu'\n",
    "   alpha = [1., 1., 30., 1., 2.] # 1 AND 3??\n",
    "   ambient=tf.cast(tf.math.abs(BASIS['AMBIENT']),tf.int32)\n",
    "\n",
    "   #nfirstlayer=tf.reduce_prod(2*(np.array(ambient)+1)).numpy().item()\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   #nn_phi = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_phi=make_nn(10,1,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_phi_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   # print(nn_phi_zero(tf.cast(data['X_val'][0:2],tf.float32)))\n",
    "   nn_phi_zero=make_nn(10,1,nlayer,nHidden,act,use_zero_network=True)\n",
    "   phimodel = PhiFSModel(nn_phi, BASIS, alpha=alpha)\n",
    "   phimodelzero = PhiFSModel(nn_phi_zero, BASIS, alpha=alpha)\n",
    "\n",
    "   #Note, currently running legacy due to ongoing tf issue with M1/M2. \n",
    "   #Use the commented line instead if not on an M1/M2 machine\n",
    "   opt = tfk.optimizers.Adam(learning_rate=lRate)\n",
    "   #opt = tfk.optimizers.legacy.Adam(learning_rate=lRate)\n",
    "   # compile so we can test on validation set before training\n",
    "   phimodel.compile(custom_metrics=cmetrics)\n",
    "   phimodelzero.compile(custom_metrics=cmetrics)\n",
    "\n",
    "   ## compare validation loss before training for zero network and nonzero network\n",
    "   datacasted=[tf.cast(data['X_val'],tf.float32),tf.cast(data['y_val'],tf.float32)]\n",
    "   #need to re-enable learning, in case there's been a problem:\n",
    "   phimodel.learn_transition = False\n",
    "   phimodelzero.learn_transition = False\n",
    "   phimodel.learn_volk = True\n",
    "   phimodelzero.learn_volk = True\n",
    "   #phimodel.learn_ricci_val= True\n",
    "   #phimodelzero.learn_ricci_val= True\n",
    "   valzero=phimodelzero.test_step(datacasted)\n",
    "   valraw=phimodel.test_step(datacasted)\n",
    "   # phimodel.learn_ricci_val=False \n",
    "   # phimodelzero.learn_ricci_val=False \n",
    "   valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   valraw = {key: value.numpy() for key, value in valraw.items()}\n",
    "\n",
    "   phimodel, training_history = train_model(phimodel, data, optimizer=opt, epochs=nEpochs, batch_sizes=bSizes, \n",
    "                                       verbose=1, custom_metrics=cmetrics, callbacks=cb_list)\n",
    "   return phimodel,training_history\n",
    "   print(\"finished training\\n\")\n",
    "   phimodel.model.save(os.path.join(dirname, name))\n",
    "   np.savez_compressed(os.path.join(dirname, 'trainingHistory-' + name),training_history)\n",
    "   #now print the initial losses and final losses for each metric\n",
    "   # first_metrics = {key: value[0] for key, value in training_history.items()}\n",
    "   # lastometrics = {key: value[-1] for key, value in training_history.items()}\n",
    "   phimodel.learn_transition = True\n",
    "   phimodel.learn_volk = True\n",
    "   #phimodel.learn_ricci_val= True\n",
    "   valfinal=phimodel.test_step(datacasted)\n",
    "   valfinal = {key: value.numpy() for key, value in valfinal.items()}\n",
    "   #phimodel.learn_ricci_val=False \n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for raw network: \")\n",
    "   print(valraw)\n",
    "   print(\"validation loss for final network: \")\n",
    "   print(valfinal)\n",
    "   print(\"ratio of final to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valfinal.items()}))\n",
    "   print(\"ratio of final to raw: \" + str({key + \" ratio\": value/(valraw[key]+1e-8) for key, value in valfinal.items()}))\n",
    "\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(phimodel,tf.cast(data[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   #IMPLEMENT THE FOLLOWING\n",
    "   #meanfailuretosolveequation,_,_=measure_laplacian_failure(phimodel,data)\n",
    "   print(\"\\n\\n\")\n",
    "   return phimodel,training_history\n",
    "\n",
    "def load_nn_phimodel(free_coefficient,nlayer=3,nHidden=128,nEpochs=50,bSizes=[192,50000],stddev=0.1,lRate=0.001,set_weights_to_zero=False):\n",
    "   dirname = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   name = 'phimodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(bSizes[1]) + 's' + str(nlayer) + 'x' +str(nHidden)\n",
    "   print(dirname)\n",
    "   print(name)\n",
    "   \n",
    "   data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirname, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetrics(data)\n",
    "\n",
    "\n",
    "   act = 'gelu'\n",
    "\n",
    "   alpha = [1., 1., 30., 1., 2.] # 1 AND 3??\n",
    " \n",
    "\n",
    "\n",
    "   ambient=tf.cast(BASIS['AMBIENT'],tf.int32)\n",
    "\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   nn_phi = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_phi_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "\n",
    "\n",
    "#    nn_phi = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "#    nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "   phimodel = PhiFSModel(nn_phi, BASIS, alpha=alpha)\n",
    "   phimodelzero = PhiFSModel(nn_phi_zero, BASIS, alpha=alpha)\n",
    "\n",
    "   if set_weights_to_zero:\n",
    "      training_history=0\n",
    "   else:\n",
    "      phimodel.model=tf.keras.models.load_model(os.path.join(dirname,name))\n",
    "      training_history=np.load(os.path.join(dirname, 'trainingHistory-' + name +'.npz'),allow_pickle=True)['arr_0'].item()\n",
    "\n",
    "   phimodel.compile(custom_metrics=cmetrics)\n",
    "   phimodelzero.compile(custom_metrics=cmetrics)\n",
    "\n",
    "   # compare validation loss before training for zero network and nonzero network\n",
    "   datacasted=[tf.cast(data['X_val'],tf.float32),tf.cast(data['y_val'],tf.float32)]\n",
    "   #need to re-enable learning, in case there's been a problem:\n",
    "   phimodel.learn_transition = True\n",
    "   phimodelzero.learn_transition = True\n",
    "   phimodel.learn_volk = True\n",
    "   phimodelzero.learn_volk = True\n",
    "   #phimodel.learn_ricci_val= True\n",
    "   #phimodelzero.learn_ricci_val= True\n",
    "   valzero=phimodelzero.evaluate(datacasted[0],datacasted[1],return_dict=True)\n",
    "   valtrained=phimodel.evaluate(datacasted[0],datacasted[1],return_dict=True)\n",
    "   #metricsnames=phimodel.metrics_names\n",
    "   # phimodel.learn_ricci_val=False \n",
    "   # phimodelzero.learn_ricci_val=False \n",
    "   #valzero = {metricsnames[i]: valzero[i] for i in range(len(valzero))}\n",
    "   #valtrained= {metricsnames[i]: valtrained[i] for i in range(len(valtrained))}\n",
    "\n",
    "   #valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   #valtrained = {key: value.numpy() for key, value in valtrained.items()}\n",
    "\n",
    "   #valtrained = {key: value.numpy() for key, value in valtrained.items()}\n",
    "\n",
    "   phimodel.learn_transition = True\n",
    "   phimodel.learn_volk = True\n",
    "\n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for final network: \")\n",
    "   print(valtrained)\n",
    "   print(\"ratio of trained to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valtrained.items()}))\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(phimodel,tf.cast(data[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   print(\"\\n\\n\")\n",
    "   #IMPLEMENT THE FOLLOWING\n",
    "   #meanfailuretosolveequation,_,_=measure_laplacian_failure(phimodel,data)\n",
    "   #print(\"\\n\\n\")\n",
    "   #print(\"mean of difference/mean of absolute value of source, weighted by sqrt(g): \" + str(meanfailuretosolveequation))\n",
    "   return phimodel,training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_points_and_save_using_defaultsAlpha(free_coefficient,phimodel,euler_char,force_generate=False,seed_set=0):\n",
    "   coefficients=coefficientsTQ\n",
    "   # coefficients=np.array([1, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, \\\n",
    "   # 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, free_coefficient, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 1, 0, 2, 0, 0, 0, 2, 0, 1])\n",
    "   monomials=monomialsTQ\n",
    "   kmoduli=kmoduliTQ\n",
    "   ambient=ambientTQ\n",
    "   # monomials = 5*np.eye(5, dtype=np.int64)\n",
    "   # coefficients = np.ones(5)\n",
    "   \n",
    "   # kmoduli = np.ones(1)\n",
    "   # ambient = np.array([4])\n",
    "   pg = PointGenerator(monomials, coefficients, kmoduli, ambient)\n",
    "   pg._set_seed(seed_set)\n",
    "\n",
    "\n",
    "   dirnameForMetric = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   dirnameAlpha= 'dataAlphaP/'+nameofmanifold+ \"Alpha_pg_with_\" + str(free_coefficient) \n",
    "   #dirnameAlpha = 'dataAlphaP/tetraquadricAlpha_pg_with_'+str(free_coefficient)+'forLB_'+lbstring\n",
    "   #dirnameForMetric = 'dataAlphaP/tetraquadric_pg_with_'+str(free_coefficient)\n",
    "   print(\"dirname for alpha: \" + dirnameForMetric)\n",
    "   print(\"dirname for alpha: \" + dirnameAlpha)\n",
    "\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirnameForMetric, 'basis.pickle'), allow_pickle=True))\n",
    "   \n",
    "   data=np.load(os.path.join(dirnameForMetric, 'dataset.npz'))\n",
    "\n",
    "   if force_generate or (not os.path.exists(dirnameAlpha)):\n",
    "      print(\"Generating: forced? \" + str(force_generate))\n",
    "      kappaAlpha = prepare_dataset_Alpha(pg,data,dirnameAlpha,phimodel,euler_char,BASIS,normalize_to_vol_j=True);\n",
    "   elif os.path.exists(dirnameAlpha):\n",
    "      try:\n",
    "         print(\"loading prexisting dataset\")\n",
    "         data = np.load(os.path.join(dirnameAlpha, 'dataset.npz'))\n",
    "      except:\n",
    "         print(\"problem loading data - generating anyway\")\n",
    "         kappaAlpha = prepare_dataset_Alpha(pg,data, dirnameAlpha,phimodel,euler_char,BASIS,normalize_to_vol_j=True);\n",
    "      \n",
    "   \n",
    "\n",
    "def getcallbacksandmetricsAlpha(dataalpha):\n",
    "   dataalpha_val_dict=dict(list(dict(dataalpha).items())[len(dict(dataalpha))//2:])\n",
    "   tcb = TransitionCallback((dataalpha['X_val'], dataalpha['y_val']))\n",
    "   lplcb = LaplacianCallback(dataalpha_val_dict)\n",
    "   # lplcb = LaplacianCallback(data_val)\n",
    "   cb_list = [lplcb,tcb]\n",
    "   cmetrics = [TotalLoss(), LaplacianLoss(), TransitionLoss()]\n",
    "   return cb_list, cmetrics\n",
    "\n",
    "   \n",
    "def train_and_save_nn_Alpha(free_coefficient,phimodel,euler_char,alphaprime,nlayer=3,nHidden=128,nEpochs=30,bSizes=[192,50000],stddev=0.1,lRate=0.001,alpha=[1,1],load_network=False,use_zero_network=False):\n",
    "   \n",
    "   dirnameForMetric = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   dirnameAlpha= 'dataAlphaP/'+nameofmanifold+ \"Alpha_pg_with_\" + str(free_coefficient) \n",
    "\n",
    "   #data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirnameForMetric, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "\n",
    "   dataalpha = np.load(os.path.join(dirnameAlpha, 'dataset.npz'))\n",
    "   dataalpha_train=tf.data.Dataset.from_tensor_slices(dict(list(dict(dataalpha).items())[:len(dict(dataalpha))//2]))\n",
    "   dataalpha_val_dict=dict(list(dict(dataalpha).items())[len(dict(dataalpha))//2:])\n",
    "   dataalpha_val=tf.data.Dataset.from_tensor_slices(dataalpha_val_dict)\n",
    "   # batch_sizes=[64,10000]\n",
    "   dataalpha_train=dataalpha_train.shuffle(buffer_size=1024).batch(bSizes[0])\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetricsAlpha(dataalpha)\n",
    "\n",
    "   #nlayer = 3\n",
    "   #nHidden = 128\n",
    "   act = 'gelu'\n",
    "   #nEpochs = 30\n",
    "   #bSizes = [192, 150000]\n",
    "   #alpha = [1., 1.] # 1 AND 3??\n",
    "   nfold = 3\n",
    "   n_in = 2*8\n",
    "   n_out = 1\n",
    "   #lRate = 0.001\n",
    "   name = 'alphamodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(nlayer) + 'x' +str(nHidden)\n",
    "   print(\"name: \" + name)\n",
    "\n",
    "   ambient=tf.cast(BASIS['AMBIENT'],tf.int32)\n",
    "\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   #nfirstlayer=tf.reduce_sum(((np.array(ambient)+1)**2)).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=stddev)\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   activ=tf.square\n",
    "   #activ=tfk.activations.gelu\n",
    "   #nn_alpha = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #copie from phi above\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "\n",
    "   #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.2)\n",
    "   #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)#note we don't need a last bias (flat direction)\n",
    "   #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network,kernel_initializer=initializer)#note we don't need a last bias (flat direction)\n",
    "   #nn_alpha_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)#note we don't need a last bias (flat direction)\n",
    "   \n",
    "   alphamodel= AlphaPrimeModel(nn_alpha,BASIS, phimodel,alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "   if load_network:\n",
    "      print(\"loading network\")\n",
    "      alphamodel.model=tf.keras.models.load_model(os.path.join(dirnameAlpha,name))\n",
    "      print(\"network loaded\")\n",
    "\n",
    "   alphamodelzero= AlphaPrimeModel(nn_alpha_zero,BASIS,phimodel, alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "\n",
    "   #Note, currently running legacy due to ongoing tf issue with M1/M2. \n",
    "   #Use the commented line instead if not on an M1/M2 machine\n",
    "   opt = tfk.optimizers.Adam(learning_rate=lRate)\n",
    "   #opt = tfk.optimizers.legacy.Adam(learning_rate=lRate)\n",
    "   # compile so we can test on validation set before training\n",
    "   alphamodel.compile(custom_metrics=cmetrics)\n",
    "   alphamodelzero.compile(custom_metrics=cmetrics)\n",
    "   \n",
    "   #datacasted=[tf.cast(data['X_val'],tf.float32),tf.cast(data['y_val'],tf.float32)]\n",
    "   valzero=alphamodelzero.test_step(dataalpha_val_dict)\n",
    "   valraw=alphamodel.test_step(dataalpha_val_dict)\n",
    "   valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   valraw = {key: value.numpy() for key, value in valraw.items()}\n",
    "   \n",
    "   training_historyAlpha={'transition_loss': [10**(-8)],'laplacian_loss': [1000000000000000]}\n",
    "   i=0\n",
    "   newLR=lRate\n",
    "   #while (training_historyAlpha['transition_loss'][-1]<10**(-5)) or (training_historyAlpha['laplacian_loss'][-1]>1.):\n",
    "   # continue looping if >10 or is nan\n",
    "   while i==0:#(training_historyAlpha['laplacian_loss'][-1]>10000000000000.) or (np.isnan( training_historyAlpha['laplacian_loss'][-1])):\n",
    "      print(\"trying iteration of training \"+str(i))\n",
    "      if i >0:\n",
    "\n",
    "         print('trying again laplacian_loss too big')\n",
    "         #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.2)\n",
    "         #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network,kernel_initializer=initializer)#note we don't need a last bias (flat direction)\n",
    "         #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         #nn_alpha = BiholoModelFuncGENERALforAlphainv2(shapeofnetwork,BASIS,activation=tfk.activations.gelu,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         #nn_alpha = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=False)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)#note we don't need a last bias (flat direction)\n",
    "         if newLR>0.0002:\n",
    "             newLR=newLR/2\n",
    "             print(\"new LR \" + str(newLR))\n",
    "         opt = tfk.optimizers.legacy.Adam(learning_rate=newLR)\n",
    "         alphamodel= AlphaPrimeModel(nn_alpha,BASIS, phimodel,alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "         cb_list, cmetrics = getcallbacksandmetricsAlpha(dataalpha)\n",
    "         alphamodel.compile(custom_metrics=cmetrics)\n",
    "      alphamodel, training_historyAlpha= train_modelalpha(alphamodel, dataalpha_train, optimizer=opt, epochs=nEpochs, batch_sizes=bSizes, \n",
    "                                        verbose=1, custom_metrics=cmetrics, callbacks=cb_list)\n",
    "      i+=1\n",
    "   print(\"finished training\\n\")\n",
    "   alphamodel.model.save(os.path.join(dirnameAlpha, name))\n",
    "   np.savez_compressed(os.path.join(dirnameAlpha, 'trainingHistory-' + name),training_historyAlpha)\n",
    "   valfinal =alphamodel.test_step(dataalpha_val_dict)\n",
    "   valfinal = {key: value.numpy() for key, value in valfinal.items()}\n",
    "   #return training_historyAlpha\n",
    "   #now print the initial losses and final losses for each metric, by taking the first element of each key in the dictionary\n",
    "   #first_metrics = {key: value[0] for key, value in training_historyAlpha.items()}\n",
    "   #last_metrics = {key: value[-1] for key, value in training_historyAlpha.items()}\n",
    "\n",
    "   #print(\"initial losses\")\n",
    "   #print(first_metrics)\n",
    "   #print(\"final losses\")\n",
    "   #print(last_metrics)\n",
    "\n",
    "\n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for raw network: \")\n",
    "   print(valraw)\n",
    "   print(\"validation loss for final network: \")\n",
    "   print(valfinal)\n",
    "   print(\"ratio of final to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valfinal.items()}))\n",
    "   print(\"ratio of final to raw: \" + str({key + \" ratio\": value/(valraw[key]+1e-8) for key, value in valfinal.items()}))\n",
    "\n",
    "\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(alphamodel,tf.cast(dataalpha[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   meanfailuretosolveequation,_,_=HYM_measure_val(alphamodel,dataalpha)\n",
    "   print(\"mean of difference/mean of absolute value of source, weighted by sqrt(g): \" + str(meanfailuretosolveequation))\n",
    "   print(\"\\n\\n\")\n",
    "   tf.keras.backend.clear_session()\n",
    "   return alphamodel,training_historyAlpha\n",
    "\n",
    "def load_nn_Alpha(free_coefficient,phimodel,euler_char,alphaprime,nlayer=3,nHidden=128,nEpochs=30,bSizes=[192,50000],stddev=0.1,lRate=0.001,alpha=[1,1],set_weights_to_zero=False):\n",
    "   dirnameForMetric = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   dirnameAlpha= 'dataAlphaP/'+nameofmanifold+ \"Alpha_pg_with_\" + str(free_coefficient) \n",
    "\n",
    "   name = 'alphamodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(nlayer) + 'x' +str(nHidden)\n",
    "   print(\"name: \" + name)\n",
    "\n",
    "   #data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirnameForMetric, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "\n",
    "   dataalpha = np.load(os.path.join(dirnameAlpha, 'dataset.npz'))\n",
    "   dataalpha_train=tf.data.Dataset.from_tensor_slices(dict(list(dict(dataalpha).items())[:len(dict(dataalpha))//2]))\n",
    "   dataalpha_val_dict=dict(list(dict(dataalpha).items())[len(dict(dataalpha))//2:])\n",
    "   dataalpha_val=tf.data.Dataset.from_tensor_slices(dataalpha_val_dict)\n",
    "   # batch_sizes=[64,10000]\n",
    "   dataalpha_train=dataalpha_train.shuffle(buffer_size=1024).batch(bSizes[0])\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetricsAlpha(dataalpha)\n",
    "\n",
    "   #nlayer = 3\n",
    "   #nHidden = 128\n",
    "   act = 'gelu'\n",
    "   #nEpochs = 30\n",
    "   #bSizes = [192, 150000]\n",
    "   alpha = [1., 1.] # 1 AND 3??\n",
    "   nfold = 3\n",
    "   n_in = 2*8\n",
    "   n_out = 1\n",
    "   #lRate = 0.001\n",
    "   ambient=tf.cast(BASIS['AMBIENT'],tf.int32)\n",
    "\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=stddev)\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   activ=tf.square\n",
    "   nn_alpha = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,stddev=stddev,use_zero_network=set_weights_to_zero)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_alpha_zero = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #copie from phi above\n",
    "   #nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "   \n",
    "   #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)#note we don't need a last bias (flat direction)\n",
    "   #nn_alpha_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)#note we don't need a last bias (flat direction)\n",
    "   alphamodel= AlphaPrimeModel(nn_alpha,BASIS, phimodel,alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "   alphamodelzero= AlphaPrimeModel(nn_alpha_zero,BASIS,phimodel, alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "\n",
    "   alphamodel(dataalpha['X_train'][0:1])\n",
    "   alphamodelzero(dataalpha['X_train'][0:1])\n",
    "\n",
    "   if set_weights_to_zero:\n",
    "      training_historyAlpha=0\n",
    "   else:\n",
    "      alphamodel.model=tf.keras.models.load_model(os.path.join(dirnameAlpha,name))\n",
    "      training_historyAlpha=np.load(os.path.join(dirnameAlpha, 'trainingHistory-' + name +'.npz'),allow_pickle=True)['arr_0'].item()\n",
    "\n",
    "   alphamodel.compile(custom_metrics=cmetrics)\n",
    "   alphamodelzero.compile(custom_metrics=cmetrics)\n",
    "   \n",
    "   valzero=alphamodelzero.evaluate(dataalpha_val_dict,return_dict=True)\n",
    "   valtrained=alphamodel.evaluate(dataalpha_val_dict,return_dict=True)\n",
    "   #valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   #valtrained= {key: value.numpy() for key, value in valtrained.items()}\n",
    "\n",
    "\n",
    "   #metricsnames=alphamodel.metrics_names\n",
    "\n",
    "   #valtrained= {metricsnames[i]: valtrained[i] for i in range(len(valtrained))}\n",
    "   ##valzero = {metricsnames[i]: valzero[i] for i in range(len(valzero))}\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for trained network: \")\n",
    "   print(valtrained)\n",
    "   print(\"ratio of trained to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valtrained.items()}))\n",
    "\n",
    "\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(alphamodel,tf.cast(dataalpha[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   meanfailuretosolveequation,_,_=HYM_measure_val(alphamodel,dataalpha)\n",
    "   print(\"mean of difference/mean of absolute value of source, weighted by sqrt(g): \" + str(meanfailuretosolveequation))\n",
    "   print(\"\\n\\n\")\n",
    "   return alphamodel,training_historyAlpha\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb10fbce",
   "metadata": {},
   "source": [
    "Now generate example points with a point generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a2a6fe4",
   "metadata": {},
   "source": [
    "Geneate the point cloud for our NN training - note that this will take a few mins\n",
    "\n",
    "\n",
    "Note that \"free_coefficient\" is just a label for this particular quintic - for the TQ it was psi. Here, it just lets you have different runs not overwrite each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a21f4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirname: dataAlphaP/Quintic_pg_with_1e-14\n",
      "loading prexisting dataset\n"
     ]
    }
   ],
   "source": [
    "monomialsTQ = 5*np.eye(5, dtype=np.int64)\n",
    "coefficientsTQ = np.ones(5)\n",
    "kmoduliTQ = np.ones(1)\n",
    "ambientTQ = np.array([4])\n",
    "nameofmanifold=\"Quintic\"\n",
    "\n",
    "nPoints=500000\n",
    "\n",
    "free_coefficient = 1.9#float(sys.argv[1])\n",
    "free_coefficient=2.342351\n",
    "free_coefficient=2.342343234\n",
    "free_coefficient=0.00000000000001\n",
    "#free_coefficient=1.# when the coefficient is 1, ensure that it's 1., not 1 for the sake of the filename\n",
    "#nEpochsPhi=100\n",
    "nEpochsPhi=10\n",
    "\n",
    "depthPhi=4\n",
    "widthPhi=64#128 4 in the 1.0s\n",
    "\n",
    "\n",
    "train_phi=False\n",
    "generate_points_and_save_using_defaults(free_coefficient,nPoints)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01658c4c",
   "metadata": {},
   "source": [
    "## Training the NN\n",
    "\n",
    "Now we can start preperation for training the NN\n",
    "\n",
    "Begin by loading in the required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f3caf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataAlphaP/Quintic_pg_with_1e-14\n",
      "phimodel_for_10_64_50000s4x64\n",
      "network shape: [25, 64, 64, 64, 64, 1]\n",
      "nns made\n",
      "compiling\n",
      "1563/1563 [==============================] - 6s 2ms/step - loss: 0.4989 - sigma_loss: 0.4989 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 2.6878e-07\n",
      "1563/1563 [==============================] - 6s 2ms/step - loss: 0.5476 - sigma_loss: 0.0022 - kaehler_loss: 0.0000e+00 - transition_loss: 6.9553e-10 - ricci_loss: 0.0000e+00 - volk_loss: 0.2727\n",
      "zero network validation loss: \n",
      "{'loss': 0.5725029110908508, 'sigma_loss': 0.5725029110908508, 'kaehler_loss': 0.0, 'transition_loss': 0.0, 'ricci_loss': 0.0, 'volk_loss': 1.173486197103557e-08}\n",
      "validation loss for final network: \n",
      "{'loss': 0.5150485634803772, 'sigma_loss': 0.0020871683955192566, 'kaehler_loss': 0.0, 'transition_loss': 6.273492281216875e-10, 'ricci_loss': 0.0, 'volk_loss': 0.25648069381713867}\n",
      "ratio of trained to zero: {'loss ratio': 0.8996435555280665, 'sigma_loss ratio': 0.003645690386247029, 'kaehler_loss ratio': 0.0, 'transition_loss ratio': 0.06273492281216875, 'ricci_loss ratio': 0.0, 'volk_loss ratio': 11800428.921928806}\n",
      "average transition discrepancy in standard deviations: tf.Tensor(1.0658346e-06, shape=(), dtype=float32)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# phimodel2,training_history=load_nn_phimodel(free_coefficient,depthPhi,widthPhi,nEpochsPhi,[64,50000],set_weights_to_zero=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd5dbc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataAlphaP/Quintic_pg_with_1e-14\n",
      "phimodel_for_10_64_50000s4x64\n",
      "network shape: [25, 64, 64, 64, 64, 1]\n",
      "nns made\n",
      "compiling\n",
      "1563/1563 [==============================] - 6s 2ms/step - loss: 0.4989 - sigma_loss: 0.4989 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - ricci_loss: 0.0000e+00 - volk_loss: 2.6878e-07\n",
      "1563/1563 [==============================] - 6s 2ms/step - loss: 0.5476 - sigma_loss: 0.0022 - kaehler_loss: 0.0000e+00 - transition_loss: 6.9553e-10 - ricci_loss: 0.0000e+00 - volk_loss: 0.2727\n",
      "zero network validation loss: \n",
      "{'loss': 0.5725029110908508, 'sigma_loss': 0.5725029110908508, 'kaehler_loss': 0.0, 'transition_loss': 0.0, 'ricci_loss': 0.0, 'volk_loss': 1.173486197103557e-08}\n",
      "validation loss for final network: \n",
      "{'loss': 0.5150485634803772, 'sigma_loss': 0.0020871683955192566, 'kaehler_loss': 0.0, 'transition_loss': 6.273492281216875e-10, 'ricci_loss': 0.0, 'volk_loss': 0.25648069381713867}\n",
      "ratio of trained to zero: {'loss ratio': 0.8996435555280665, 'sigma_loss ratio': 0.003645690386247029, 'kaehler_loss ratio': 0.0, 'transition_loss ratio': 0.06273492281216875, 'ricci_loss ratio': 0.0, 'volk_loss ratio': 11800428.921928806}\n",
      "average transition discrepancy in standard deviations: tf.Tensor(1.0658346e-06, shape=(), dtype=float32)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if train_phi:\n",
    "    phimodel1,training_history=train_and_save_nn(free_coefficient,depthPhi,widthPhi,nEpochsPhi,stddev=0.05,bSizes=[64,50000],lRate=0.1) \n",
    "else:\n",
    "    phimodel1,training_history=load_nn_phimodel(free_coefficient,depthPhi,widthPhi,nEpochsPhi,[64,50000],set_weights_to_zero=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e909660a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling\n",
      "compiled\n",
      "starting christoffel tape\n",
      "christoffel tape1\n",
      "christoffel tape2\n",
      "del christoffel tape\n",
      "starting tapeR\n",
      "only runs during compilation\n",
      "first gradienttape\n",
      "second gradienttape\n",
      "tapes deleted\n",
      "computing batched with batch size 256 and total length 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/f/fraser-talientec/cymetricTESTING217/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py:6577: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  _result = pywrap_tfe.TFE_Py_FastPathExecute(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 256 euler: -75588.75 vol 288.73486\n",
      "in 512 euler: -71299.516 vol 287.3044\n",
      "in 768 euler: -72095.62 vol 284.4358\n",
      "in 1024 euler: -67482.48 vol 282.04205\n",
      "in 1280 euler: -69609.33 vol 282.03625\n",
      "in 1536 euler: -67711.4 vol 279.9933\n",
      "in 1792 euler: -68179.35 vol 281.1218\n",
      "in 2048 euler: -68394.7 vol 282.11652\n",
      "time for batch size 256: 0.043286034185439345\n",
      "compiling\n",
      "compiled\n",
      "starting christoffel tape\n",
      "christoffel tape1\n",
      "christoffel tape2\n",
      "del christoffel tape\n",
      "starting tapeR\n",
      "only runs during compilation\n",
      "first gradienttape\n",
      "second gradienttape\n",
      "tapes deleted\n",
      "computing batched with batch size 384 and total length 2048\n",
      "in 384 euler: -75560.4 vol 293.43234\n",
      "in 768 euler: -72095.62 vol 284.4358\n",
      "in 1152 euler: -68053.24 vol 281.74615\n",
      "in 1536 euler: -67711.4 vol 279.9933\n",
      "in 1920 euler: -68250.05 vol 281.4318\n",
      "in 2304 euler: -65747.72 vol 281.90646\n",
      "time for batch size 384: 0.042992850703497724\n",
      "compiling\n",
      "compiled\n",
      "starting christoffel tape\n",
      "christoffel tape1\n",
      "christoffel tape2\n",
      "del christoffel tape\n",
      "starting tapeR\n",
      "only runs during compilation\n",
      "first gradienttape\n",
      "second gradienttape\n",
      "tapes deleted\n",
      "computing batched with batch size 512 and total length 2048\n",
      "in 512 euler: -71299.516 vol 287.3044\n",
      "in 1024 euler: -67482.48 vol 282.04205\n",
      "in 1536 euler: -67711.4 vol 279.9933\n",
      "in 2048 euler: -68394.7 vol 282.11652\n",
      "time for batch size 512: 0.028399168560281397\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# data=np.load('dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) + '/dataset.npz')\n",
    "# X_total=tf.cast(data['X_val'],tf.float32)[0:2048]\n",
    "# cy_weights=tf.cast(data['y_val'][:,0],tf.float32)\n",
    "# for batch_size in [256,384,512]:\n",
    "#     #batch_size=128\n",
    "#     source_computing_class= Q_compiled_function(phimodel1,X_total[0:batch_size],batch_size)    \n",
    "#     start_time=time.time()\n",
    "#     Q_values,euler_all_with_sqrtg = compute_batched_func(source_computing_class.compute_Q,X_total[:batch_size*10], batch_size,cy_weights)\n",
    "#     print(\"time for batch size \"+str(batch_size)+\": \"+str((time.time()-start_time)/(batch_size*10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f06c6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling\n",
      "compiled\n",
      "starting christoffel tape\n",
      "christoffel tape1\n",
      "christoffel tape2\n",
      "del christoffel tape\n",
      "starting tapeR\n",
      "only runs during compilation\n",
      "first gradienttape\n",
      "second gradienttape\n",
      "tapes deleted\n",
      "computing batched with batch size 64 and total length 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/f/fraser-talientec/cymetricTESTING217/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py:6577: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  _result = pywrap_tfe.TFE_Py_FastPathExecute(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 64 euler: -28083.748 vol 265.53772\n",
      "in 128 euler: -54543.06 vol 279.85168\n",
      "in 192 euler: -65535.902 vol 282.9739\n",
      "in 256 euler: -75588.75 vol 288.73486\n",
      "in 320 euler: -72049.4 vol 289.6844\n",
      "in 384 euler: -75560.4 vol 293.43234\n",
      "in 448 euler: -72986.875 vol 289.02023\n",
      "in 512 euler: -71299.516 vol 287.3044\n",
      "in 576 euler: -73450.125 vol 288.3213\n",
      "in 640 euler: -75365.29 vol 287.72202\n",
      "time for batch size 64: 0.06081747338175773\n",
      "compiling\n",
      "compiled\n",
      "starting christoffel tape\n",
      "christoffel tape1\n",
      "christoffel tape2\n",
      "del christoffel tape\n",
      "starting tapeR\n",
      "only runs during compilation\n",
      "first gradienttape\n",
      "second gradienttape\n",
      "tapes deleted\n",
      "computing batched with batch size 128 and total length 1280\n",
      "in 128 euler: -54543.043 vol 279.85168\n",
      "in 256 euler: -75588.74 vol 288.73486\n",
      "in 384 euler: -75560.39 vol 293.43234\n",
      "in 512 euler: -71299.516 vol 287.3044\n",
      "in 640 euler: -75365.29 vol 287.72202\n",
      "in 768 euler: -72095.62 vol 284.4358\n",
      "in 896 euler: -68884.77 vol 282.46387\n",
      "in 1024 euler: -67482.48 vol 282.04205\n",
      "in 1152 euler: -68053.24 vol 281.74615\n",
      "in 1280 euler: -69609.33 vol 282.03625\n",
      "time for batch size 128: 0.047612459771335124\n",
      "compiling\n",
      "compiled\n",
      "starting christoffel tape\n",
      "christoffel tape1\n",
      "christoffel tape2\n",
      "del christoffel tape\n",
      "starting tapeR\n",
      "only runs during compilation\n",
      "first gradienttape\n",
      "second gradienttape\n",
      "tapes deleted\n",
      "computing batched with batch size 256 and total length 2048\n",
      "in 256 euler: -75588.75 vol 288.73486\n",
      "in 512 euler: -71299.516 vol 287.3044\n",
      "in 768 euler: -72095.62 vol 284.4358\n",
      "in 1024 euler: -67482.48 vol 282.04205\n",
      "in 1280 euler: -69609.33 vol 282.03625\n",
      "in 1536 euler: -67711.4 vol 279.9933\n",
      "in 1792 euler: -68179.35 vol 281.1218\n",
      "in 2048 euler: -68394.71 vol 282.11652\n",
      "time for batch size 256: 0.04795359969139099\n",
      "compiling\n",
      "compiled\n",
      "starting christoffel tape\n",
      "christoffel tape1\n",
      "christoffel tape2\n",
      "del christoffel tape\n",
      "starting tapeR\n",
      "only runs during compilation\n",
      "first gradienttape\n",
      "second gradienttape\n",
      "tapes deleted\n",
      "computing batched with batch size 512 and total length 2048\n",
      "in 512 euler: -71299.51 vol 287.3044\n",
      "in 1024 euler: -67482.48 vol 282.04205\n",
      "in 1536 euler: -67711.4 vol 279.9933\n",
      "in 2048 euler: -68394.71 vol 282.11652\n",
      "time for batch size 512: 0.02685925904661417\n",
      "compiling\n",
      "compiled\n",
      "starting christoffel tape\n",
      "christoffel tape1\n",
      "christoffel tape2\n",
      "del christoffel tape\n",
      "starting tapeR\n",
      "only runs during compilation\n",
      "first gradienttape\n",
      "second gradienttape\n",
      "tapes deleted\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "data=np.load('dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) + '/dataset.npz')\n",
    "X_total=tf.cast(data['X_val'],tf.float32)[0:2048]\n",
    "cy_weights=tf.cast(data['y_val'][:,0],tf.float32)\n",
    "for batch_size in [256,384,512]:\n",
    "    #batch_size=128\n",
    "    source_computing_class= Q_compiled_function(phimodel1,X_total[0:batch_size],batch_size)    \n",
    "    start_time=time.time()\n",
    "    Q_values,euler_all_with_sqrtg = compute_batched_func(source_computing_class.compute_Q,X_total[:batch_size*10], batch_size,cy_weights)\n",
    "    print(\"time for batch size \"+str(batch_size)+\": \"+str((time.time()-start_time)/(batch_size*10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cbe52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #compute ricci scalar\n",
    "# from cymetric.models.measures import ricci_scalar_fn\n",
    "# R_scalar_cymetric_train=ricci_scalar_fn(phimodel1,tf.cast(X_total[0:100],np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32fd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-26.42 -15.88 8.34 37.44 -21.00 58.78 -37.50 -12.50 55.89 -14.08 14.36\n",
      " -31.41 14.55 3.01 -17.73 -37.55 -36.73 12.50 -32.13 -26.40 -2.15 -34.69\n",
      " -26.19 -34.96 22.06 4.58 -1.72 5.24 64.00 48.70 -11.64 6.86 -25.40 -8.96\n",
      " 24.06 -31.53 21.98 64.09 -34.84 13.21 -10.32 52.61 13.37 6.64 4.61 -18.74\n",
      " -14.62 -1.76 5.37 -34.79 -11.98 -35.05 -2.36 -10.76 -34.16 -33.27 -17.06\n",
      " 11.13 -29.20 22.80 10.76 53.24 -15.38 -1.14 18.03 19.18 23.92 -34.45\n",
      " -31.54 -32.06 -36.48 22.89 38.67 -4.93 16.31 1.46 13.80 -36.13 49.92\n",
      " -35.97 50.06 5.98 2.76 -15.94 -15.63 -32.80 -11.93 -31.42 -15.20 -34.02\n",
      " -3.66 40.01 4.99 16.00 -3.19 -33.77 -5.83 -3.56 14.31 14.54]\n"
     ]
    }
   ],
   "source": [
    "# R_scalar_cymetric_train[0]\n",
    "# #round to 2dp, and print in standard format\n",
    "# print(np.array2string(np.round(R_scalar_cymetric_train[0], 2), formatter={'float_kind':lambda x: f\"{x:.2f}\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1eb5fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirname for alpha: dataAlphaP/Quintic_pg_with_1e-14\n",
      "dirname for alpha: dataAlphaP/QuinticAlpha_pg_with_1e-14\n",
      "loading prexisting dataset\n",
      "problem loading data - generating anyway\n",
      "kappa over 6 \n",
      "tf.Tensor(0.01761915, shape=(), dtype=float32)\n",
      "compiling\n",
      "compiled\n",
      "starting christoffel tape\n",
      "christoffel tape1\n",
      "christoffel tape2\n",
      "del christoffel tape\n",
      "starting tapeR\n",
      "only runs during compilation\n",
      "first gradienttape\n",
      "second gradienttape\n",
      "tapes deleted\n",
      "computing batched with batch size 128 and total length 500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/f/fraser-talientec/cymetricTESTING217/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py:6577: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  _result = pywrap_tfe.TFE_Py_FastPathExecute(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 128 euler: -192.93784 vol 0.8479486\n",
      "in 256 euler: -199.59787 vol 0.85239446\n",
      "in 384 euler: -185.89746 vol 0.8357242\n",
      "in 512 euler: -174.5135 vol 0.8177432\n",
      "in 640 euler: -170.65315 vol 0.81246585\n",
      "in 768 euler: -183.50537 vol 0.82104367\n",
      "in 896 euler: -192.5739 vol 0.83224946\n",
      "in 1024 euler: -190.34424 vol 0.8311457\n",
      "in 1152 euler: -184.94272 vol 0.82402825\n",
      "in 1280 euler: -195.87485 vol 0.8286231\n",
      "in 1408 euler: -195.72719 vol 0.8272517\n",
      "in 1536 euler: -196.35579 vol 0.8314883\n",
      "in 1664 euler: -195.30574 vol 0.83052385\n",
      "in 1792 euler: -193.61978 vol 0.8240824\n",
      "in 1920 euler: -191.31787 vol 0.8254054\n",
      "in 2048 euler: -191.44888 vol 0.8267883\n",
      "in 2176 euler: -190.92386 vol 0.8275673\n",
      "in 2304 euler: -188.39027 vol 0.8252374\n",
      "in 2432 euler: -191.95251 vol 0.82986975\n",
      "in 2560 euler: -191.71683 vol 0.8301508\n",
      "in 2688 euler: -190.84465 vol 0.827933\n",
      "in 2816 euler: -189.98195 vol 0.8302399\n",
      "in 2944 euler: -190.3781 vol 0.8277679\n",
      "in 3072 euler: -188.16199 vol 0.82433146\n",
      "in 3200 euler: -188.33644 vol 0.82623285\n",
      "in 3328 euler: -188.66284 vol 0.82642853\n",
      "in 3456 euler: -190.86679 vol 0.8278855\n",
      "in 3584 euler: -191.46892 vol 0.8287129\n",
      "in 3712 euler: -193.54863 vol 0.829446\n",
      "in 3840 euler: -192.72308 vol 0.8282545\n",
      "in 3968 euler: -194.4586 vol 0.8285328\n",
      "in 4096 euler: -193.1907 vol 0.82733154\n",
      "in 4224 euler: -192.69205 vol 0.8266933\n",
      "in 4352 euler: -191.61977 vol 0.82450587\n",
      "in 4480 euler: -191.65814 vol 0.8241428\n",
      "in 4608 euler: -192.79378 vol 0.82576996\n",
      "in 4736 euler: -193.25388 vol 0.82671374\n",
      "in 4864 euler: -194.73412 vol 0.82832193\n",
      "in 4992 euler: -194.85292 vol 0.828235\n",
      "in 5120 euler: -197.54262 vol 0.8297164\n",
      "in 5248 euler: -196.02032 vol 0.82869244\n",
      "in 5376 euler: -194.38068 vol 0.8278458\n",
      "in 5504 euler: -195.4633 vol 0.8286007\n",
      "in 5632 euler: -194.71558 vol 0.82800364\n",
      "in 5760 euler: -195.47986 vol 0.8284649\n",
      "in 5888 euler: -197.61655 vol 0.8310703\n",
      "in 6016 euler: -198.99202 vol 0.8319137\n",
      "in 6144 euler: -198.57784 vol 0.8311928\n",
      "in 6272 euler: -199.86101 vol 0.83176976\n",
      "in 6400 euler: -198.82779 vol 0.83039045\n",
      "in 6528 euler: -198.6283 vol 0.83065706\n",
      "in 6656 euler: -199.47018 vol 0.8313474\n",
      "in 6784 euler: -202.60393 vol 0.83361787\n",
      "in 6912 euler: -202.56651 vol 0.83344346\n",
      "in 7040 euler: -202.96625 vol 0.83346546\n",
      "in 7168 euler: -202.41284 vol 0.8335182\n",
      "in 7296 euler: -202.36345 vol 0.8335677\n",
      "in 7424 euler: -202.52115 vol 0.8341428\n",
      "in 7552 euler: -202.44582 vol 0.8347787\n",
      "in 7680 euler: -203.34238 vol 0.83603376\n",
      "in 7808 euler: -203.21669 vol 0.83533084\n",
      "in 7936 euler: -202.40602 vol 0.834631\n",
      "in 8064 euler: -201.56583 vol 0.834369\n",
      "in 8192 euler: -202.24763 vol 0.8348692\n",
      "in 8320 euler: -201.81432 vol 0.8349755\n",
      "in 8448 euler: -202.3898 vol 0.8352308\n",
      "in 8576 euler: -201.07153 vol 0.8344424\n",
      "in 8704 euler: -200.83095 vol 0.8347841\n",
      "in 8832 euler: -201.63799 vol 0.835586\n",
      "in 8960 euler: -200.46632 vol 0.8344513\n",
      "in 9088 euler: -200.8547 vol 0.8351532\n",
      "in 9216 euler: -201.03986 vol 0.8350035\n",
      "in 9344 euler: -201.45569 vol 0.83497995\n",
      "in 9472 euler: -200.87659 vol 0.8344378\n",
      "in 9600 euler: -201.08308 vol 0.8344281\n",
      "in 9728 euler: -200.644 vol 0.8342272\n",
      "in 9856 euler: -200.73085 vol 0.8338653\n",
      "in 9984 euler: -200.18304 vol 0.8329471\n",
      "in 10112 euler: -199.61461 vol 0.83273256\n",
      "in 10240 euler: -199.82692 vol 0.83299464\n",
      "in 10368 euler: -200.16249 vol 0.83346784\n",
      "in 10496 euler: -199.87495 vol 0.83332545\n",
      "in 10624 euler: -199.74983 vol 0.83380806\n",
      "in 10752 euler: -199.96524 vol 0.8340981\n",
      "in 10880 euler: -200.01932 vol 0.8344448\n",
      "in 11008 euler: -199.14012 vol 0.8339905\n",
      "in 11136 euler: -200.33885 vol 0.83489525\n",
      "in 11264 euler: -201.33446 vol 0.83538437\n",
      "in 11392 euler: -201.24017 vol 0.8353364\n",
      "in 11520 euler: -200.61957 vol 0.8345209\n",
      "in 11648 euler: -201.48358 vol 0.8349047\n",
      "in 11776 euler: -202.24452 vol 0.8353557\n",
      "in 11904 euler: -202.01134 vol 0.83523196\n",
      "in 12032 euler: -201.68484 vol 0.83454823\n",
      "in 12160 euler: -201.349 vol 0.8344298\n",
      "in 12288 euler: -201.50061 vol 0.83446795\n",
      "in 12416 euler: -201.82877 vol 0.8348414\n",
      "in 12544 euler: -201.7724 vol 0.83468854\n",
      "in 12672 euler: -202.15294 vol 0.8348381\n",
      "in 12800 euler: -202.46625 vol 0.8347037\n",
      "in 12928 euler: -202.87994 vol 0.83484036\n",
      "in 13056 euler: -203.17766 vol 0.83523256\n",
      "in 13184 euler: -203.56055 vol 0.8350564\n",
      "in 13312 euler: -203.59969 vol 0.8352307\n",
      "in 13440 euler: -203.4442 vol 0.83539546\n",
      "in 13568 euler: -203.95302 vol 0.8354558\n",
      "in 13696 euler: -204.79958 vol 0.836225\n",
      "in 13824 euler: -204.08151 vol 0.83559966\n",
      "in 13952 euler: -203.7286 vol 0.8353233\n",
      "in 14080 euler: -203.33476 vol 0.83509994\n",
      "in 14208 euler: -203.82845 vol 0.83550924\n",
      "in 14336 euler: -203.77992 vol 0.83553857\n",
      "in 14464 euler: -203.83293 vol 0.8357733\n",
      "in 14592 euler: -203.87247 vol 0.83591175\n",
      "in 14720 euler: -203.84444 vol 0.8358645\n",
      "in 14848 euler: -204.1751 vol 0.8363025\n",
      "in 14976 euler: -203.84024 vol 0.8361557\n",
      "in 15104 euler: -204.67984 vol 0.8363443\n",
      "in 15232 euler: -205.05078 vol 0.83694166\n",
      "in 15360 euler: -204.69547 vol 0.83668834\n",
      "in 15488 euler: -205.2305 vol 0.8371366\n",
      "in 15616 euler: -205.35545 vol 0.8371672\n",
      "in 15744 euler: -205.89537 vol 0.8377203\n",
      "in 15872 euler: -206.0817 vol 0.83779305\n",
      "in 16000 euler: -206.39528 vol 0.8380556\n",
      "in 16128 euler: -205.81166 vol 0.83742374\n",
      "in 16256 euler: -205.30664 vol 0.8369855\n",
      "in 16384 euler: -205.00961 vol 0.8369733\n",
      "in 16512 euler: -204.83313 vol 0.83697546\n",
      "in 16640 euler: -204.16449 vol 0.8364028\n",
      "in 16768 euler: -203.8436 vol 0.8361399\n",
      "in 16896 euler: -203.70587 vol 0.8361194\n",
      "in 17024 euler: -203.3606 vol 0.8361933\n",
      "in 17152 euler: -203.0841 vol 0.8359845\n",
      "in 17280 euler: -203.16913 vol 0.835619\n",
      "in 17408 euler: -203.24251 vol 0.8353927\n",
      "in 17536 euler: -203.20221 vol 0.8355132\n",
      "in 17664 euler: -203.8564 vol 0.8362074\n",
      "in 17792 euler: -203.3323 vol 0.83585864\n",
      "in 17920 euler: -203.27216 vol 0.8358082\n",
      "in 18048 euler: -203.25964 vol 0.83579814\n",
      "in 18176 euler: -203.01772 vol 0.8359344\n",
      "in 18304 euler: -202.93706 vol 0.83600634\n",
      "in 18432 euler: -203.23245 vol 0.8359659\n",
      "in 18560 euler: -203.05142 vol 0.8359364\n",
      "in 18688 euler: -203.39761 vol 0.8365595\n",
      "in 18816 euler: -203.3423 vol 0.8365761\n",
      "in 18944 euler: -203.5748 vol 0.8367314\n",
      "in 19072 euler: -203.73418 vol 0.8367749\n",
      "in 19200 euler: -203.48752 vol 0.836534\n",
      "in 19328 euler: -203.55066 vol 0.8366696\n",
      "in 19456 euler: -203.63464 vol 0.83668023\n",
      "in 19584 euler: -203.94821 vol 0.83711535\n",
      "in 19712 euler: -203.76555 vol 0.8371386\n",
      "in 19840 euler: -203.75078 vol 0.83739054\n",
      "in 19968 euler: -203.63896 vol 0.8372496\n",
      "in 20096 euler: -203.94989 vol 0.8374467\n",
      "in 20224 euler: -203.81812 vol 0.83729136\n",
      "in 20352 euler: -203.53033 vol 0.8371554\n",
      "in 20480 euler: -203.64575 vol 0.8373934\n",
      "in 20608 euler: -203.3326 vol 0.8372175\n",
      "in 20736 euler: -203.44464 vol 0.8373608\n",
      "in 20864 euler: -203.91437 vol 0.8375836\n",
      "in 20992 euler: -203.78302 vol 0.8373791\n",
      "in 21120 euler: -203.94673 vol 0.83760273\n",
      "in 21248 euler: -203.6096 vol 0.83744085\n",
      "in 21376 euler: -203.39835 vol 0.83755136\n",
      "in 21504 euler: -203.48889 vol 0.83767\n",
      "in 21632 euler: -203.39848 vol 0.8375869\n",
      "in 21760 euler: -203.10147 vol 0.8371376\n",
      "in 21888 euler: -202.78207 vol 0.8368512\n",
      "in 22016 euler: -203.05165 vol 0.837215\n",
      "in 22144 euler: -203.0256 vol 0.83735305\n",
      "in 22272 euler: -203.05963 vol 0.83721346\n",
      "in 22400 euler: -202.99794 vol 0.8372358\n",
      "in 22528 euler: -202.96284 vol 0.8370726\n",
      "in 22656 euler: -202.88829 vol 0.83727396\n",
      "in 22784 euler: -202.76277 vol 0.8371684\n",
      "in 22912 euler: -202.55429 vol 0.8368097\n",
      "in 23040 euler: -202.31158 vol 0.83668536\n",
      "in 23168 euler: -202.37439 vol 0.8367797\n",
      "in 23296 euler: -202.19818 vol 0.83670133\n",
      "in 23424 euler: -201.96866 vol 0.8366482\n",
      "in 23552 euler: -202.2878 vol 0.8369599\n",
      "in 23680 euler: -202.12418 vol 0.8368659\n",
      "in 23808 euler: -202.05016 vol 0.8367988\n",
      "in 23936 euler: -202.30305 vol 0.8368426\n",
      "in 24064 euler: -201.86919 vol 0.8362505\n",
      "in 24192 euler: -201.78218 vol 0.8363361\n",
      "in 24320 euler: -201.74449 vol 0.83639807\n",
      "in 24448 euler: -201.8054 vol 0.8364427\n",
      "in 24576 euler: -201.73439 vol 0.8363738\n",
      "in 24704 euler: -201.67467 vol 0.8361805\n",
      "in 24832 euler: -201.75806 vol 0.8363572\n",
      "in 24960 euler: -201.97543 vol 0.8364648\n",
      "in 25088 euler: -202.07715 vol 0.83640414\n",
      "in 25216 euler: -202.10965 vol 0.83630186\n",
      "in 25344 euler: -202.27162 vol 0.83629256\n",
      "in 25472 euler: -202.47546 vol 0.83644193\n",
      "in 25600 euler: -202.63492 vol 0.83640504\n",
      "in 25728 euler: -202.5989 vol 0.8364238\n",
      "in 25856 euler: -202.4754 vol 0.8365009\n",
      "in 25984 euler: -202.15712 vol 0.83628374\n",
      "in 26112 euler: -202.32938 vol 0.83650345\n",
      "in 26240 euler: -202.57433 vol 0.8364408\n",
      "in 26368 euler: -202.63087 vol 0.836404\n",
      "in 26496 euler: -202.58696 vol 0.83642364\n",
      "in 26624 euler: -202.43799 vol 0.8362572\n",
      "in 26752 euler: -202.70311 vol 0.8364973\n",
      "in 26880 euler: -202.84978 vol 0.8367525\n",
      "in 27008 euler: -202.85529 vol 0.83675224\n",
      "in 27136 euler: -202.60785 vol 0.8364921\n",
      "in 27264 euler: -202.71729 vol 0.8365884\n",
      "in 27392 euler: -202.52145 vol 0.8362354\n",
      "in 27520 euler: -202.66566 vol 0.83632356\n",
      "in 27648 euler: -202.5615 vol 0.8362587\n",
      "in 27776 euler: -202.52736 vol 0.8361887\n",
      "in 27904 euler: -202.8149 vol 0.83623976\n",
      "in 28032 euler: -203.02115 vol 0.83650124\n",
      "in 28160 euler: -203.04507 vol 0.8364538\n",
      "in 28288 euler: -203.08437 vol 0.83652085\n",
      "in 28416 euler: -203.06802 vol 0.8365766\n",
      "in 28544 euler: -202.87756 vol 0.8366222\n",
      "in 28672 euler: -202.74858 vol 0.8364523\n",
      "in 28800 euler: -202.70784 vol 0.836091\n",
      "in 28928 euler: -202.604 vol 0.83596534\n",
      "in 29056 euler: -202.81143 vol 0.83611363\n",
      "in 29184 euler: -202.87593 vol 0.83619344\n",
      "in 29312 euler: -202.49199 vol 0.835909\n",
      "in 29440 euler: -202.15471 vol 0.83569294\n",
      "in 29568 euler: -202.27042 vol 0.8356598\n",
      "in 29696 euler: -202.43253 vol 0.8358099\n",
      "in 29824 euler: -202.5723 vol 0.8359795\n",
      "in 29952 euler: -202.66817 vol 0.8360511\n",
      "in 30080 euler: -202.59843 vol 0.83613217\n",
      "in 30208 euler: -202.39935 vol 0.835934\n",
      "in 30336 euler: -202.46687 vol 0.8359812\n",
      "in 30464 euler: -202.44797 vol 0.8359484\n",
      "in 30592 euler: -202.37196 vol 0.8356437\n",
      "in 30720 euler: -202.09714 vol 0.835328\n",
      "in 30848 euler: -202.16801 vol 0.8353581\n",
      "in 30976 euler: -201.87366 vol 0.8351343\n",
      "in 31104 euler: -201.73296 vol 0.83510965\n",
      "in 31232 euler: -201.5509 vol 0.83499324\n",
      "in 31360 euler: -201.57883 vol 0.83505327\n",
      "in 31488 euler: -201.68727 vol 0.83510494\n",
      "in 31616 euler: -201.47507 vol 0.83497834\n",
      "in 31744 euler: -201.6599 vol 0.8353365\n",
      "in 31872 euler: -201.49963 vol 0.8352549\n",
      "in 32000 euler: -201.70805 vol 0.8355621\n",
      "in 32128 euler: -201.82591 vol 0.83560085\n",
      "in 32256 euler: -201.56172 vol 0.8353578\n",
      "in 32384 euler: -201.44704 vol 0.8354076\n",
      "in 32512 euler: -201.49498 vol 0.83534545\n",
      "in 32640 euler: -201.79797 vol 0.8355675\n",
      "in 32768 euler: -201.9603 vol 0.83551\n",
      "in 32896 euler: -202.09952 vol 0.8356475\n",
      "in 33024 euler: -201.83246 vol 0.8354647\n",
      "in 33152 euler: -201.76414 vol 0.835559\n",
      "in 33280 euler: -201.67776 vol 0.83536506\n",
      "in 33408 euler: -201.75117 vol 0.8354722\n",
      "in 33536 euler: -201.9617 vol 0.83558995\n",
      "in 33664 euler: -202.21169 vol 0.8357995\n",
      "in 33792 euler: -202.09035 vol 0.835803\n",
      "in 33920 euler: -201.93225 vol 0.8357895\n",
      "in 34048 euler: -202.03926 vol 0.8359208\n",
      "in 34176 euler: -202.32768 vol 0.8361715\n",
      "in 34304 euler: -202.41815 vol 0.83627003\n",
      "in 34432 euler: -202.26938 vol 0.83603644\n",
      "in 34560 euler: -202.25935 vol 0.83614624\n",
      "in 34688 euler: -202.4479 vol 0.83637\n",
      "in 34816 euler: -202.43993 vol 0.8363416\n",
      "in 34944 euler: -202.82126 vol 0.83660096\n",
      "in 35072 euler: -202.59346 vol 0.83635956\n",
      "in 35200 euler: -202.59486 vol 0.83635694\n",
      "in 35328 euler: -202.60182 vol 0.83647823\n",
      "in 35456 euler: -202.90671 vol 0.83668613\n",
      "in 35584 euler: -202.72539 vol 0.8367516\n",
      "in 35712 euler: -202.63614 vol 0.83674014\n",
      "in 35840 euler: -202.47107 vol 0.83662313\n",
      "in 35968 euler: -202.419 vol 0.8366596\n",
      "in 36096 euler: -202.3641 vol 0.836688\n",
      "in 36224 euler: -202.15738 vol 0.83649683\n",
      "in 36352 euler: -202.10466 vol 0.8365156\n",
      "in 36480 euler: -202.30957 vol 0.8367376\n",
      "in 36608 euler: -202.35066 vol 0.8367396\n",
      "in 36736 euler: -202.18227 vol 0.8364677\n",
      "in 36864 euler: -202.10979 vol 0.83628106\n",
      "in 36992 euler: -202.16084 vol 0.83622205\n",
      "in 37120 euler: -201.90459 vol 0.8359135\n",
      "in 37248 euler: -201.9837 vol 0.8361122\n",
      "in 37376 euler: -202.13826 vol 0.83622867\n",
      "in 37504 euler: -202.0393 vol 0.8361609\n",
      "in 37632 euler: -202.2453 vol 0.83614165\n",
      "in 37760 euler: -202.42192 vol 0.83621156\n",
      "in 37888 euler: -202.60402 vol 0.8363956\n",
      "in 38016 euler: -202.29892 vol 0.8361507\n",
      "in 38144 euler: -202.23734 vol 0.83616143\n",
      "in 38272 euler: -202.35886 vol 0.8362293\n",
      "in 38400 euler: -202.29286 vol 0.8361864\n",
      "in 38528 euler: -202.6728 vol 0.8364859\n",
      "in 38656 euler: -202.752 vol 0.8365401\n",
      "in 38784 euler: -202.52821 vol 0.8361979\n",
      "in 38912 euler: -202.57632 vol 0.8362334\n",
      "in 39040 euler: -202.62628 vol 0.8363148\n",
      "in 39168 euler: -202.58644 vol 0.8364715\n",
      "in 39296 euler: -202.53961 vol 0.83639425\n",
      "in 39424 euler: -202.40086 vol 0.836276\n",
      "in 39552 euler: -202.19374 vol 0.8360432\n",
      "in 39680 euler: -202.02356 vol 0.8358376\n",
      "in 39808 euler: -202.2184 vol 0.8359225\n",
      "in 39936 euler: -202.19278 vol 0.83598036\n",
      "in 40064 euler: -201.95757 vol 0.83580166\n",
      "in 40192 euler: -201.98732 vol 0.83583564\n",
      "in 40320 euler: -202.08339 vol 0.835874\n",
      "in 40448 euler: -202.12953 vol 0.83594775\n",
      "in 40576 euler: -202.05673 vol 0.8360078\n",
      "in 40704 euler: -202.10794 vol 0.83606476\n",
      "in 40832 euler: -202.01254 vol 0.8360447\n",
      "in 40960 euler: -201.98782 vol 0.8358941\n",
      "in 41088 euler: -201.82762 vol 0.8356958\n",
      "in 41216 euler: -201.80757 vol 0.83569336\n",
      "in 41344 euler: -201.84396 vol 0.8357118\n",
      "in 41472 euler: -201.90524 vol 0.83570147\n",
      "in 41600 euler: -201.7065 vol 0.8354708\n",
      "in 41728 euler: -201.73859 vol 0.8354866\n",
      "in 41856 euler: -201.86491 vol 0.835598\n",
      "in 41984 euler: -202.13927 vol 0.8357272\n",
      "in 42112 euler: -202.36949 vol 0.83579904\n",
      "in 42240 euler: -202.31126 vol 0.8359103\n",
      "in 42368 euler: -202.42636 vol 0.83601654\n",
      "in 42496 euler: -202.26389 vol 0.8357403\n",
      "in 42624 euler: -202.43413 vol 0.83582085\n",
      "in 42752 euler: -202.4864 vol 0.83578783\n",
      "in 42880 euler: -202.38382 vol 0.8357056\n",
      "in 43008 euler: -202.48051 vol 0.83579475\n",
      "in 43136 euler: -202.54308 vol 0.8359797\n",
      "in 43264 euler: -202.43556 vol 0.83589816\n",
      "in 43392 euler: -202.57536 vol 0.8360312\n",
      "in 43520 euler: -202.75131 vol 0.8361274\n",
      "in 43648 euler: -202.65076 vol 0.83608425\n",
      "in 43776 euler: -202.8152 vol 0.83619815\n",
      "in 43904 euler: -202.5645 vol 0.8360291\n",
      "in 44032 euler: -202.50429 vol 0.8360031\n",
      "in 44160 euler: -202.52522 vol 0.8359898\n",
      "in 44288 euler: -202.40088 vol 0.8359276\n",
      "in 44416 euler: -202.25252 vol 0.8360398\n",
      "in 44544 euler: -202.26639 vol 0.8360471\n",
      "in 44672 euler: -202.2641 vol 0.8360416\n",
      "in 44800 euler: -202.24 vol 0.83592075\n",
      "in 44928 euler: -202.0767 vol 0.8357424\n",
      "in 45056 euler: -201.96733 vol 0.8356323\n",
      "in 45184 euler: -201.77997 vol 0.8355291\n",
      "in 45312 euler: -201.9044 vol 0.8356144\n",
      "in 45440 euler: -201.73112 vol 0.8354119\n",
      "in 45568 euler: -201.71855 vol 0.83535045\n",
      "in 45696 euler: -201.72374 vol 0.83528405\n",
      "in 45824 euler: -201.6635 vol 0.83509827\n",
      "in 45952 euler: -201.53278 vol 0.83505106\n",
      "in 46080 euler: -201.53902 vol 0.8350939\n",
      "in 46208 euler: -201.44002 vol 0.8349682\n",
      "in 46336 euler: -201.3403 vol 0.83493227\n",
      "in 46464 euler: -201.28056 vol 0.83493453\n",
      "in 46592 euler: -201.38496 vol 0.8350944\n",
      "in 46720 euler: -201.19818 vol 0.8349147\n",
      "in 46848 euler: -201.03648 vol 0.834819\n",
      "in 46976 euler: -201.05933 vol 0.83484244\n",
      "in 47104 euler: -201.11829 vol 0.8349183\n",
      "in 47232 euler: -201.1792 vol 0.83498114\n",
      "in 47360 euler: -201.16103 vol 0.834953\n",
      "in 47488 euler: -201.18759 vol 0.83504725\n",
      "in 47616 euler: -201.05814 vol 0.8348314\n",
      "in 47744 euler: -200.93484 vol 0.83475953\n",
      "in 47872 euler: -201.054 vol 0.83485895\n",
      "in 48000 euler: -200.8694 vol 0.8347179\n",
      "in 48128 euler: -200.73093 vol 0.83457786\n",
      "in 48256 euler: -200.86136 vol 0.83471906\n",
      "in 48384 euler: -200.88629 vol 0.8347066\n",
      "in 48512 euler: -200.8163 vol 0.83464956\n",
      "in 48640 euler: -200.87796 vol 0.83465976\n",
      "in 48768 euler: -200.7473 vol 0.8346379\n",
      "in 48896 euler: -200.72237 vol 0.834644\n",
      "in 49024 euler: -200.67206 vol 0.8345711\n",
      "in 49152 euler: -200.63977 vol 0.8345625\n",
      "in 49280 euler: -200.6164 vol 0.83460474\n",
      "in 49408 euler: -200.48006 vol 0.8345114\n",
      "in 49536 euler: -200.31686 vol 0.8344856\n",
      "in 49664 euler: -200.51031 vol 0.8346543\n",
      "in 49792 euler: -200.97644 vol 0.8350811\n",
      "in 49920 euler: -200.87451 vol 0.8350648\n",
      "in 50048 euler: -200.90785 vol 0.8350418\n",
      "in 50176 euler: -200.96895 vol 0.83498406\n",
      "in 50304 euler: -201.1019 vol 0.83507496\n",
      "in 50432 euler: -201.18732 vol 0.83526164\n",
      "in 50560 euler: -201.22751 vol 0.8353156\n",
      "in 50688 euler: -201.30429 vol 0.8353199\n",
      "in 50816 euler: -201.35481 vol 0.8354306\n",
      "in 50944 euler: -201.20148 vol 0.83524185\n",
      "in 51072 euler: -201.14238 vol 0.8351726\n",
      "in 51200 euler: -201.19655 vol 0.835259\n",
      "in 51328 euler: -201.24554 vol 0.8352524\n",
      "in 51456 euler: -201.27919 vol 0.8352572\n",
      "in 51584 euler: -201.11992 vol 0.83507246\n",
      "in 51712 euler: -201.20119 vol 0.83500594\n",
      "in 51840 euler: -201.13608 vol 0.83495957\n",
      "in 51968 euler: -200.98547 vol 0.8348359\n",
      "in 52096 euler: -201.08699 vol 0.8350054\n",
      "in 52224 euler: -200.99623 vol 0.83498937\n",
      "in 52352 euler: -200.96185 vol 0.8349256\n",
      "in 52480 euler: -200.83794 vol 0.8348208\n",
      "in 52608 euler: -200.79669 vol 0.8347785\n",
      "in 52736 euler: -200.75752 vol 0.8347048\n",
      "in 52864 euler: -200.62045 vol 0.83452016\n",
      "in 52992 euler: -200.59314 vol 0.8345315\n",
      "in 53120 euler: -200.64507 vol 0.83447707\n",
      "in 53248 euler: -200.5779 vol 0.8344498\n",
      "in 53376 euler: -200.61516 vol 0.834457\n",
      "in 53504 euler: -200.57494 vol 0.8344494\n",
      "in 53632 euler: -200.67474 vol 0.8345468\n",
      "in 53760 euler: -200.56535 vol 0.8343957\n",
      "in 53888 euler: -200.5248 vol 0.83436424\n",
      "in 54016 euler: -200.50818 vol 0.8343639\n",
      "in 54144 euler: -200.54706 vol 0.83443713\n",
      "in 54272 euler: -200.4036 vol 0.83424205\n",
      "in 54400 euler: -200.37135 vol 0.83419067\n",
      "in 54528 euler: -200.31989 vol 0.8341904\n",
      "in 54656 euler: -200.25304 vol 0.8341816\n",
      "in 54784 euler: -200.2561 vol 0.8340918\n",
      "in 54912 euler: -200.11665 vol 0.8340067\n",
      "in 55040 euler: -200.0104 vol 0.8339621\n",
      "in 55168 euler: -199.96129 vol 0.8339358\n",
      "in 55296 euler: -199.83574 vol 0.83389086\n",
      "in 55424 euler: -199.76328 vol 0.83387625\n",
      "in 55552 euler: -199.76346 vol 0.8339366\n",
      "in 55680 euler: -199.86691 vol 0.8341055\n",
      "in 55808 euler: -199.82361 vol 0.83405614\n",
      "in 55936 euler: -200.08315 vol 0.8343011\n",
      "in 56064 euler: -200.23627 vol 0.83444667\n",
      "in 56192 euler: -200.17398 vol 0.8344613\n",
      "in 56320 euler: -200.12411 vol 0.8343778\n",
      "in 56448 euler: -200.10168 vol 0.83441067\n",
      "in 56576 euler: -200.11168 vol 0.8343915\n",
      "in 56704 euler: -199.91478 vol 0.8342669\n",
      "in 56832 euler: -200.11385 vol 0.83439714\n",
      "in 56960 euler: -200.14624 vol 0.834489\n",
      "in 57088 euler: -200.0137 vol 0.83437335\n",
      "in 57216 euler: -200.09454 vol 0.8344258\n",
      "in 57344 euler: -200.1738 vol 0.834436\n",
      "in 57472 euler: -200.21426 vol 0.83443284\n",
      "in 57600 euler: -200.27267 vol 0.8345737\n",
      "in 57728 euler: -200.29033 vol 0.834749\n",
      "in 57856 euler: -200.31433 vol 0.83481866\n",
      "in 57984 euler: -200.55229 vol 0.835024\n",
      "in 58112 euler: -200.49391 vol 0.8349711\n",
      "in 58240 euler: -200.56113 vol 0.83506894\n",
      "in 58368 euler: -200.70308 vol 0.8351455\n",
      "in 58496 euler: -200.75397 vol 0.8350838\n",
      "in 58624 euler: -200.71654 vol 0.8350655\n",
      "in 58752 euler: -200.6409 vol 0.83495206\n",
      "in 58880 euler: -200.85336 vol 0.8351198\n",
      "in 59008 euler: -200.7891 vol 0.83503026\n",
      "in 59136 euler: -200.75284 vol 0.83502126\n",
      "in 59264 euler: -200.72699 vol 0.8349988\n",
      "in 59392 euler: -200.71147 vol 0.8350233\n",
      "in 59520 euler: -200.76918 vol 0.8351446\n",
      "in 59648 euler: -200.75282 vol 0.8350924\n",
      "in 59776 euler: -200.66924 vol 0.8349953\n",
      "in 59904 euler: -200.70811 vol 0.8349778\n",
      "in 60032 euler: -200.63206 vol 0.8348547\n",
      "in 60160 euler: -200.77109 vol 0.8349995\n",
      "in 60288 euler: -200.70677 vol 0.83493614\n",
      "in 60416 euler: -200.62758 vol 0.8349143\n",
      "in 60544 euler: -200.7462 vol 0.8349899\n",
      "in 60672 euler: -200.86845 vol 0.83509564\n",
      "in 60800 euler: -200.73825 vol 0.83496696\n",
      "in 60928 euler: -200.71405 vol 0.8349556\n",
      "in 61056 euler: -200.81413 vol 0.83511686\n",
      "in 61184 euler: -200.81754 vol 0.8351411\n",
      "in 61312 euler: -200.73346 vol 0.8350374\n",
      "in 61440 euler: -200.81184 vol 0.8350643\n",
      "in 61568 euler: -200.84392 vol 0.83503616\n",
      "in 61696 euler: -200.91034 vol 0.8350423\n",
      "in 61824 euler: -200.85568 vol 0.8350027\n",
      "in 61952 euler: -200.81915 vol 0.834983\n",
      "in 62080 euler: -200.84465 vol 0.8350451\n",
      "in 62208 euler: -200.67949 vol 0.834917\n",
      "in 62336 euler: -200.60675 vol 0.8348478\n",
      "in 62464 euler: -200.40353 vol 0.8346385\n",
      "in 62592 euler: -200.49724 vol 0.83462334\n",
      "in 62720 euler: -200.4973 vol 0.8346435\n",
      "in 62848 euler: -200.53242 vol 0.8347216\n",
      "in 62976 euler: -200.50998 vol 0.83474135\n",
      "in 63104 euler: -200.39642 vol 0.8346928\n",
      "in 63232 euler: -200.47948 vol 0.83469325\n",
      "in 63360 euler: -200.36906 vol 0.8345327\n",
      "in 63488 euler: -200.43723 vol 0.8345403\n",
      "in 63616 euler: -200.44865 vol 0.834481\n",
      "in 63744 euler: -200.56056 vol 0.83455867\n",
      "in 63872 euler: -200.63533 vol 0.8346008\n",
      "in 64000 euler: -200.66833 vol 0.83464676\n",
      "in 64128 euler: -200.58734 vol 0.83452314\n",
      "in 64256 euler: -200.48827 vol 0.8345212\n",
      "in 64384 euler: -200.47801 vol 0.83447886\n",
      "in 64640 euler: -200.44788 vol 0.8345039\n",
      "in 64768 euler: -200.36945 vol 0.8344779\n",
      "in 64896 euler: -200.36191 vol 0.8344322\n",
      "in 65024 euler: -200.40216 vol 0.8344801\n",
      "in 65152 euler: -200.29955 vol 0.8343938\n",
      "in 65280 euler: -200.40549 vol 0.834436\n",
      "in 65408 euler: -200.38197 vol 0.8344307\n",
      "in 65536 euler: -200.48006 vol 0.8343602\n",
      "in 65664 euler: -200.50069 vol 0.8343595\n",
      "in 65792 euler: -200.55254 vol 0.8344188\n",
      "in 65920 euler: -200.63458 vol 0.83452404\n",
      "in 66048 euler: -200.63097 vol 0.8345706\n",
      "in 66176 euler: -200.66168 vol 0.8345577\n",
      "in 66304 euler: -200.51286 vol 0.8344203\n",
      "in 66432 euler: -200.45609 vol 0.83438987\n",
      "in 66560 euler: -200.41917 vol 0.8343766\n",
      "in 66688 euler: -200.4516 vol 0.83431906\n",
      "in 66816 euler: -200.38222 vol 0.83430076\n",
      "in 66944 euler: -200.38109 vol 0.8342608\n",
      "in 67072 euler: -200.44373 vol 0.8342687\n",
      "in 67200 euler: -200.44109 vol 0.8342497\n",
      "in 67328 euler: -200.35843 vol 0.83423877\n",
      "in 67456 euler: -200.33144 vol 0.8342261\n",
      "in 67584 euler: -200.33049 vol 0.834181\n",
      "in 67712 euler: -200.34431 vol 0.83414197\n",
      "in 67840 euler: -200.2602 vol 0.83400536\n",
      "in 67968 euler: -200.25432 vol 0.83396083\n",
      "in 68096 euler: -200.36725 vol 0.8340398\n",
      "in 68224 euler: -200.28963 vol 0.8339539\n",
      "in 68352 euler: -200.2263 vol 0.8339256\n",
      "in 68480 euler: -200.35501 vol 0.83399874\n",
      "in 68608 euler: -200.26134 vol 0.83391595\n",
      "in 68736 euler: -200.38312 vol 0.8339666\n",
      "in 68864 euler: -200.37692 vol 0.8339893\n",
      "in 68992 euler: -200.36308 vol 0.83402556\n",
      "in 69120 euler: -200.29277 vol 0.83400756\n",
      "in 69248 euler: -200.20966 vol 0.8339571\n",
      "in 69376 euler: -200.31696 vol 0.8340168\n",
      "in 69504 euler: -200.26387 vol 0.8340293\n",
      "in 69632 euler: -200.24046 vol 0.83404535\n",
      "in 69760 euler: -200.41513 vol 0.83412087\n",
      "in 69888 euler: -200.46126 vol 0.8342055\n",
      "in 70016 euler: -200.35286 vol 0.83406246\n",
      "in 70144 euler: -200.2949 vol 0.83403105\n",
      "in 70272 euler: -200.35022 vol 0.8341065\n",
      "in 70400 euler: -200.30096 vol 0.8340656\n",
      "in 70528 euler: -200.31958 vol 0.83411366\n",
      "in 70656 euler: -200.34387 vol 0.834135\n",
      "in 70784 euler: -200.24455 vol 0.8339775\n",
      "in 70912 euler: -200.20465 vol 0.83388954\n",
      "in 71040 euler: -200.16019 vol 0.83390176\n",
      "in 71168 euler: -200.17897 vol 0.8339038\n",
      "in 71296 euler: -200.12138 vol 0.8338955\n",
      "in 71424 euler: -200.16467 vol 0.8339452\n",
      "in 71552 euler: -200.17117 vol 0.8339261\n",
      "in 71680 euler: -200.16515 vol 0.83398896\n",
      "in 71808 euler: -200.13785 vol 0.83395445\n",
      "in 71936 euler: -200.1656 vol 0.8339603\n",
      "in 72064 euler: -200.27254 vol 0.8340602\n",
      "in 72192 euler: -200.14247 vol 0.8339152\n",
      "in 72320 euler: -200.11731 vol 0.83389145\n",
      "in 72448 euler: -200.05678 vol 0.83376443\n",
      "in 72576 euler: -200.07895 vol 0.8337571\n",
      "in 72704 euler: -200.1312 vol 0.8337998\n",
      "in 72832 euler: -199.99037 vol 0.8336853\n",
      "in 72960 euler: -199.99915 vol 0.8337229\n",
      "in 73088 euler: -200.04497 vol 0.8338019\n",
      "in 73216 euler: -200.01726 vol 0.8337899\n",
      "in 73344 euler: -199.93039 vol 0.83374715\n",
      "in 73472 euler: -200.1717 vol 0.83386815\n",
      "in 73600 euler: -200.08836 vol 0.83378494\n",
      "in 73728 euler: -200.07088 vol 0.8338244\n",
      "in 73856 euler: -200.08325 vol 0.83388525\n",
      "in 73984 euler: -200.04462 vol 0.8338526\n",
      "in 74112 euler: -199.99902 vol 0.833814\n",
      "in 74240 euler: -199.93971 vol 0.83375686\n",
      "in 74368 euler: -199.86362 vol 0.8336793\n",
      "in 74496 euler: -199.9073 vol 0.83371913\n",
      "in 74624 euler: -199.8876 vol 0.8336734\n",
      "in 74752 euler: -199.93782 vol 0.8337393\n",
      "in 74880 euler: -199.91832 vol 0.8337605\n",
      "in 75008 euler: -199.94963 vol 0.83378655\n",
      "in 75136 euler: -199.906 vol 0.833805\n",
      "in 75264 euler: -199.81639 vol 0.833758\n",
      "in 75392 euler: -199.79936 vol 0.83375245\n",
      "in 75520 euler: -199.79774 vol 0.8338002\n",
      "in 75648 euler: -199.76785 vol 0.8338475\n",
      "in 75776 euler: -199.71947 vol 0.8338588\n",
      "in 75904 euler: -199.65128 vol 0.8338003\n",
      "in 76032 euler: -199.62357 vol 0.8337784\n",
      "in 76160 euler: -199.60011 vol 0.8337956\n",
      "in 76288 euler: -199.59041 vol 0.83376646\n",
      "in 76416 euler: -199.68245 vol 0.8338829\n",
      "in 76544 euler: -199.77357 vol 0.8338773\n",
      "in 76672 euler: -199.8382 vol 0.8339342\n",
      "in 76800 euler: -199.95877 vol 0.8340604\n",
      "in 76928 euler: -199.96495 vol 0.83403945\n",
      "in 77056 euler: -199.96219 vol 0.833991\n",
      "in 77184 euler: -199.93509 vol 0.8339894\n",
      "in 77312 euler: -199.90706 vol 0.8339408\n",
      "in 77440 euler: -199.98215 vol 0.83401114\n",
      "in 77568 euler: -199.95253 vol 0.8339177\n",
      "in 77696 euler: -199.94461 vol 0.83386743\n",
      "in 77824 euler: -200.02095 vol 0.8339725\n",
      "in 77952 euler: -200.05925 vol 0.83400846\n",
      "in 78080 euler: -199.95677 vol 0.8338582\n",
      "in 78208 euler: -199.8982 vol 0.83381337\n",
      "in 78336 euler: -199.90683 vol 0.8337944\n",
      "in 78464 euler: -199.8247 vol 0.8337098\n",
      "in 78592 euler: -199.93674 vol 0.83378917\n",
      "in 78720 euler: -200.00444 vol 0.83389366\n",
      "in 78848 euler: -199.91208 vol 0.83384603\n",
      "in 78976 euler: -199.89856 vol 0.8337972\n",
      "in 79104 euler: -199.94402 vol 0.8338627\n",
      "in 79232 euler: -200.06554 vol 0.8339338\n",
      "in 79360 euler: -200.07718 vol 0.83400506\n",
      "in 79488 euler: -200.12958 vol 0.8340691\n",
      "in 79616 euler: -200.2149 vol 0.83408064\n",
      "in 79744 euler: -200.14676 vol 0.8339996\n",
      "in 79872 euler: -200.07253 vol 0.8339433\n",
      "in 80000 euler: -200.05338 vol 0.8338704\n",
      "in 80128 euler: -200.19518 vol 0.8340017\n",
      "in 80256 euler: -200.1717 vol 0.8340044\n",
      "in 80384 euler: -200.03308 vol 0.8338822\n",
      "in 80512 euler: -200.03035 vol 0.8338794\n",
      "in 80640 euler: -199.92311 vol 0.8338228\n",
      "in 80768 euler: -199.80344 vol 0.8336797\n",
      "in 80896 euler: -199.76584 vol 0.8335888\n",
      "in 81024 euler: -199.80176 vol 0.8336564\n",
      "in 81152 euler: -199.84909 vol 0.8337313\n",
      "in 81280 euler: -199.90819 vol 0.83375937\n",
      "in 81408 euler: -199.92804 vol 0.833803\n",
      "in 81536 euler: -199.9598 vol 0.83378965\n",
      "in 81664 euler: -199.97096 vol 0.83375704\n",
      "in 81792 euler: -200.05661 vol 0.8338138\n",
      "in 81920 euler: -200.04295 vol 0.8337892\n",
      "in 82048 euler: -200.09627 vol 0.83385146\n",
      "in 82176 euler: -200.00945 vol 0.8338414\n",
      "in 82304 euler: -200.0702 vol 0.8339337\n",
      "in 82432 euler: -200.0121 vol 0.83396673\n",
      "in 82560 euler: -200.00005 vol 0.8339852\n",
      "in 82688 euler: -199.97804 vol 0.8339343\n",
      "in 82816 euler: -199.98041 vol 0.8339577\n",
      "in 82944 euler: -200.04716 vol 0.834005\n",
      "in 83072 euler: -200.06422 vol 0.8340602\n",
      "in 83200 euler: -200.07433 vol 0.8340903\n",
      "in 83328 euler: -200.05359 vol 0.834104\n",
      "in 83456 euler: -200.03815 vol 0.83406\n",
      "in 83584 euler: -200.12273 vol 0.8341057\n",
      "in 83712 euler: -200.29922 vol 0.83422536\n",
      "in 83840 euler: -200.33028 vol 0.83426374\n",
      "in 83968 euler: -200.36922 vol 0.8342674\n",
      "in 84096 euler: -200.33852 vol 0.834216\n",
      "in 84224 euler: -200.34256 vol 0.83422816\n",
      "in 84352 euler: -200.38728 vol 0.8343217\n",
      "in 84480 euler: -200.38205 vol 0.8343608\n",
      "in 84608 euler: -200.33261 vol 0.83433205\n",
      "in 84736 euler: -200.33534 vol 0.83438534\n",
      "in 84864 euler: -200.36842 vol 0.8344321\n",
      "in 84992 euler: -200.3458 vol 0.8344642\n",
      "in 85120 euler: -200.4217 vol 0.83451265\n",
      "in 85248 euler: -200.39151 vol 0.83438337\n",
      "in 85376 euler: -200.39107 vol 0.83439285\n",
      "in 85504 euler: -200.30652 vol 0.8342964\n",
      "in 85632 euler: -200.4004 vol 0.83438784\n",
      "in 85760 euler: -200.41512 vol 0.8343532\n",
      "in 85888 euler: -200.34706 vol 0.8343315\n",
      "in 86016 euler: -200.32195 vol 0.83435297\n",
      "in 86144 euler: -200.38007 vol 0.83439785\n",
      "in 86272 euler: -200.48286 vol 0.8345136\n",
      "in 86400 euler: -200.43163 vol 0.83446443\n",
      "in 86528 euler: -200.3302 vol 0.8343995\n",
      "in 86656 euler: -200.37857 vol 0.8344164\n",
      "in 86784 euler: -200.36177 vol 0.8344128\n",
      "in 86912 euler: -200.37143 vol 0.83444804\n",
      "in 87040 euler: -200.40901 vol 0.8345027\n",
      "in 87168 euler: -200.2846 vol 0.83441514\n",
      "in 87296 euler: -200.36308 vol 0.8344896\n",
      "in 87424 euler: -200.40982 vol 0.8344995\n",
      "in 87552 euler: -200.39513 vol 0.834501\n",
      "in 87680 euler: -200.33734 vol 0.8344548\n",
      "in 87808 euler: -200.25456 vol 0.8343955\n",
      "in 87936 euler: -200.22574 vol 0.83431786\n",
      "in 88064 euler: -200.27353 vol 0.83435464\n",
      "in 88192 euler: -200.2131 vol 0.83435255\n",
      "in 88320 euler: -200.11708 vol 0.834257\n",
      "in 88448 euler: -200.05096 vol 0.8342523\n",
      "in 88576 euler: -200.10095 vol 0.8342884\n",
      "in 88704 euler: -200.06001 vol 0.83428866\n",
      "in 88832 euler: -199.99034 vol 0.834205\n",
      "in 88960 euler: -199.94943 vol 0.8342366\n",
      "in 89088 euler: -199.86786 vol 0.8341716\n",
      "in 89216 euler: -199.80069 vol 0.83408386\n",
      "in 89344 euler: -199.80046 vol 0.8340255\n",
      "in 89472 euler: -199.81393 vol 0.8340362\n",
      "in 89600 euler: -199.84196 vol 0.83407253\n",
      "in 89728 euler: -199.73042 vol 0.83396626\n",
      "in 89856 euler: -199.80551 vol 0.8340412\n",
      "in 89984 euler: -199.78246 vol 0.83405036\n",
      "in 90112 euler: -199.6878 vol 0.83397955\n",
      "in 90240 euler: -199.66814 vol 0.83391476\n",
      "in 90368 euler: -199.7888 vol 0.8339469\n",
      "in 90496 euler: -199.75206 vol 0.8339577\n",
      "in 90624 euler: -199.75548 vol 0.8340023\n",
      "in 90752 euler: -199.65643 vol 0.8339324\n",
      "in 90880 euler: -199.75554 vol 0.8340598\n",
      "in 91008 euler: -199.67291 vol 0.83400816\n",
      "in 91136 euler: -199.71294 vol 0.83404505\n",
      "in 91264 euler: -199.65721 vol 0.8339987\n",
      "in 91392 euler: -199.61446 vol 0.8340112\n",
      "in 91520 euler: -199.46988 vol 0.8338519\n",
      "in 91648 euler: -199.54378 vol 0.8339119\n",
      "in 91776 euler: -199.53268 vol 0.8338981\n",
      "in 91904 euler: -199.51015 vol 0.83387905\n",
      "in 92032 euler: -199.58559 vol 0.83394516\n",
      "in 92160 euler: -199.52 vol 0.8338989\n",
      "in 92288 euler: -199.47005 vol 0.83386093\n",
      "in 92416 euler: -199.4156 vol 0.8337817\n",
      "in 92544 euler: -199.48178 vol 0.83378804\n",
      "in 92672 euler: -199.50769 vol 0.8338611\n",
      "in 92800 euler: -199.48746 vol 0.8338194\n",
      "in 92928 euler: -199.68573 vol 0.83402085\n",
      "in 93056 euler: -199.77005 vol 0.83413905\n",
      "in 93184 euler: -199.86598 vol 0.8342481\n",
      "in 93312 euler: -199.85541 vol 0.83428854\n",
      "in 93440 euler: -199.90063 vol 0.8342892\n",
      "in 93568 euler: -199.90381 vol 0.834321\n",
      "in 93696 euler: -199.89575 vol 0.83431107\n",
      "in 93824 euler: -199.91188 vol 0.8343219\n",
      "in 93952 euler: -200.01268 vol 0.8343859\n",
      "in 94080 euler: -200.08665 vol 0.8344108\n",
      "in 94208 euler: -200.1077 vol 0.83445525\n",
      "in 94336 euler: -200.05318 vol 0.8344773\n",
      "in 94464 euler: -200.03868 vol 0.83445513\n",
      "in 94592 euler: -199.93784 vol 0.83439434\n",
      "in 94720 euler: -199.87334 vol 0.83435\n",
      "in 94848 euler: -199.73723 vol 0.8342147\n",
      "in 94976 euler: -199.75815 vol 0.83418614\n",
      "in 95104 euler: -199.88812 vol 0.83424544\n",
      "in 95232 euler: -199.85387 vol 0.8342246\n",
      "in 95360 euler: -199.78448 vol 0.8341615\n",
      "in 95488 euler: -199.7884 vol 0.8341953\n",
      "in 95616 euler: -199.86337 vol 0.8342243\n",
      "in 95744 euler: -199.89735 vol 0.8342623\n",
      "in 95872 euler: -200.06238 vol 0.83435553\n",
      "in 96000 euler: -200.10654 vol 0.83437955\n",
      "in 96128 euler: -200.14528 vol 0.83442116\n",
      "in 96256 euler: -200.16872 vol 0.8344127\n",
      "in 96384 euler: -200.24597 vol 0.8344994\n",
      "in 96512 euler: -200.29637 vol 0.8345111\n",
      "in 96640 euler: -200.27377 vol 0.83449936\n",
      "in 96768 euler: -200.30502 vol 0.8345374\n",
      "in 96896 euler: -200.26242 vol 0.83453625\n",
      "in 97024 euler: -200.25237 vol 0.834578\n",
      "in 97152 euler: -200.2075 vol 0.83454067\n",
      "in 97280 euler: -200.19087 vol 0.8345351\n",
      "in 97408 euler: -200.12843 vol 0.83446866\n",
      "in 97536 euler: -200.1346 vol 0.8345079\n",
      "in 97664 euler: -200.17868 vol 0.83460546\n",
      "in 97792 euler: -200.14812 vol 0.8345699\n",
      "in 97920 euler: -200.11722 vol 0.8345732\n",
      "in 98048 euler: -200.155 vol 0.8346387\n",
      "in 98176 euler: -200.26729 vol 0.83465344\n",
      "in 98304 euler: -200.2593 vol 0.83464116\n",
      "in 98432 euler: -200.14569 vol 0.83456975\n",
      "in 98560 euler: -200.0882 vol 0.8345318\n",
      "in 98688 euler: -200.06326 vol 0.8345107\n",
      "in 98816 euler: -200.02385 vol 0.8345092\n",
      "in 98944 euler: -200.04341 vol 0.8345344\n",
      "in 99072 euler: -200.00444 vol 0.8345102\n",
      "in 99200 euler: -199.91084 vol 0.8345095\n",
      "in 99328 euler: -199.943 vol 0.83443916\n",
      "in 99456 euler: -199.96469 vol 0.8344366\n",
      "in 99584 euler: -199.93192 vol 0.8343692\n",
      "in 99712 euler: -199.91078 vol 0.83438015\n",
      "in 99840 euler: -199.8878 vol 0.8343033\n",
      "in 99968 euler: -199.79901 vol 0.8341916\n",
      "in 100096 euler: -199.76138 vol 0.8342194\n",
      "in 100224 euler: -199.81697 vol 0.83422804\n",
      "in 100352 euler: -199.8271 vol 0.83427334\n",
      "in 100480 euler: -199.92412 vol 0.83425784\n",
      "in 100608 euler: -199.91675 vol 0.83421206\n",
      "in 100736 euler: -199.85991 vol 0.8341856\n",
      "in 100864 euler: -199.87064 vol 0.8341382\n",
      "in 100992 euler: -199.84595 vol 0.83408123\n",
      "in 101120 euler: -199.922 vol 0.8341719\n",
      "in 101248 euler: -199.87177 vol 0.83413815\n",
      "in 101376 euler: -199.80206 vol 0.83410805\n",
      "in 101504 euler: -199.77945 vol 0.8340983\n",
      "in 101632 euler: -199.86154 vol 0.8341399\n",
      "in 101760 euler: -199.85812 vol 0.8341237\n",
      "in 101888 euler: -199.85196 vol 0.8341594\n",
      "in 102016 euler: -199.92485 vol 0.8341861\n",
      "in 102144 euler: -199.8353 vol 0.8341006\n",
      "in 102272 euler: -199.78954 vol 0.83407176\n",
      "in 102400 euler: -199.71915 vol 0.8340057\n",
      "in 102528 euler: -199.80544 vol 0.8341099\n",
      "in 102656 euler: -199.83786 vol 0.8341601\n",
      "in 102784 euler: -199.80495 vol 0.83415735\n",
      "in 102912 euler: -199.81085 vol 0.83418447\n",
      "in 103040 euler: -199.75836 vol 0.8341426\n",
      "in 103168 euler: -199.75926 vol 0.83412474\n",
      "in 103296 euler: -199.75381 vol 0.83415633\n",
      "in 103424 euler: -199.73755 vol 0.8341415\n",
      "in 103552 euler: -199.87537 vol 0.83417\n",
      "in 103680 euler: -199.79274 vol 0.8340329\n",
      "in 103808 euler: -199.73029 vol 0.83399564\n",
      "in 103936 euler: -199.86908 vol 0.8340946\n",
      "in 104064 euler: -199.92505 vol 0.8341721\n",
      "in 104192 euler: -200.00067 vol 0.8342567\n",
      "in 104320 euler: -200.0524 vol 0.83433574\n",
      "in 104448 euler: -200.08383 vol 0.83436537\n",
      "in 104576 euler: -200.13268 vol 0.8344329\n",
      "in 104704 euler: -200.14723 vol 0.834452\n",
      "in 104832 euler: -200.1225 vol 0.8343898\n",
      "in 104960 euler: -200.17847 vol 0.83446306\n",
      "in 105088 euler: -200.15146 vol 0.83447903\n",
      "in 105216 euler: -200.1393 vol 0.8344916\n",
      "in 105344 euler: -200.10066 vol 0.8344923\n",
      "in 105472 euler: -200.01299 vol 0.8344196\n",
      "in 105600 euler: -200.05276 vol 0.8344833\n",
      "in 105728 euler: -200.05011 vol 0.8344322\n",
      "in 105856 euler: -200.06319 vol 0.83438337\n",
      "in 105984 euler: -200.03502 vol 0.8343531\n",
      "in 106112 euler: -200.1149 vol 0.8344013\n",
      "in 106240 euler: -200.10988 vol 0.8343889\n",
      "in 106368 euler: -200.11082 vol 0.8343462\n",
      "in 106496 euler: -200.0786 vol 0.8343108\n",
      "in 106624 euler: -200.12184 vol 0.8343813\n",
      "in 106752 euler: -200.25064 vol 0.8344926\n",
      "in 106880 euler: -200.27217 vol 0.8345113\n",
      "in 107008 euler: -200.21739 vol 0.8344097\n",
      "in 107136 euler: -200.32465 vol 0.8345223\n",
      "in 107264 euler: -200.31998 vol 0.8344728\n",
      "in 107392 euler: -200.3181 vol 0.8344846\n",
      "in 107520 euler: -200.3794 vol 0.8345132\n",
      "in 107648 euler: -200.35164 vol 0.8344053\n",
      "in 107776 euler: -200.43967 vol 0.8344416\n",
      "in 107904 euler: -200.567 vol 0.83449453\n",
      "in 108032 euler: -200.55426 vol 0.8345024\n",
      "in 108160 euler: -200.52455 vol 0.834445\n",
      "in 108288 euler: -200.51843 vol 0.83444595\n",
      "in 108416 euler: -200.48347 vol 0.8344084\n",
      "in 108544 euler: -200.49127 vol 0.8343864\n",
      "in 108672 euler: -200.49695 vol 0.8344187\n",
      "in 108800 euler: -200.54628 vol 0.83447033\n",
      "in 108928 euler: -200.53896 vol 0.8344854\n",
      "in 109056 euler: -200.63744 vol 0.83457214\n",
      "in 109184 euler: -200.52422 vol 0.8344292\n",
      "in 109312 euler: -200.51689 vol 0.83441526\n",
      "in 109440 euler: -200.49104 vol 0.8344618\n",
      "in 109568 euler: -200.4831 vol 0.8344432\n",
      "in 109696 euler: -200.45627 vol 0.8343869\n",
      "in 109824 euler: -200.34708 vol 0.8342865\n",
      "in 109952 euler: -200.28636 vol 0.8342092\n",
      "in 110080 euler: -200.28572 vol 0.8342067\n",
      "in 110208 euler: -200.28621 vol 0.83415127\n",
      "in 110336 euler: -200.40065 vol 0.8342384\n",
      "in 110464 euler: -200.3673 vol 0.83420163\n",
      "in 110592 euler: -200.31041 vol 0.83414274\n",
      "in 110720 euler: -200.27321 vol 0.8341265\n",
      "in 110848 euler: -200.25862 vol 0.8341297\n",
      "in 110976 euler: -200.2147 vol 0.83408636\n",
      "in 111104 euler: -200.192 vol 0.83408624\n",
      "in 111232 euler: -200.16808 vol 0.8340993\n",
      "in 111360 euler: -200.17924 vol 0.8340996\n",
      "in 111488 euler: -200.08202 vol 0.83395857\n",
      "in 111616 euler: -200.12473 vol 0.833998\n",
      "in 111744 euler: -200.12808 vol 0.83397895\n",
      "in 111872 euler: -200.07619 vol 0.83394134\n",
      "in 112000 euler: -200.19115 vol 0.83405846\n",
      "in 112128 euler: -200.15733 vol 0.83403337\n",
      "in 112256 euler: -200.19788 vol 0.83407766\n",
      "in 112384 euler: -200.27267 vol 0.8341584\n",
      "in 112512 euler: -200.24858 vol 0.83417845\n",
      "in 112640 euler: -200.27692 vol 0.8341443\n",
      "in 112768 euler: -200.29358 vol 0.8341569\n",
      "in 112896 euler: -200.35869 vol 0.8341771\n",
      "in 113024 euler: -200.35121 vol 0.83418703\n",
      "in 113152 euler: -200.40077 vol 0.8342242\n",
      "in 113280 euler: -200.48119 vol 0.83429676\n",
      "in 113408 euler: -200.466 vol 0.8342697\n",
      "in 113536 euler: -200.47705 vol 0.8342965\n",
      "in 113664 euler: -200.44981 vol 0.83428323\n",
      "in 113792 euler: -200.43367 vol 0.8342458\n",
      "in 113920 euler: -200.40312 vol 0.8341989\n",
      "in 114048 euler: -200.37099 vol 0.83419687\n",
      "in 114176 euler: -200.34465 vol 0.83412385\n",
      "in 114304 euler: -200.28224 vol 0.8341431\n",
      "in 114432 euler: -200.28117 vol 0.83416224\n",
      "in 114560 euler: -200.34613 vol 0.8342391\n",
      "in 114688 euler: -200.37523 vol 0.83423215\n",
      "in 114816 euler: -200.46712 vol 0.83430785\n",
      "in 114944 euler: -200.4298 vol 0.83429134\n",
      "in 115072 euler: -200.44577 vol 0.8343238\n",
      "in 115200 euler: -200.35727 vol 0.8342129\n",
      "in 115328 euler: -200.46562 vol 0.8343348\n",
      "in 115456 euler: -200.51993 vol 0.83440447\n",
      "in 115584 euler: -200.54703 vol 0.83437747\n",
      "in 115712 euler: -200.44742 vol 0.8343097\n",
      "in 115840 euler: -200.44261 vol 0.8343341\n",
      "in 115968 euler: -200.57385 vol 0.8343908\n",
      "in 116096 euler: -200.57346 vol 0.83442175\n",
      "in 116224 euler: -200.71246 vol 0.8344783\n",
      "in 116352 euler: -200.74597 vol 0.8345047\n",
      "in 116480 euler: -200.74684 vol 0.8345116\n",
      "in 116608 euler: -200.71864 vol 0.83446383\n",
      "in 116736 euler: -200.69267 vol 0.8344563\n",
      "in 116864 euler: -200.70415 vol 0.834501\n",
      "in 116992 euler: -200.77539 vol 0.8345612\n",
      "in 117120 euler: -200.72336 vol 0.83452666\n",
      "in 117248 euler: -200.72182 vol 0.83453596\n",
      "in 117376 euler: -200.72066 vol 0.83453655\n",
      "in 117504 euler: -200.69193 vol 0.83449477\n",
      "in 117632 euler: -200.76534 vol 0.834557\n",
      "in 117760 euler: -200.80234 vol 0.8345678\n",
      "in 117888 euler: -200.78682 vol 0.8345829\n",
      "in 118016 euler: -200.76813 vol 0.834589\n",
      "in 118144 euler: -200.84853 vol 0.8346521\n",
      "in 118272 euler: -200.82033 vol 0.8346086\n",
      "in 118400 euler: -200.78162 vol 0.8345538\n",
      "in 118528 euler: -200.8039 vol 0.83457035\n",
      "in 118656 euler: -200.80403 vol 0.83455426\n",
      "in 118784 euler: -200.86127 vol 0.8346124\n",
      "in 118912 euler: -200.92432 vol 0.834652\n",
      "in 119040 euler: -200.88062 vol 0.8346136\n",
      "in 119168 euler: -200.81378 vol 0.83458555\n",
      "in 119296 euler: -200.77469 vol 0.83453804\n",
      "in 119424 euler: -200.88327 vol 0.834623\n",
      "in 119552 euler: -200.89574 vol 0.8346318\n",
      "in 119680 euler: -200.9606 vol 0.83471495\n",
      "in 119808 euler: -201.0385 vol 0.8347542\n",
      "in 119936 euler: -200.98984 vol 0.8347415\n",
      "in 120064 euler: -200.95876 vol 0.8346946\n",
      "in 120192 euler: -200.90672 vol 0.8346822\n",
      "in 120320 euler: -200.85864 vol 0.8346407\n",
      "in 120448 euler: -200.8926 vol 0.8346879\n",
      "in 120576 euler: -200.84875 vol 0.83465993\n",
      "in 120704 euler: -200.78665 vol 0.8345934\n",
      "in 120832 euler: -200.75513 vol 0.83453137\n",
      "in 120960 euler: -200.68863 vol 0.83445054\n",
      "in 121088 euler: -200.69612 vol 0.8344823\n",
      "in 121216 euler: -200.71205 vol 0.8344649\n",
      "in 121344 euler: -200.7015 vol 0.834455\n",
      "in 121472 euler: -200.59076 vol 0.83432204\n",
      "in 121600 euler: -200.54054 vol 0.834308\n",
      "in 121728 euler: -200.48894 vol 0.83431053\n",
      "in 121856 euler: -200.50806 vol 0.8343235\n",
      "in 121984 euler: -200.50687 vol 0.83434224\n",
      "in 122112 euler: -200.51184 vol 0.83436656\n",
      "in 122240 euler: -200.43767 vol 0.83427644\n",
      "in 122368 euler: -200.49562 vol 0.83436066\n",
      "in 122496 euler: -200.49722 vol 0.8343516\n",
      "in 122624 euler: -200.51797 vol 0.83437264\n",
      "in 122752 euler: -200.49557 vol 0.8343382\n",
      "in 122880 euler: -200.49472 vol 0.8342827\n",
      "in 123008 euler: -200.50676 vol 0.83426994\n",
      "in 123136 euler: -200.4758 vol 0.8342323\n",
      "in 123264 euler: -200.4536 vol 0.8342094\n",
      "in 123392 euler: -200.49072 vol 0.83423656\n",
      "in 123520 euler: -200.44337 vol 0.83419573\n",
      "in 123648 euler: -200.42398 vol 0.8341223\n",
      "in 123776 euler: -200.44989 vol 0.83414793\n",
      "in 123904 euler: -200.45396 vol 0.8341418\n",
      "in 124032 euler: -200.47496 vol 0.8341868\n",
      "in 124160 euler: -200.49112 vol 0.83419204\n",
      "in 124288 euler: -200.54297 vol 0.8342428\n",
      "in 124416 euler: -200.56552 vol 0.8342563\n",
      "in 124544 euler: -200.49239 vol 0.8342122\n",
      "in 124672 euler: -200.46974 vol 0.83419293\n",
      "in 124800 euler: -200.47061 vol 0.83424765\n",
      "in 124928 euler: -200.4888 vol 0.8342548\n",
      "in 125056 euler: -200.43907 vol 0.8342215\n",
      "in 125184 euler: -200.39995 vol 0.83420247\n",
      "in 125312 euler: -200.4555 vol 0.8342112\n",
      "in 125440 euler: -200.65485 vol 0.8343086\n",
      "in 125568 euler: -200.61005 vol 0.8342565\n",
      "in 125696 euler: -200.5706 vol 0.83422935\n",
      "in 125824 euler: -200.56429 vol 0.83423144\n",
      "in 125952 euler: -200.5437 vol 0.83419067\n",
      "in 126080 euler: -200.48024 vol 0.83413136\n",
      "in 126208 euler: -200.50273 vol 0.83415693\n",
      "in 126336 euler: -200.57555 vol 0.8342406\n",
      "in 126464 euler: -200.65416 vol 0.83428675\n",
      "in 126592 euler: -200.73961 vol 0.83440167\n",
      "in 126720 euler: -200.68588 vol 0.8343672\n",
      "in 126848 euler: -200.69696 vol 0.83438444\n",
      "in 126976 euler: -200.77048 vol 0.8344414\n",
      "in 127104 euler: -200.78874 vol 0.83445853\n",
      "in 127232 euler: -200.72763 vol 0.83437485\n",
      "in 127360 euler: -200.67934 vol 0.83429503\n",
      "in 127488 euler: -200.67018 vol 0.834301\n",
      "in 127616 euler: -200.73325 vol 0.83436143\n",
      "in 127744 euler: -200.75462 vol 0.8343831\n",
      "in 127872 euler: -200.8244 vol 0.8344432\n",
      "in 128000 euler: -200.83853 vol 0.83446145\n",
      "in 128128 euler: -200.8035 vol 0.8344345\n",
      "in 128256 euler: -200.89285 vol 0.83448386\n",
      "in 128384 euler: -200.85652 vol 0.8344808\n",
      "in 128512 euler: -200.92316 vol 0.83456475\n",
      "in 128640 euler: -201.01187 vol 0.83465433\n",
      "in 128768 euler: -200.96347 vol 0.83461994\n",
      "in 128896 euler: -200.96089 vol 0.8346299\n",
      "in 129024 euler: -200.92429 vol 0.83460057\n",
      "in 129152 euler: -200.83728 vol 0.83451986\n",
      "in 129280 euler: -200.85573 vol 0.8345217\n",
      "in 129408 euler: -200.92448 vol 0.834556\n",
      "in 129536 euler: -200.9612 vol 0.8345727\n",
      "in 129664 euler: -200.94885 vol 0.83456004\n",
      "in 129792 euler: -200.97032 vol 0.8345563\n",
      "in 129920 euler: -200.9709 vol 0.8345491\n",
      "in 130048 euler: -200.92006 vol 0.83448637\n",
      "in 130176 euler: -200.96223 vol 0.83450663\n",
      "in 130304 euler: -200.94957 vol 0.83452576\n",
      "in 130432 euler: -200.972 vol 0.8345846\n",
      "in 130560 euler: -201.01978 vol 0.8346585\n",
      "in 130688 euler: -201.03871 vol 0.83470917\n",
      "in 130816 euler: -201.00098 vol 0.8347237\n",
      "in 130944 euler: -200.9605 vol 0.83469397\n",
      "in 131072 euler: -200.9383 vol 0.8346073\n",
      "in 131200 euler: -200.96274 vol 0.834638\n",
      "in 131328 euler: -200.96715 vol 0.83465683\n",
      "in 131456 euler: -201.03206 vol 0.8346945\n",
      "in 131584 euler: -201.0106 vol 0.8346822\n",
      "in 131712 euler: -201.0287 vol 0.83470815\n",
      "in 131840 euler: -201.04472 vol 0.8347204\n",
      "in 131968 euler: -201.15894 vol 0.8348041\n",
      "in 132096 euler: -201.17897 vol 0.8348328\n",
      "in 132224 euler: -201.19385 vol 0.8348481\n",
      "in 132352 euler: -201.17749 vol 0.8348751\n",
      "in 132480 euler: -201.20114 vol 0.8348788\n",
      "in 132608 euler: -201.13837 vol 0.8348097\n",
      "in 132736 euler: -201.15994 vol 0.8348511\n",
      "in 132864 euler: -201.16798 vol 0.8348534\n",
      "in 132992 euler: -201.11879 vol 0.8348143\n",
      "in 133120 euler: -201.13687 vol 0.83482367\n",
      "in 133248 euler: -201.14774 vol 0.83484674\n",
      "in 133376 euler: -201.1647 vol 0.8348452\n",
      "in 133504 euler: -201.08145 vol 0.8347714\n",
      "in 133632 euler: -201.06741 vol 0.83471924\n",
      "in 133760 euler: -201.08377 vol 0.83475393\n",
      "in 133888 euler: -201.0358 vol 0.8347678\n",
      "in 134016 euler: -201.0676 vol 0.8348057\n",
      "in 134144 euler: -201.02353 vol 0.83481055\n",
      "in 134272 euler: -201.00021 vol 0.8347764\n",
      "in 134400 euler: -200.97983 vol 0.83475965\n",
      "in 134528 euler: -200.957 vol 0.83472526\n",
      "in 134656 euler: -200.93118 vol 0.8346843\n",
      "in 134784 euler: -200.93065 vol 0.83470607\n",
      "in 134912 euler: -200.9225 vol 0.8347064\n",
      "in 135040 euler: -200.89339 vol 0.8346411\n",
      "in 135168 euler: -200.8938 vol 0.8346565\n",
      "in 135296 euler: -200.97496 vol 0.8346711\n",
      "in 135424 euler: -200.97447 vol 0.8346508\n",
      "in 135552 euler: -201.04312 vol 0.83471125\n",
      "in 135680 euler: -201.10617 vol 0.83479017\n",
      "in 135808 euler: -201.09169 vol 0.83476675\n",
      "in 135936 euler: -201.04683 vol 0.83471406\n",
      "in 136064 euler: -201.09131 vol 0.8347223\n",
      "in 136192 euler: -201.07196 vol 0.8347137\n",
      "in 136320 euler: -201.08266 vol 0.8347158\n",
      "in 136448 euler: -201.11354 vol 0.8347556\n",
      "in 136576 euler: -201.1399 vol 0.83475786\n",
      "in 136704 euler: -201.12155 vol 0.8347387\n",
      "in 136832 euler: -201.14893 vol 0.83478665\n",
      "in 136960 euler: -201.10072 vol 0.83474475\n",
      "in 137088 euler: -201.0933 vol 0.83477926\n",
      "in 137216 euler: -201.12433 vol 0.8348122\n",
      "in 137344 euler: -201.21204 vol 0.83487713\n",
      "in 137472 euler: -201.20847 vol 0.8348639\n",
      "in 137600 euler: -201.17918 vol 0.834795\n",
      "in 137728 euler: -201.19937 vol 0.8348299\n",
      "in 137856 euler: -201.20888 vol 0.8348086\n",
      "in 137984 euler: -201.21565 vol 0.83487886\n",
      "in 138112 euler: -201.223 vol 0.8348841\n",
      "in 138240 euler: -201.22023 vol 0.834895\n",
      "in 138368 euler: -201.17708 vol 0.83483696\n",
      "in 138496 euler: -201.1731 vol 0.8348331\n",
      "in 138624 euler: -201.16443 vol 0.8348084\n",
      "in 138752 euler: -201.19226 vol 0.8347987\n",
      "in 138880 euler: -201.2317 vol 0.834802\n",
      "in 139008 euler: -201.2237 vol 0.834802\n",
      "in 139136 euler: -201.17299 vol 0.83479184\n",
      "in 139264 euler: -201.1591 vol 0.8348051\n",
      "in 139392 euler: -201.26134 vol 0.8348891\n",
      "in 139520 euler: -201.28256 vol 0.8349148\n",
      "in 139648 euler: -201.27155 vol 0.8349296\n",
      "in 139776 euler: -201.22719 vol 0.8349103\n",
      "in 139904 euler: -201.18661 vol 0.8348732\n",
      "in 140032 euler: -201.21574 vol 0.8349246\n",
      "in 140160 euler: -201.16772 vol 0.83489156\n",
      "in 140288 euler: -201.20613 vol 0.83491355\n",
      "in 140416 euler: -201.23026 vol 0.8349436\n",
      "in 140544 euler: -201.30838 vol 0.83500475\n",
      "in 140672 euler: -201.35959 vol 0.8350484\n",
      "in 140800 euler: -201.34734 vol 0.8350304\n",
      "in 140928 euler: -201.33296 vol 0.83501226\n",
      "in 141056 euler: -201.35115 vol 0.8350345\n",
      "in 141184 euler: -201.34418 vol 0.83502465\n",
      "in 141312 euler: -201.37086 vol 0.8349979\n",
      "in 141440 euler: -201.39124 vol 0.8350566\n",
      "in 141568 euler: -201.41187 vol 0.8350856\n",
      "in 141696 euler: -201.42662 vol 0.8350808\n",
      "in 141824 euler: -201.44557 vol 0.8351029\n",
      "in 141952 euler: -201.51624 vol 0.8351434\n",
      "in 142080 euler: -201.46638 vol 0.835121\n",
      "in 142208 euler: -201.46349 vol 0.83513683\n",
      "in 142336 euler: -201.41707 vol 0.83510286\n",
      "in 142464 euler: -201.44649 vol 0.83512723\n",
      "in 142592 euler: -201.42165 vol 0.8351176\n",
      "in 142720 euler: -201.46198 vol 0.8351454\n",
      "in 142848 euler: -201.37831 vol 0.83508253\n",
      "in 142976 euler: -201.40582 vol 0.83511657\n",
      "in 143104 euler: -201.39 vol 0.83510065\n",
      "in 143232 euler: -201.4625 vol 0.8351639\n",
      "in 143360 euler: -201.45766 vol 0.8351608\n",
      "in 143488 euler: -201.48865 vol 0.83521783\n",
      "in 143616 euler: -201.48036 vol 0.8352193\n",
      "in 143744 euler: -201.44537 vol 0.8351956\n",
      "in 143872 euler: -201.44969 vol 0.83522075\n",
      "in 144000 euler: -201.43102 vol 0.8352089\n",
      "in 144128 euler: -201.46164 vol 0.83524454\n",
      "in 144256 euler: -201.44203 vol 0.8352202\n",
      "in 144384 euler: -201.36613 vol 0.83511615\n",
      "in 144512 euler: -201.39581 vol 0.8351403\n",
      "in 144640 euler: -201.41837 vol 0.8351772\n",
      "in 144768 euler: -201.41924 vol 0.83519053\n",
      "in 144896 euler: -201.44357 vol 0.8352257\n",
      "in 145024 euler: -201.4692 vol 0.8352252\n",
      "in 145152 euler: -201.44844 vol 0.8352215\n",
      "in 145280 euler: -201.47783 vol 0.83520776\n",
      "in 145408 euler: -201.58403 vol 0.83523196\n",
      "in 145536 euler: -201.60246 vol 0.8352706\n",
      "in 145664 euler: -201.64996 vol 0.83526176\n",
      "in 145792 euler: -201.66907 vol 0.8352659\n",
      "in 145920 euler: -201.64365 vol 0.8352494\n",
      "in 146048 euler: -201.72658 vol 0.8353367\n",
      "in 146176 euler: -201.67195 vol 0.8352848\n",
      "in 146304 euler: -201.65909 vol 0.83528745\n",
      "in 146432 euler: -201.73894 vol 0.8353737\n",
      "in 146560 euler: -201.70432 vol 0.83535063\n",
      "in 146688 euler: -201.7151 vol 0.8353743\n",
      "in 146816 euler: -201.75246 vol 0.8354108\n",
      "in 146944 euler: -201.70586 vol 0.8353397\n",
      "in 147072 euler: -201.79353 vol 0.8354034\n",
      "in 147200 euler: -201.83417 vol 0.8354356\n",
      "in 147328 euler: -201.96065 vol 0.83552694\n",
      "in 147456 euler: -202.05646 vol 0.8355715\n",
      "in 147584 euler: -201.98674 vol 0.8355433\n",
      "in 147712 euler: -202.01587 vol 0.83556086\n",
      "in 147840 euler: -202.12091 vol 0.8356452\n",
      "in 147968 euler: -202.11548 vol 0.83569634\n",
      "in 148096 euler: -202.09207 vol 0.83570164\n",
      "in 148224 euler: -202.05692 vol 0.8356868\n",
      "in 148352 euler: -201.99916 vol 0.83563983\n",
      "in 148480 euler: -202.00047 vol 0.8356285\n",
      "in 148608 euler: -201.97914 vol 0.8356196\n",
      "in 148736 euler: -202.01108 vol 0.8356355\n",
      "in 148864 euler: -201.99019 vol 0.83562464\n",
      "in 148992 euler: -202.01485 vol 0.8356105\n",
      "in 149120 euler: -202.03465 vol 0.8356319\n",
      "in 149248 euler: -202.05261 vol 0.83562773\n",
      "in 149376 euler: -202.08777 vol 0.83564603\n",
      "in 149504 euler: -202.08392 vol 0.83562666\n",
      "in 149632 euler: -202.02861 vol 0.8355866\n",
      "in 149760 euler: -201.9763 vol 0.83554405\n",
      "in 149888 euler: -201.97728 vol 0.8355132\n",
      "in 150016 euler: -201.93433 vol 0.8354354\n",
      "in 150144 euler: -201.98537 vol 0.8354862\n",
      "in 150272 euler: -201.97025 vol 0.83548176\n",
      "in 150400 euler: -201.97298 vol 0.8355159\n",
      "in 150528 euler: -202.01558 vol 0.8355888\n",
      "in 150656 euler: -201.98842 vol 0.8355798\n",
      "in 150784 euler: -201.93317 vol 0.8355292\n",
      "in 150912 euler: -201.99309 vol 0.8355595\n",
      "in 151040 euler: -201.99953 vol 0.83559054\n",
      "in 151168 euler: -202.02406 vol 0.83559054\n",
      "in 151296 euler: -201.99728 vol 0.83562857\n",
      "in 151424 euler: -201.99924 vol 0.8356691\n",
      "in 151552 euler: -202.01134 vol 0.8356777\n",
      "in 151680 euler: -201.99814 vol 0.8356318\n",
      "in 151808 euler: -201.96176 vol 0.8356119\n",
      "in 151936 euler: -201.91003 vol 0.835589\n",
      "in 152064 euler: -201.92148 vol 0.8355978\n",
      "in 152192 euler: -201.96625 vol 0.8356469\n",
      "in 152320 euler: -201.9308 vol 0.8356197\n",
      "in 152448 euler: -201.89062 vol 0.835605\n",
      "in 152576 euler: -201.87927 vol 0.8356036\n",
      "in 152704 euler: -201.94127 vol 0.835662\n",
      "in 152832 euler: -201.98227 vol 0.83566475\n",
      "in 152960 euler: -201.98184 vol 0.83564717\n",
      "in 153088 euler: -202.06337 vol 0.83571017\n",
      "in 153216 euler: -202.04848 vol 0.8357292\n",
      "in 153344 euler: -202.0895 vol 0.8357576\n",
      "in 153472 euler: -202.08658 vol 0.83575267\n",
      "in 153600 euler: -202.08565 vol 0.83572716\n",
      "in 153728 euler: -202.05327 vol 0.8356861\n",
      "in 153856 euler: -202.0924 vol 0.83569247\n",
      "in 153984 euler: -202.10735 vol 0.8356969\n",
      "in 154112 euler: -202.10394 vol 0.83570987\n",
      "in 154240 euler: -202.07178 vol 0.8356879\n",
      "in 154368 euler: -202.11523 vol 0.83575106\n",
      "in 154496 euler: -202.09166 vol 0.83574086\n",
      "in 154624 euler: -202.15358 vol 0.83575666\n",
      "in 154752 euler: -202.10847 vol 0.8357431\n",
      "in 154880 euler: -202.16107 vol 0.8357446\n",
      "in 155008 euler: -202.12488 vol 0.8357301\n",
      "in 155136 euler: -202.10368 vol 0.8356989\n",
      "in 155264 euler: -202.08717 vol 0.8357141\n",
      "in 155392 euler: -202.09033 vol 0.83572966\n",
      "in 155520 euler: -202.07935 vol 0.835721\n",
      "in 155648 euler: -202.0321 vol 0.83569354\n",
      "in 155776 euler: -202.08829 vol 0.8357115\n",
      "in 155904 euler: -202.11353 vol 0.8357223\n",
      "in 156032 euler: -202.09047 vol 0.8357069\n",
      "in 156160 euler: -202.06189 vol 0.83569175\n",
      "in 156288 euler: -202.067 vol 0.8357055\n",
      "in 156416 euler: -202.08739 vol 0.8357165\n",
      "in 156544 euler: -202.11992 vol 0.8357555\n",
      "in 156672 euler: -202.11241 vol 0.83579725\n",
      "in 156800 euler: -202.05571 vol 0.8357913\n",
      "in 156928 euler: -202.03935 vol 0.83576715\n",
      "in 157056 euler: -202.04198 vol 0.83581036\n",
      "in 157184 euler: -202.04945 vol 0.83582425\n",
      "in 157312 euler: -202.03766 vol 0.83581305\n",
      "in 157440 euler: -202.00366 vol 0.83578587\n",
      "in 157568 euler: -201.92802 vol 0.83570963\n",
      "in 157696 euler: -201.8575 vol 0.83565474\n",
      "in 157824 euler: -201.81935 vol 0.8356304\n",
      "in 157952 euler: -201.77765 vol 0.8356239\n",
      "in 158080 euler: -201.76811 vol 0.835629\n",
      "in 158208 euler: -201.79276 vol 0.8356272\n",
      "in 158336 euler: -201.85202 vol 0.8356912\n",
      "in 158464 euler: -201.81691 vol 0.83565414\n",
      "in 158592 euler: -201.82355 vol 0.8356701\n",
      "in 158720 euler: -201.89783 vol 0.8357168\n",
      "in 158848 euler: -201.93443 vol 0.83572423\n",
      "in 158976 euler: -201.88788 vol 0.8356497\n",
      "in 159104 euler: -201.8406 vol 0.83560085\n",
      "in 159232 euler: -201.83057 vol 0.8356111\n",
      "in 159360 euler: -201.80087 vol 0.8356015\n",
      "in 159488 euler: -201.82239 vol 0.83560497\n",
      "in 159616 euler: -201.7974 vol 0.83556396\n",
      "in 159744 euler: -201.8175 vol 0.83558315\n",
      "in 159872 euler: -201.7726 vol 0.8355438\n",
      "in 160000 euler: -201.73637 vol 0.8355456\n",
      "in 160128 euler: -201.69157 vol 0.83548963\n",
      "in 160256 euler: -201.73215 vol 0.83551913\n",
      "in 160384 euler: -201.73268 vol 0.8354874\n",
      "in 160512 euler: -201.68297 vol 0.8354354\n",
      "in 160640 euler: -201.70598 vol 0.83543295\n",
      "in 160768 euler: -201.68037 vol 0.8354075\n",
      "in 160896 euler: -201.70383 vol 0.83542514\n",
      "in 161024 euler: -201.67497 vol 0.8354251\n",
      "in 161152 euler: -201.5969 vol 0.8353352\n",
      "in 161280 euler: -201.57114 vol 0.83528286\n",
      "in 161408 euler: -201.53293 vol 0.8352415\n",
      "in 161536 euler: -201.52638 vol 0.8352403\n",
      "in 161664 euler: -201.47427 vol 0.83516663\n",
      "in 161792 euler: -201.46964 vol 0.8351676\n",
      "in 161920 euler: -201.52332 vol 0.8351985\n",
      "in 162048 euler: -201.50476 vol 0.8351854\n",
      "in 162176 euler: -201.47072 vol 0.83515036\n",
      "in 162304 euler: -201.46417 vol 0.83514136\n",
      "in 162432 euler: -201.50911 vol 0.83517283\n",
      "in 162560 euler: -201.46126 vol 0.83511734\n",
      "in 162688 euler: -201.47237 vol 0.83512825\n",
      "in 162816 euler: -201.44008 vol 0.8351135\n",
      "in 162944 euler: -201.50732 vol 0.8351851\n",
      "in 163072 euler: -201.5225 vol 0.83522767\n",
      "in 163200 euler: -201.51897 vol 0.83520305\n",
      "in 163328 euler: -201.54858 vol 0.83520555\n",
      "in 163456 euler: -201.49472 vol 0.83517945\n",
      "in 163584 euler: -201.46939 vol 0.8351464\n",
      "in 163712 euler: -201.44403 vol 0.83513266\n",
      "in 163840 euler: -201.44241 vol 0.83513033\n",
      "in 163968 euler: -201.46336 vol 0.8351384\n",
      "in 164096 euler: -201.48326 vol 0.83519197\n",
      "in 164224 euler: -201.48474 vol 0.83520776\n",
      "in 164352 euler: -201.48743 vol 0.83521754\n",
      "in 164480 euler: -201.4463 vol 0.8352089\n",
      "in 164608 euler: -201.49323 vol 0.8352176\n",
      "in 164736 euler: -201.51442 vol 0.83522207\n",
      "in 164864 euler: -201.53458 vol 0.8352411\n",
      "in 164992 euler: -201.58954 vol 0.8352741\n",
      "in 165120 euler: -201.60579 vol 0.8352342\n",
      "in 165248 euler: -201.59601 vol 0.83521587\n",
      "in 165376 euler: -201.63506 vol 0.8352519\n",
      "in 165504 euler: -201.71608 vol 0.8353152\n",
      "in 165632 euler: -201.65833 vol 0.8352378\n",
      "in 165760 euler: -201.67604 vol 0.8352422\n",
      "in 165888 euler: -201.7279 vol 0.8353229\n",
      "in 166016 euler: -201.71347 vol 0.8353452\n",
      "in 166144 euler: -201.70108 vol 0.83533216\n",
      "in 166272 euler: -201.7252 vol 0.8353446\n",
      "in 166400 euler: -201.72455 vol 0.8353326\n",
      "in 166528 euler: -201.69745 vol 0.83531505\n",
      "in 166656 euler: -201.69312 vol 0.8353401\n",
      "in 166784 euler: -201.80965 vol 0.8354009\n",
      "in 166912 euler: -201.78815 vol 0.83540356\n",
      "in 167040 euler: -201.81485 vol 0.8354289\n",
      "in 167168 euler: -201.80644 vol 0.8354171\n",
      "in 167296 euler: -201.84814 vol 0.83544195\n",
      "in 167424 euler: -201.79616 vol 0.835393\n",
      "in 167552 euler: -201.77419 vol 0.8353417\n",
      "in 167680 euler: -201.77997 vol 0.8353481\n",
      "in 167808 euler: -201.81949 vol 0.835387\n",
      "in 167936 euler: -201.80774 vol 0.83538735\n",
      "in 168064 euler: -201.83934 vol 0.8354286\n",
      "in 168192 euler: -201.86435 vol 0.83547133\n",
      "in 168320 euler: -201.86386 vol 0.83547854\n",
      "in 168448 euler: -201.8703 vol 0.83545285\n",
      "in 168576 euler: -201.82388 vol 0.83540046\n",
      "in 168704 euler: -201.78798 vol 0.8353683\n",
      "in 168832 euler: -201.80426 vol 0.8353682\n",
      "in 168960 euler: -201.79555 vol 0.83536685\n",
      "in 169088 euler: -201.73784 vol 0.8353295\n",
      "in 169216 euler: -201.74738 vol 0.83533233\n",
      "in 169344 euler: -201.78268 vol 0.83535624\n",
      "in 169472 euler: -201.8153 vol 0.835395\n",
      "in 169600 euler: -201.82716 vol 0.8354177\n",
      "in 169728 euler: -201.77715 vol 0.83535385\n",
      "in 169856 euler: -201.80006 vol 0.8353922\n",
      "in 169984 euler: -201.8312 vol 0.83541095\n",
      "in 170112 euler: -201.89659 vol 0.8354365\n",
      "in 170240 euler: -201.88881 vol 0.83543676\n",
      "in 170368 euler: -201.89911 vol 0.8354346\n",
      "in 170496 euler: -201.84502 vol 0.8353829\n",
      "in 170624 euler: -201.88152 vol 0.83540124\n",
      "in 170752 euler: -201.87596 vol 0.83540756\n",
      "in 170880 euler: -201.85655 vol 0.83540505\n",
      "in 171008 euler: -201.83463 vol 0.8353703\n",
      "in 171136 euler: -201.86488 vol 0.83536524\n",
      "in 171264 euler: -201.82684 vol 0.8353409\n",
      "in 171392 euler: -201.7979 vol 0.83531976\n",
      "in 171520 euler: -201.75963 vol 0.83529127\n",
      "in 171648 euler: -201.75662 vol 0.83525676\n",
      "in 171776 euler: -201.72433 vol 0.8352457\n",
      "in 171904 euler: -201.65617 vol 0.83517855\n",
      "in 172032 euler: -201.65295 vol 0.8351893\n",
      "in 172160 euler: -201.68314 vol 0.83522785\n",
      "in 172288 euler: -201.73589 vol 0.8352736\n",
      "in 172416 euler: -201.73541 vol 0.83522373\n",
      "in 172544 euler: -201.76624 vol 0.83522195\n",
      "in 172672 euler: -201.78267 vol 0.83525366\n",
      "in 172800 euler: -201.76024 vol 0.8352462\n",
      "in 172928 euler: -201.76321 vol 0.8352815\n",
      "in 173056 euler: -201.74968 vol 0.8352658\n",
      "in 173184 euler: -201.7534 vol 0.8352716\n",
      "in 173312 euler: -201.73883 vol 0.835274\n",
      "in 173440 euler: -201.75815 vol 0.8352928\n",
      "in 173568 euler: -201.73746 vol 0.83527565\n",
      "in 173696 euler: -201.769 vol 0.8352911\n",
      "in 173824 euler: -201.82034 vol 0.8353148\n",
      "in 173952 euler: -201.82016 vol 0.8353146\n",
      "in 174080 euler: -201.86774 vol 0.83534455\n",
      "in 174208 euler: -201.90567 vol 0.83534527\n",
      "in 174336 euler: -201.89941 vol 0.8353423\n",
      "in 174464 euler: -201.86154 vol 0.8352792\n",
      "in 174592 euler: -201.89574 vol 0.83531433\n",
      "in 174720 euler: -201.92369 vol 0.8353498\n",
      "in 174848 euler: -201.90436 vol 0.8353463\n",
      "in 174976 euler: -201.92969 vol 0.8353444\n",
      "in 175104 euler: -201.90915 vol 0.83532035\n",
      "in 175232 euler: -201.89464 vol 0.8353476\n",
      "in 175360 euler: -201.86966 vol 0.8353306\n",
      "in 175488 euler: -201.87762 vol 0.83538526\n",
      "in 175616 euler: -201.90388 vol 0.8354125\n",
      "in 175744 euler: -201.83911 vol 0.83534247\n",
      "in 175872 euler: -201.83633 vol 0.835371\n",
      "in 176000 euler: -201.84709 vol 0.83540463\n",
      "in 176128 euler: -201.8696 vol 0.8354492\n",
      "in 176256 euler: -201.83075 vol 0.835399\n",
      "in 176384 euler: -201.81097 vol 0.83537483\n",
      "in 176512 euler: -201.76967 vol 0.8353169\n",
      "in 176640 euler: -201.77916 vol 0.8353385\n",
      "in 176768 euler: -201.80673 vol 0.83538383\n",
      "in 176896 euler: -201.7746 vol 0.83537376\n",
      "in 177024 euler: -201.77217 vol 0.83541083\n",
      "in 177152 euler: -201.84628 vol 0.8354741\n",
      "in 177280 euler: -201.82036 vol 0.83548015\n",
      "in 177408 euler: -201.81326 vol 0.83545846\n",
      "in 177536 euler: -201.89651 vol 0.83552325\n",
      "in 177664 euler: -201.88536 vol 0.8355291\n",
      "in 177792 euler: -201.91736 vol 0.8355687\n",
      "in 177920 euler: -201.9017 vol 0.8355845\n",
      "in 178048 euler: -201.90533 vol 0.83558744\n",
      "in 178176 euler: -201.84914 vol 0.83554393\n",
      "in 178304 euler: -201.86617 vol 0.8355357\n",
      "in 178432 euler: -201.87389 vol 0.83555037\n",
      "in 178560 euler: -201.8094 vol 0.8354864\n",
      "in 178688 euler: -201.80597 vol 0.83551997\n",
      "in 178816 euler: -201.84476 vol 0.8355452\n",
      "in 178944 euler: -201.85802 vol 0.8355643\n",
      "in 179072 euler: -201.93178 vol 0.83562696\n",
      "in 179200 euler: -201.96852 vol 0.83567226\n",
      "in 179328 euler: -201.98753 vol 0.8356857\n",
      "in 179456 euler: -201.95415 vol 0.835683\n",
      "in 179584 euler: -201.97359 vol 0.8357165\n",
      "in 179712 euler: -201.9388 vol 0.83568937\n",
      "in 179840 euler: -201.92361 vol 0.8356614\n",
      "in 179968 euler: -201.96013 vol 0.83569735\n",
      "in 180096 euler: -201.9332 vol 0.8356425\n",
      "in 180224 euler: -201.92914 vol 0.8356599\n",
      "in 180352 euler: -201.9904 vol 0.8357245\n",
      "in 180480 euler: -201.96788 vol 0.83570176\n",
      "in 180608 euler: -201.96587 vol 0.83572537\n",
      "in 180736 euler: -201.92427 vol 0.83569545\n",
      "in 180864 euler: -201.93443 vol 0.83572567\n",
      "in 180992 euler: -201.93468 vol 0.8357136\n",
      "in 181120 euler: -201.9413 vol 0.8357404\n",
      "in 181248 euler: -201.92938 vol 0.8357312\n",
      "in 181376 euler: -201.90901 vol 0.8357292\n",
      "in 181504 euler: -201.89598 vol 0.8357193\n",
      "in 181632 euler: -201.89302 vol 0.8357157\n",
      "in 181760 euler: -201.90526 vol 0.83570886\n",
      "in 181888 euler: -201.93521 vol 0.83574903\n",
      "in 182016 euler: -201.88779 vol 0.8357174\n",
      "in 182144 euler: -201.8728 vol 0.8357661\n",
      "in 182272 euler: -201.89108 vol 0.8357516\n",
      "in 182400 euler: -201.87245 vol 0.8357304\n",
      "in 182528 euler: -201.8448 vol 0.8357283\n",
      "in 182656 euler: -201.84442 vol 0.8357534\n",
      "in 182784 euler: -201.85373 vol 0.835755\n",
      "in 182912 euler: -201.79877 vol 0.83568424\n",
      "in 183040 euler: -201.8313 vol 0.83569616\n",
      "in 183168 euler: -201.80717 vol 0.8357015\n",
      "in 183296 euler: -201.85281 vol 0.8357522\n",
      "in 183424 euler: -201.82806 vol 0.8357634\n",
      "in 183552 euler: -201.84196 vol 0.83578026\n",
      "in 183680 euler: -201.86171 vol 0.8357973\n",
      "in 183808 euler: -201.87479 vol 0.83582395\n",
      "in 183936 euler: -201.85164 vol 0.8358242\n",
      "in 184064 euler: -201.83688 vol 0.8358115\n",
      "in 184192 euler: -201.82678 vol 0.8357984\n",
      "in 184320 euler: -201.82231 vol 0.83580256\n",
      "in 184448 euler: -201.81151 vol 0.8358034\n",
      "in 184576 euler: -201.85576 vol 0.83582103\n",
      "in 184704 euler: -201.85764 vol 0.8358152\n",
      "in 184832 euler: -201.90562 vol 0.83584553\n",
      "in 184960 euler: -201.92584 vol 0.8358481\n",
      "in 185088 euler: -201.8888 vol 0.83584994\n",
      "in 185216 euler: -201.87634 vol 0.8358295\n",
      "in 185344 euler: -201.90306 vol 0.83585227\n",
      "in 185472 euler: -201.89476 vol 0.8358652\n",
      "in 185600 euler: -201.8694 vol 0.8358255\n",
      "in 185728 euler: -201.85104 vol 0.83580434\n",
      "in 185856 euler: -201.8844 vol 0.8358386\n",
      "in 185984 euler: -201.81744 vol 0.835797\n",
      "in 186112 euler: -201.80751 vol 0.8357902\n",
      "in 186240 euler: -201.7948 vol 0.83581114\n",
      "in 186368 euler: -201.81937 vol 0.8358425\n",
      "in 186496 euler: -201.80034 vol 0.8358182\n",
      "in 186624 euler: -201.85648 vol 0.83585614\n",
      "in 186752 euler: -201.86964 vol 0.83587927\n",
      "in 186880 euler: -201.82423 vol 0.8358561\n",
      "in 187008 euler: -201.8107 vol 0.83584607\n",
      "in 187136 euler: -201.7879 vol 0.8358231\n",
      "in 187264 euler: -201.79181 vol 0.8358216\n",
      "in 187392 euler: -201.81635 vol 0.83585197\n",
      "in 187520 euler: -201.79942 vol 0.83585435\n",
      "in 187648 euler: -201.78989 vol 0.8358689\n",
      "in 187776 euler: -201.80373 vol 0.8358946\n",
      "in 187904 euler: -201.81769 vol 0.8359177\n",
      "in 188032 euler: -201.82254 vol 0.8359276\n",
      "in 188160 euler: -201.82202 vol 0.8359473\n",
      "in 188288 euler: -201.81824 vol 0.8359521\n",
      "in 188416 euler: -201.81767 vol 0.8359778\n",
      "in 188544 euler: -201.8714 vol 0.83600944\n",
      "in 188672 euler: -201.82397 vol 0.83598125\n",
      "in 188800 euler: -201.77264 vol 0.8359473\n",
      "in 188928 euler: -201.73485 vol 0.8358967\n",
      "in 189056 euler: -201.74644 vol 0.8359029\n",
      "in 189184 euler: -201.74141 vol 0.83589506\n",
      "in 189312 euler: -201.77415 vol 0.8359164\n",
      "in 189440 euler: -201.78679 vol 0.8359274\n",
      "in 189568 euler: -201.75095 vol 0.8358867\n",
      "in 189696 euler: -201.74574 vol 0.8358713\n",
      "in 189824 euler: -201.72899 vol 0.8358593\n",
      "in 189952 euler: -201.67348 vol 0.8357784\n",
      "in 190080 euler: -201.69861 vol 0.8357685\n",
      "in 190208 euler: -201.71812 vol 0.8357912\n",
      "in 190336 euler: -201.66972 vol 0.8357415\n",
      "in 190464 euler: -201.6905 vol 0.8357342\n",
      "in 190592 euler: -201.6487 vol 0.83570683\n",
      "in 190720 euler: -201.58626 vol 0.8356667\n",
      "in 190848 euler: -201.55579 vol 0.8356486\n",
      "in 190976 euler: -201.57053 vol 0.83566195\n",
      "in 191104 euler: -201.63315 vol 0.8357142\n",
      "in 191232 euler: -201.63115 vol 0.83572215\n",
      "in 191360 euler: -201.70438 vol 0.83578324\n",
      "in 191488 euler: -201.77774 vol 0.83582646\n",
      "in 191616 euler: -201.73885 vol 0.83580947\n",
      "in 191744 euler: -201.68652 vol 0.835748\n",
      "in 191872 euler: -201.65834 vol 0.83576095\n",
      "in 192000 euler: -201.62459 vol 0.835733\n",
      "in 192128 euler: -201.62202 vol 0.83571386\n",
      "in 192256 euler: -201.63512 vol 0.83573496\n",
      "in 192384 euler: -201.62515 vol 0.8357765\n",
      "in 192512 euler: -201.64589 vol 0.8357885\n",
      "in 192640 euler: -201.67444 vol 0.8357569\n",
      "in 192768 euler: -201.69919 vol 0.8357781\n",
      "in 192896 euler: -201.69751 vol 0.8357662\n",
      "in 193024 euler: -201.67221 vol 0.83572066\n",
      "in 193152 euler: -201.64464 vol 0.8357054\n",
      "in 193280 euler: -201.65302 vol 0.83572483\n",
      "in 193408 euler: -201.6339 vol 0.8357043\n",
      "in 193536 euler: -201.59172 vol 0.83566266\n",
      "in 193664 euler: -201.59456 vol 0.835655\n",
      "in 193792 euler: -201.63965 vol 0.83567256\n",
      "in 193920 euler: -201.61906 vol 0.8356468\n",
      "in 194048 euler: -201.60822 vol 0.8356097\n",
      "in 194176 euler: -201.53621 vol 0.83555126\n",
      "in 194304 euler: -201.57785 vol 0.83558065\n",
      "in 194432 euler: -201.57756 vol 0.8355995\n",
      "in 194560 euler: -201.57533 vol 0.8355933\n",
      "in 194688 euler: -201.58917 vol 0.8356011\n",
      "in 194816 euler: -201.5447 vol 0.83554995\n",
      "in 194944 euler: -201.56387 vol 0.83555555\n",
      "in 195072 euler: -201.59125 vol 0.83555984\n",
      "in 195200 euler: -201.57748 vol 0.8355403\n",
      "in 195328 euler: -201.62967 vol 0.8355722\n",
      "in 195456 euler: -201.59038 vol 0.835543\n",
      "in 195584 euler: -201.61778 vol 0.8355398\n",
      "in 195712 euler: -201.58058 vol 0.83551687\n",
      "in 195840 euler: -201.5595 vol 0.8355237\n",
      "in 195968 euler: -201.585 vol 0.8355579\n",
      "in 196096 euler: -201.58066 vol 0.83556604\n",
      "in 196224 euler: -201.58293 vol 0.8355791\n",
      "in 196352 euler: -201.60693 vol 0.83557564\n",
      "in 196480 euler: -201.6125 vol 0.8355779\n",
      "in 196608 euler: -201.61174 vol 0.8355797\n",
      "in 196736 euler: -201.5708 vol 0.8355705\n",
      "in 196864 euler: -201.58932 vol 0.83557975\n",
      "in 196992 euler: -201.55942 vol 0.83553195\n",
      "in 197120 euler: -201.58336 vol 0.8355477\n",
      "in 197248 euler: -201.60294 vol 0.8355541\n",
      "in 197376 euler: -201.65305 vol 0.83558387\n",
      "in 197504 euler: -201.64282 vol 0.8355631\n",
      "in 197632 euler: -201.6138 vol 0.83552283\n",
      "in 197760 euler: -201.62791 vol 0.835537\n",
      "in 197888 euler: -201.62679 vol 0.83553094\n",
      "in 198016 euler: -201.65039 vol 0.83554673\n",
      "in 198144 euler: -201.64912 vol 0.83556616\n",
      "in 198272 euler: -201.6485 vol 0.8355744\n",
      "in 198400 euler: -201.62828 vol 0.8355296\n",
      "in 198528 euler: -201.5932 vol 0.8354828\n",
      "in 198656 euler: -201.55101 vol 0.83544815\n",
      "in 198784 euler: -201.59904 vol 0.8354906\n",
      "in 198912 euler: -201.60626 vol 0.8355345\n",
      "in 199040 euler: -201.59882 vol 0.8355215\n",
      "in 199168 euler: -201.6295 vol 0.8355574\n",
      "in 199296 euler: -201.6229 vol 0.8355568\n",
      "in 199424 euler: -201.6179 vol 0.835573\n",
      "in 199552 euler: -201.64046 vol 0.8355905\n",
      "in 199680 euler: -201.59451 vol 0.83555484\n",
      "in 199808 euler: -201.56618 vol 0.8355251\n",
      "in 199936 euler: -201.53922 vol 0.8355082\n",
      "in 200064 euler: -201.54672 vol 0.83550406\n",
      "in 200192 euler: -201.55255 vol 0.8355064\n",
      "in 200320 euler: -201.54669 vol 0.83551896\n",
      "in 200448 euler: -201.52339 vol 0.83546776\n",
      "in 200576 euler: -201.53586 vol 0.8354615\n",
      "in 200704 euler: -201.5054 vol 0.8354403\n",
      "in 200832 euler: -201.51195 vol 0.83544344\n",
      "in 200960 euler: -201.50085 vol 0.8354013\n",
      "in 201088 euler: -201.48607 vol 0.8353849\n",
      "in 201216 euler: -201.48947 vol 0.8354021\n",
      "in 201344 euler: -201.51578 vol 0.8354258\n",
      "in 201472 euler: -201.53665 vol 0.8354381\n",
      "in 201600 euler: -201.51822 vol 0.83542556\n",
      "in 201728 euler: -201.53438 vol 0.83545154\n",
      "in 201856 euler: -201.52304 vol 0.8354417\n",
      "in 201984 euler: -201.51369 vol 0.83542067\n",
      "in 202112 euler: -201.47806 vol 0.83538234\n",
      "in 202240 euler: -201.44803 vol 0.83535427\n",
      "in 202368 euler: -201.42441 vol 0.8353203\n",
      "in 202496 euler: -201.45985 vol 0.83531064\n",
      "in 202624 euler: -201.42729 vol 0.8352615\n",
      "in 202752 euler: -201.42093 vol 0.83528954\n",
      "in 202880 euler: -201.41718 vol 0.8353121\n",
      "in 203008 euler: -201.47229 vol 0.83534783\n",
      "in 203136 euler: -201.46732 vol 0.83535093\n",
      "in 203264 euler: -201.46951 vol 0.8353673\n",
      "in 203392 euler: -201.45811 vol 0.83533716\n",
      "in 203520 euler: -201.4522 vol 0.8353218\n",
      "in 203648 euler: -201.47177 vol 0.8353666\n",
      "in 203776 euler: -201.52026 vol 0.83539486\n",
      "in 203904 euler: -201.51993 vol 0.8354232\n",
      "in 204032 euler: -201.54721 vol 0.8354142\n",
      "in 204160 euler: -201.54243 vol 0.83540535\n",
      "in 204288 euler: -201.56908 vol 0.8354087\n",
      "in 204416 euler: -201.54594 vol 0.8353867\n",
      "in 204544 euler: -201.52827 vol 0.8353777\n",
      "in 204672 euler: -201.56906 vol 0.83539593\n",
      "in 204800 euler: -201.56139 vol 0.83536935\n",
      "in 204928 euler: -201.54594 vol 0.8353556\n",
      "in 205056 euler: -201.5049 vol 0.8352811\n",
      "in 205184 euler: -201.5 vol 0.8353059\n",
      "in 205312 euler: -201.51662 vol 0.8353069\n",
      "in 205440 euler: -201.51505 vol 0.8352903\n",
      "in 205568 euler: -201.4716 vol 0.83523333\n",
      "in 205696 euler: -201.48729 vol 0.8352487\n",
      "in 205824 euler: -201.49193 vol 0.8352576\n",
      "in 205952 euler: -201.49133 vol 0.8352591\n",
      "in 206080 euler: -201.46646 vol 0.8352167\n",
      "in 206208 euler: -201.43848 vol 0.83520615\n",
      "in 206336 euler: -201.43414 vol 0.83520174\n",
      "in 206464 euler: -201.44675 vol 0.83522415\n",
      "in 206592 euler: -201.41516 vol 0.8351859\n",
      "in 206720 euler: -201.37384 vol 0.8351558\n",
      "in 206848 euler: -201.35085 vol 0.83512276\n",
      "in 206976 euler: -201.34462 vol 0.83511794\n",
      "in 207104 euler: -201.33113 vol 0.83511937\n",
      "in 207232 euler: -201.34134 vol 0.83511704\n",
      "in 207360 euler: -201.37772 vol 0.8351728\n",
      "in 207488 euler: -201.44463 vol 0.835215\n",
      "in 207616 euler: -201.49837 vol 0.8352592\n",
      "in 207744 euler: -201.47237 vol 0.8352237\n",
      "in 207872 euler: -201.49858 vol 0.83525467\n",
      "in 208000 euler: -201.467 vol 0.83520883\n",
      "in 208128 euler: -201.44395 vol 0.8351978\n",
      "in 208256 euler: -201.47699 vol 0.835254\n",
      "in 208384 euler: -201.48564 vol 0.83526456\n",
      "in 208512 euler: -201.48044 vol 0.83525586\n",
      "in 208640 euler: -201.47595 vol 0.83525586\n",
      "in 208768 euler: -201.42812 vol 0.83520156\n",
      "in 208896 euler: -201.42532 vol 0.8352033\n",
      "in 209024 euler: -201.37047 vol 0.83516276\n",
      "in 209152 euler: -201.33545 vol 0.83513767\n",
      "in 209280 euler: -201.30138 vol 0.8351159\n",
      "in 209408 euler: -201.27187 vol 0.8350936\n",
      "in 209536 euler: -201.25372 vol 0.83506685\n",
      "in 209664 euler: -201.29672 vol 0.83512026\n",
      "in 209792 euler: -201.27188 vol 0.8351122\n",
      "in 209920 euler: -201.30191 vol 0.8351114\n",
      "in 210048 euler: -201.30989 vol 0.8350998\n",
      "in 210176 euler: -201.27884 vol 0.8350735\n",
      "in 210304 euler: -201.27682 vol 0.8350363\n",
      "in 210432 euler: -201.29277 vol 0.8350361\n",
      "in 210560 euler: -201.30258 vol 0.83502007\n",
      "in 210688 euler: -201.32104 vol 0.8350277\n",
      "in 210816 euler: -201.32378 vol 0.83502793\n",
      "in 210944 euler: -201.28197 vol 0.8350104\n",
      "in 211072 euler: -201.2362 vol 0.8349695\n",
      "in 211200 euler: -201.19795 vol 0.8349652\n",
      "in 211328 euler: -201.16689 vol 0.8349279\n",
      "in 211456 euler: -201.22263 vol 0.83495384\n",
      "in 211584 euler: -201.24155 vol 0.8349736\n",
      "in 211712 euler: -201.27158 vol 0.8349954\n",
      "in 211840 euler: -201.23582 vol 0.8349441\n",
      "in 211968 euler: -201.20433 vol 0.83493334\n",
      "in 212096 euler: -201.2165 vol 0.8349499\n",
      "in 212224 euler: -201.20839 vol 0.8349599\n",
      "in 212352 euler: -201.23483 vol 0.83497417\n",
      "in 212480 euler: -201.26393 vol 0.8349833\n",
      "in 212608 euler: -201.2442 vol 0.8349906\n",
      "in 212736 euler: -201.22815 vol 0.83497673\n",
      "in 212864 euler: -201.1811 vol 0.83494985\n",
      "in 212992 euler: -201.18202 vol 0.83496857\n",
      "in 213120 euler: -201.18915 vol 0.83498716\n",
      "in 213248 euler: -201.22784 vol 0.8350181\n",
      "in 213376 euler: -201.2272 vol 0.8350259\n",
      "in 213504 euler: -201.19038 vol 0.8349885\n",
      "in 213632 euler: -201.23422 vol 0.83502156\n",
      "in 213760 euler: -201.21729 vol 0.83502936\n",
      "in 213888 euler: -201.29485 vol 0.8350901\n",
      "in 214016 euler: -201.3011 vol 0.83507985\n",
      "in 214144 euler: -201.29614 vol 0.8350556\n",
      "in 214272 euler: -201.29025 vol 0.83503765\n",
      "in 214400 euler: -201.25368 vol 0.8350156\n",
      "in 214528 euler: -201.24985 vol 0.8349956\n",
      "in 214656 euler: -201.21555 vol 0.8349757\n",
      "in 214784 euler: -201.2418 vol 0.8350095\n",
      "in 214912 euler: -201.22041 vol 0.83499044\n",
      "in 215040 euler: -201.19804 vol 0.8349742\n",
      "in 215168 euler: -201.20657 vol 0.834972\n",
      "in 215296 euler: -201.21329 vol 0.8349732\n",
      "in 215424 euler: -201.20428 vol 0.8349749\n",
      "in 215552 euler: -201.18262 vol 0.8349652\n",
      "in 215680 euler: -201.16621 vol 0.83495384\n",
      "in 215808 euler: -201.1521 vol 0.8349252\n",
      "in 215936 euler: -201.12044 vol 0.83490884\n",
      "in 216064 euler: -201.08981 vol 0.83488613\n",
      "in 216192 euler: -201.0663 vol 0.83487463\n",
      "in 216320 euler: -201.10071 vol 0.8349169\n",
      "in 216448 euler: -201.0878 vol 0.83490723\n",
      "in 216576 euler: -201.07921 vol 0.8349043\n",
      "in 216704 euler: -201.02414 vol 0.83484745\n",
      "in 216832 euler: -200.99904 vol 0.83483243\n",
      "in 216960 euler: -201.01575 vol 0.8348457\n",
      "in 217088 euler: -201.05252 vol 0.83487844\n",
      "in 217216 euler: -201.07352 vol 0.83491594\n",
      "in 217344 euler: -201.07689 vol 0.8349156\n",
      "in 217472 euler: -201.07346 vol 0.8348984\n",
      "in 217600 euler: -201.05937 vol 0.8348848\n",
      "in 217728 euler: -201.0605 vol 0.83489925\n",
      "in 217856 euler: -201.03133 vol 0.83486575\n",
      "in 217984 euler: -201.03209 vol 0.83485764\n",
      "in 218112 euler: -201.03227 vol 0.83484447\n",
      "in 218240 euler: -201.05345 vol 0.83487165\n",
      "in 218368 euler: -201.03627 vol 0.8348423\n",
      "in 218496 euler: -201.0273 vol 0.8348393\n",
      "in 218624 euler: -201.0501 vol 0.8348683\n",
      "in 218752 euler: -201.01918 vol 0.8348172\n",
      "in 218880 euler: -200.97437 vol 0.83477825\n",
      "in 219008 euler: -200.97403 vol 0.83477074\n",
      "in 219136 euler: -200.98624 vol 0.8347829\n",
      "in 219264 euler: -200.99405 vol 0.8348131\n",
      "in 219392 euler: -200.98927 vol 0.834783\n",
      "in 219520 euler: -200.9644 vol 0.8347453\n",
      "in 219648 euler: -200.97278 vol 0.8347444\n",
      "in 219776 euler: -200.95743 vol 0.8347217\n",
      "in 219904 euler: -200.92128 vol 0.83467716\n",
      "in 220032 euler: -200.90434 vol 0.8346776\n",
      "in 220160 euler: -200.86177 vol 0.8346536\n",
      "in 220288 euler: -200.8792 vol 0.834679\n",
      "in 220416 euler: -200.85439 vol 0.834652\n",
      "in 220544 euler: -200.911 vol 0.83468455\n",
      "in 220672 euler: -200.92195 vol 0.8346799\n",
      "in 220800 euler: -200.90355 vol 0.8346538\n",
      "in 220928 euler: -200.85799 vol 0.83461833\n",
      "in 221056 euler: -200.839 vol 0.83458525\n",
      "in 221184 euler: -200.88968 vol 0.83463776\n",
      "in 221312 euler: -200.883 vol 0.83462036\n",
      "in 221440 euler: -200.90314 vol 0.8346265\n",
      "in 221568 euler: -200.8915 vol 0.83462906\n",
      "in 221696 euler: -200.91383 vol 0.83463746\n",
      "in 221824 euler: -200.8987 vol 0.8346132\n",
      "in 221952 euler: -200.87328 vol 0.83455855\n",
      "in 222080 euler: -200.8758 vol 0.83456624\n",
      "in 222208 euler: -200.8385 vol 0.83453083\n",
      "in 222336 euler: -200.8115 vol 0.83451885\n",
      "in 222464 euler: -200.77197 vol 0.8344596\n",
      "in 222592 euler: -200.79837 vol 0.83450085\n",
      "in 222720 euler: -200.78815 vol 0.8344902\n",
      "in 222848 euler: -200.8008 vol 0.8345084\n",
      "in 222976 euler: -200.798 vol 0.8344983\n",
      "in 223104 euler: -200.79343 vol 0.8345\n",
      "in 223232 euler: -200.76193 vol 0.8344728\n",
      "in 223360 euler: -200.77327 vol 0.8344878\n",
      "in 223488 euler: -200.80888 vol 0.83453214\n",
      "in 223616 euler: -200.81378 vol 0.8345382\n",
      "in 223744 euler: -200.8476 vol 0.8345329\n",
      "in 223872 euler: -200.84744 vol 0.83453554\n",
      "in 224000 euler: -200.88515 vol 0.8345853\n",
      "in 224128 euler: -200.85965 vol 0.8345546\n",
      "in 224256 euler: -200.84564 vol 0.83454347\n",
      "in 224384 euler: -200.83824 vol 0.8345394\n",
      "in 224512 euler: -200.84056 vol 0.8345428\n",
      "in 224640 euler: -200.87822 vol 0.8345428\n",
      "in 224768 euler: -200.86479 vol 0.8345392\n",
      "in 224896 euler: -200.84154 vol 0.834501\n",
      "in 225024 euler: -200.86002 vol 0.8345106\n",
      "in 225152 euler: -200.82162 vol 0.8344933\n",
      "in 225280 euler: -200.82277 vol 0.8345202\n",
      "in 225408 euler: -200.81763 vol 0.8345215\n",
      "in 225536 euler: -200.7995 vol 0.83450353\n",
      "in 225664 euler: -200.77528 vol 0.8344728\n",
      "in 225792 euler: -200.78412 vol 0.8344588\n",
      "in 225920 euler: -200.79094 vol 0.83443683\n",
      "in 226048 euler: -200.76292 vol 0.83441126\n",
      "in 226176 euler: -200.78362 vol 0.83444613\n",
      "in 226304 euler: -200.79959 vol 0.83442914\n",
      "in 226432 euler: -200.75632 vol 0.8344057\n",
      "in 226560 euler: -200.75188 vol 0.8343813\n",
      "in 226688 euler: -200.77719 vol 0.8344136\n",
      "in 226816 euler: -200.75613 vol 0.83438104\n",
      "in 226944 euler: -200.71985 vol 0.83437616\n",
      "in 227072 euler: -200.7112 vol 0.83436114\n",
      "in 227200 euler: -200.68971 vol 0.83435017\n",
      "in 227328 euler: -200.68587 vol 0.83434826\n",
      "in 227456 euler: -200.75703 vol 0.83441716\n",
      "in 227584 euler: -200.78139 vol 0.83442456\n",
      "in 227712 euler: -200.7786 vol 0.83442146\n",
      "in 227840 euler: -200.81598 vol 0.8344382\n",
      "in 227968 euler: -200.81363 vol 0.83443016\n",
      "in 228096 euler: -200.83583 vol 0.83446264\n",
      "in 228224 euler: -200.88243 vol 0.8344847\n",
      "in 228352 euler: -200.84991 vol 0.83446324\n",
      "in 228480 euler: -200.82916 vol 0.83445007\n",
      "in 228608 euler: -200.8155 vol 0.83443\n",
      "in 228736 euler: -200.8229 vol 0.83444667\n",
      "in 228864 euler: -200.81789 vol 0.83445615\n",
      "in 228992 euler: -200.8177 vol 0.83447737\n",
      "in 229120 euler: -200.80087 vol 0.83447915\n",
      "in 229248 euler: -200.8058 vol 0.8344912\n",
      "in 229376 euler: -200.79573 vol 0.8344899\n",
      "in 229504 euler: -200.78387 vol 0.83446145\n",
      "in 229632 euler: -200.7918 vol 0.83447987\n",
      "in 229760 euler: -200.8003 vol 0.83448535\n",
      "in 229888 euler: -200.78258 vol 0.83447033\n",
      "in 230016 euler: -200.78993 vol 0.8344702\n",
      "in 230144 euler: -200.80934 vol 0.8344898\n",
      "in 230272 euler: -200.7639 vol 0.8344638\n",
      "in 230400 euler: -200.76491 vol 0.8344678\n",
      "in 230528 euler: -200.72585 vol 0.8344408\n",
      "in 230656 euler: -200.72247 vol 0.8344365\n",
      "in 230784 euler: -200.7543 vol 0.8344814\n",
      "in 230912 euler: -200.76442 vol 0.8344617\n",
      "in 231040 euler: -200.74751 vol 0.83445966\n",
      "in 231168 euler: -200.72014 vol 0.83440286\n",
      "in 231296 euler: -200.76482 vol 0.83444226\n",
      "in 231424 euler: -200.78767 vol 0.83446145\n",
      "in 231552 euler: -200.75868 vol 0.83444047\n",
      "in 231680 euler: -200.7914 vol 0.8344612\n",
      "in 231808 euler: -200.80385 vol 0.83446795\n",
      "in 231936 euler: -200.78323 vol 0.83445096\n",
      "in 232064 euler: -200.77136 vol 0.8344604\n",
      "in 232192 euler: -200.77051 vol 0.8344513\n",
      "in 232320 euler: -200.74892 vol 0.83444834\n",
      "in 232448 euler: -200.7542 vol 0.8344475\n",
      "in 232576 euler: -200.76651 vol 0.83444643\n",
      "in 232704 euler: -200.79472 vol 0.83446693\n",
      "in 232832 euler: -200.75455 vol 0.83443373\n",
      "in 232960 euler: -200.73749 vol 0.8344241\n",
      "in 233088 euler: -200.74547 vol 0.8344232\n",
      "in 233216 euler: -200.74863 vol 0.8344461\n",
      "in 233344 euler: -200.72258 vol 0.83443487\n",
      "in 233472 euler: -200.71326 vol 0.83445626\n",
      "in 233600 euler: -200.70454 vol 0.8344708\n",
      "in 233728 euler: -200.6814 vol 0.83444405\n",
      "in 233856 euler: -200.66995 vol 0.8344336\n",
      "in 233984 euler: -200.66597 vol 0.83444303\n",
      "in 234112 euler: -200.64261 vol 0.83441687\n",
      "in 234240 euler: -200.6872 vol 0.83442664\n",
      "in 234368 euler: -200.73123 vol 0.83444613\n",
      "in 234496 euler: -200.78362 vol 0.8344828\n",
      "in 234624 euler: -200.78639 vol 0.8344796\n",
      "in 234752 euler: -200.76266 vol 0.8344498\n",
      "in 234880 euler: -200.76404 vol 0.8344433\n",
      "in 235008 euler: -200.76181 vol 0.8344057\n",
      "in 235136 euler: -200.73991 vol 0.83438945\n",
      "in 235264 euler: -200.72354 vol 0.83436257\n",
      "in 235392 euler: -200.68393 vol 0.83431727\n",
      "in 235520 euler: -200.65866 vol 0.83428776\n",
      "in 235648 euler: -200.70248 vol 0.8343201\n",
      "in 235776 euler: -200.6748 vol 0.83430076\n",
      "in 235904 euler: -200.67644 vol 0.8343138\n",
      "in 236032 euler: -200.72437 vol 0.8343518\n",
      "in 236160 euler: -200.71771 vol 0.83436453\n",
      "in 236288 euler: -200.73022 vol 0.8343681\n",
      "in 236416 euler: -200.73448 vol 0.8343818\n",
      "in 236544 euler: -200.73251 vol 0.8343748\n",
      "in 236672 euler: -200.79669 vol 0.8344125\n",
      "in 236800 euler: -200.80333 vol 0.8344253\n",
      "in 236928 euler: -200.78998 vol 0.83440745\n",
      "in 237056 euler: -200.77902 vol 0.83439606\n",
      "in 237184 euler: -200.76535 vol 0.83438694\n",
      "in 237312 euler: -200.75696 vol 0.83437085\n",
      "in 237440 euler: -200.75874 vol 0.83439094\n",
      "in 237568 euler: -200.73926 vol 0.83436203\n",
      "in 237696 euler: -200.71735 vol 0.8343345\n",
      "in 237824 euler: -200.70358 vol 0.8343147\n",
      "in 237952 euler: -200.68318 vol 0.8343094\n",
      "in 238080 euler: -200.67203 vol 0.8342956\n",
      "in 238208 euler: -200.65697 vol 0.8342777\n",
      "in 238336 euler: -200.6392 vol 0.8342421\n",
      "in 238464 euler: -200.6213 vol 0.8342492\n",
      "in 238592 euler: -200.62514 vol 0.8342602\n",
      "in 238720 euler: -200.63197 vol 0.8342487\n",
      "in 238848 euler: -200.63483 vol 0.83426183\n",
      "in 238976 euler: -200.59819 vol 0.8342348\n",
      "in 239104 euler: -200.5722 vol 0.83422625\n",
      "in 239232 euler: -200.5887 vol 0.83422303\n",
      "in 239360 euler: -200.5655 vol 0.8342068\n",
      "in 239488 euler: -200.56772 vol 0.83417946\n",
      "in 239616 euler: -200.56097 vol 0.83419305\n",
      "in 239744 euler: -200.5212 vol 0.83416206\n",
      "in 239872 euler: -200.5458 vol 0.8341824\n",
      "in 240000 euler: -200.5811 vol 0.8342193\n",
      "in 240128 euler: -200.62189 vol 0.83425206\n",
      "in 240256 euler: -200.65347 vol 0.8342677\n",
      "in 240384 euler: -200.63219 vol 0.83425415\n",
      "in 240512 euler: -200.62347 vol 0.83424777\n",
      "in 240640 euler: -200.63307 vol 0.8342227\n",
      "in 240768 euler: -200.64891 vol 0.8342433\n",
      "in 240896 euler: -200.6809 vol 0.8342624\n",
      "in 241024 euler: -200.66168 vol 0.8342046\n",
      "in 241152 euler: -200.64735 vol 0.8342102\n",
      "in 241280 euler: -200.64464 vol 0.83421224\n",
      "in 241408 euler: -200.67262 vol 0.83422464\n",
      "in 241536 euler: -200.6978 vol 0.83423024\n",
      "in 241664 euler: -200.74026 vol 0.8342545\n",
      "in 241792 euler: -200.70435 vol 0.83423233\n",
      "in 241920 euler: -200.72173 vol 0.83425397\n",
      "in 242048 euler: -200.70047 vol 0.83424103\n",
      "in 242176 euler: -200.7075 vol 0.8342531\n",
      "in 242304 euler: -200.68097 vol 0.8342525\n",
      "in 242432 euler: -200.6958 vol 0.83427453\n",
      "in 242560 euler: -200.71896 vol 0.83430135\n",
      "in 242688 euler: -200.67772 vol 0.8342647\n",
      "in 242816 euler: -200.70541 vol 0.8342932\n",
      "in 242944 euler: -200.73128 vol 0.8342939\n",
      "in 243072 euler: -200.76082 vol 0.834306\n",
      "in 243200 euler: -200.78053 vol 0.83434474\n",
      "in 243328 euler: -200.7826 vol 0.83434874\n",
      "in 243456 euler: -200.78653 vol 0.8343723\n",
      "in 243584 euler: -200.76028 vol 0.8343681\n",
      "in 243712 euler: -200.74722 vol 0.83436394\n",
      "in 243840 euler: -200.74776 vol 0.83437634\n",
      "in 243968 euler: -200.75905 vol 0.8343819\n",
      "in 244096 euler: -200.81198 vol 0.8344147\n",
      "in 244224 euler: -200.79199 vol 0.8344159\n",
      "in 244352 euler: -200.80325 vol 0.83443564\n",
      "in 244480 euler: -200.82526 vol 0.8344713\n",
      "in 244608 euler: -200.80226 vol 0.83444685\n",
      "in 244736 euler: -200.76245 vol 0.8344207\n",
      "in 244864 euler: -200.77121 vol 0.83442277\n",
      "in 244992 euler: -200.74875 vol 0.8343983\n",
      "in 245120 euler: -200.78358 vol 0.8344132\n",
      "in 245248 euler: -200.80927 vol 0.83440834\n",
      "in 245376 euler: -200.78145 vol 0.8343966\n",
      "in 245504 euler: -200.79521 vol 0.8344048\n",
      "in 245632 euler: -200.83174 vol 0.8344433\n",
      "in 245760 euler: -200.80028 vol 0.83442503\n",
      "in 245888 euler: -200.78914 vol 0.83441013\n",
      "in 246016 euler: -200.79816 vol 0.8344144\n",
      "in 246144 euler: -200.79044 vol 0.83440626\n",
      "in 246272 euler: -200.74991 vol 0.8343941\n",
      "in 246400 euler: -200.78105 vol 0.8343982\n",
      "in 246528 euler: -200.7694 vol 0.8343966\n",
      "in 246656 euler: -200.77023 vol 0.83438915\n",
      "in 246784 euler: -200.76805 vol 0.83436364\n",
      "in 246912 euler: -200.74794 vol 0.83435\n",
      "in 247040 euler: -200.75964 vol 0.83435875\n",
      "in 247168 euler: -200.75256 vol 0.8343567\n",
      "in 247296 euler: -200.75356 vol 0.8343455\n",
      "in 247424 euler: -200.75972 vol 0.834363\n",
      "in 247552 euler: -200.78325 vol 0.8344152\n",
      "in 247680 euler: -200.82664 vol 0.83445525\n",
      "in 247808 euler: -200.80946 vol 0.8344482\n",
      "in 247936 euler: -200.8015 vol 0.8344482\n",
      "in 248064 euler: -200.78427 vol 0.8344005\n",
      "in 248192 euler: -200.76727 vol 0.8343631\n",
      "in 248320 euler: -200.75206 vol 0.8343498\n",
      "in 248448 euler: -200.76248 vol 0.83436275\n",
      "in 248576 euler: -200.77574 vol 0.8343713\n",
      "in 248704 euler: -200.77328 vol 0.83437455\n",
      "in 248832 euler: -200.78398 vol 0.8343797\n",
      "in 248960 euler: -200.77475 vol 0.83438\n",
      "in 249088 euler: -200.7573 vol 0.83437717\n",
      "in 249216 euler: -200.777 vol 0.8343894\n",
      "in 249344 euler: -200.77429 vol 0.83438265\n",
      "in 249472 euler: -200.76067 vol 0.83435786\n",
      "in 249600 euler: -200.75948 vol 0.83436126\n",
      "in 249728 euler: -200.76205 vol 0.8343496\n",
      "in 249856 euler: -200.79636 vol 0.83434504\n",
      "in 249984 euler: -200.78114 vol 0.8343294\n",
      "in 250112 euler: -200.81084 vol 0.83435315\n",
      "in 250240 euler: -200.82709 vol 0.8343796\n",
      "in 250368 euler: -200.8457 vol 0.83438873\n",
      "in 250496 euler: -200.83505 vol 0.83437544\n",
      "in 250624 euler: -200.817 vol 0.83436817\n",
      "in 250752 euler: -200.81554 vol 0.83439857\n",
      "in 250880 euler: -200.79688 vol 0.8343862\n",
      "in 251008 euler: -200.78009 vol 0.8343775\n",
      "in 251136 euler: -200.7823 vol 0.83439785\n",
      "in 251264 euler: -200.77121 vol 0.8344015\n",
      "in 251392 euler: -200.75548 vol 0.83440864\n",
      "in 251520 euler: -200.75694 vol 0.83438313\n",
      "in 251648 euler: -200.79237 vol 0.8344126\n",
      "in 251776 euler: -200.78247 vol 0.8344255\n",
      "in 251904 euler: -200.75705 vol 0.83440495\n",
      "in 252032 euler: -200.73615 vol 0.8343852\n",
      "in 252160 euler: -200.75514 vol 0.8343949\n",
      "in 252288 euler: -200.73505 vol 0.8343843\n",
      "in 252416 euler: -200.76208 vol 0.83441037\n",
      "in 252544 euler: -200.75298 vol 0.8344186\n",
      "in 252672 euler: -200.76494 vol 0.83443415\n",
      "in 252800 euler: -200.74359 vol 0.8344168\n",
      "in 252928 euler: -200.75027 vol 0.83442795\n",
      "in 253056 euler: -200.72398 vol 0.8344119\n",
      "in 253184 euler: -200.74371 vol 0.8344368\n",
      "in 253312 euler: -200.76909 vol 0.83444905\n",
      "in 253440 euler: -200.76804 vol 0.834437\n",
      "in 253568 euler: -200.78171 vol 0.83446085\n",
      "in 253696 euler: -200.77376 vol 0.83447707\n",
      "in 253824 euler: -200.79955 vol 0.8344967\n",
      "in 253952 euler: -200.79427 vol 0.83447826\n",
      "in 254080 euler: -200.80562 vol 0.8344744\n",
      "in 254208 euler: -200.83453 vol 0.83450437\n",
      "in 254336 euler: -200.83138 vol 0.83449244\n",
      "in 254464 euler: -200.84937 vol 0.83451325\n",
      "in 254592 euler: -200.83563 vol 0.8344898\n",
      "in 254720 euler: -200.7985 vol 0.83444273\n",
      "in 254848 euler: -200.78365 vol 0.83443856\n",
      "in 254976 euler: -200.78131 vol 0.83443856\n",
      "in 255104 euler: -200.77893 vol 0.8344386\n",
      "in 255232 euler: -200.76463 vol 0.8344289\n",
      "in 255360 euler: -200.79219 vol 0.83446074\n",
      "in 255488 euler: -200.75916 vol 0.83443576\n",
      "in 255616 euler: -200.75134 vol 0.83446026\n",
      "in 255744 euler: -200.79198 vol 0.834475\n",
      "in 255872 euler: -200.79298 vol 0.83447313\n",
      "in 256000 euler: -200.76816 vol 0.83447313\n",
      "in 256128 euler: -200.76015 vol 0.8344759\n",
      "in 256256 euler: -200.75362 vol 0.8344878\n",
      "in 256384 euler: -200.74797 vol 0.8344911\n",
      "in 256512 euler: -200.74121 vol 0.83449167\n",
      "in 256640 euler: -200.74203 vol 0.8344997\n",
      "in 256768 euler: -200.73442 vol 0.83450824\n",
      "in 256896 euler: -200.70982 vol 0.8344951\n",
      "in 257024 euler: -200.70256 vol 0.8344977\n",
      "in 257152 euler: -200.73021 vol 0.83452034\n",
      "in 257280 euler: -200.71541 vol 0.8345187\n",
      "in 257408 euler: -200.71237 vol 0.8345067\n",
      "in 257536 euler: -200.70633 vol 0.8344789\n",
      "in 257664 euler: -200.72571 vol 0.8344952\n",
      "in 257792 euler: -200.75229 vol 0.834515\n",
      "in 257920 euler: -200.77689 vol 0.8345393\n",
      "in 258048 euler: -200.79012 vol 0.83452815\n",
      "in 258176 euler: -200.80327 vol 0.83452487\n",
      "in 258304 euler: -200.81667 vol 0.83451223\n",
      "in 258432 euler: -200.7982 vol 0.8344979\n",
      "in 258560 euler: -200.84496 vol 0.83452934\n",
      "in 258688 euler: -200.82176 vol 0.8345162\n",
      "in 258816 euler: -200.82635 vol 0.83451235\n",
      "in 258944 euler: -200.82004 vol 0.83448946\n",
      "in 259072 euler: -200.78925 vol 0.83447087\n",
      "in 259200 euler: -200.8038 vol 0.83448476\n",
      "in 259328 euler: -200.81325 vol 0.8344883\n",
      "in 259456 euler: -200.82448 vol 0.83449143\n",
      "in 259584 euler: -200.79823 vol 0.8344606\n",
      "in 259712 euler: -200.81436 vol 0.8344647\n",
      "in 259840 euler: -200.83623 vol 0.83449435\n",
      "in 259968 euler: -200.86072 vol 0.83451736\n",
      "in 260096 euler: -200.83948 vol 0.8345032\n",
      "in 260224 euler: -200.83273 vol 0.83449584\n",
      "in 260352 euler: -200.81242 vol 0.8344888\n",
      "in 260480 euler: -200.80244 vol 0.8344673\n",
      "in 260608 euler: -200.78519 vol 0.83444595\n",
      "in 260736 euler: -200.78767 vol 0.8344491\n",
      "in 260864 euler: -200.7557 vol 0.8344077\n",
      "in 260992 euler: -200.77675 vol 0.83442664\n",
      "in 261120 euler: -200.79529 vol 0.8344268\n",
      "in 261248 euler: -200.82083 vol 0.83446115\n",
      "in 261376 euler: -200.84538 vol 0.83447146\n",
      "in 261504 euler: -200.83339 vol 0.8344756\n",
      "in 261632 euler: -200.82722 vol 0.83446383\n",
      "in 261760 euler: -200.80452 vol 0.83443934\n",
      "in 261888 euler: -200.82597 vol 0.8344517\n",
      "in 262016 euler: -200.83629 vol 0.8344605\n",
      "in 262144 euler: -200.81635 vol 0.8344306\n",
      "in 262272 euler: -200.81316 vol 0.834443\n",
      "in 262400 euler: -200.82063 vol 0.8344589\n",
      "in 262528 euler: -200.77011 vol 0.83441114\n",
      "in 262656 euler: -200.75398 vol 0.83435917\n",
      "in 262784 euler: -200.73334 vol 0.8343296\n",
      "in 262912 euler: -200.73308 vol 0.83434796\n",
      "in 263040 euler: -200.73251 vol 0.8343618\n",
      "in 263168 euler: -200.75117 vol 0.8343814\n",
      "in 263296 euler: -200.70639 vol 0.83432955\n",
      "in 263424 euler: -200.7189 vol 0.8343533\n",
      "in 263552 euler: -200.72606 vol 0.83438075\n",
      "in 263680 euler: -200.74785 vol 0.8344004\n",
      "in 263808 euler: -200.74997 vol 0.83440304\n",
      "in 263936 euler: -200.71004 vol 0.8343442\n",
      "in 264064 euler: -200.70456 vol 0.83432937\n",
      "in 264192 euler: -200.7312 vol 0.8343561\n",
      "in 264320 euler: -200.71468 vol 0.83433765\n",
      "in 264448 euler: -200.73079 vol 0.8343396\n",
      "in 264576 euler: -200.72383 vol 0.83433163\n",
      "in 264704 euler: -200.71275 vol 0.83430034\n",
      "in 264832 euler: -200.72691 vol 0.8343357\n",
      "in 264960 euler: -200.73663 vol 0.83434564\n",
      "in 265088 euler: -200.70691 vol 0.8343253\n",
      "in 265216 euler: -200.72292 vol 0.8343644\n",
      "in 265344 euler: -200.71346 vol 0.8343636\n",
      "in 265472 euler: -200.66818 vol 0.83431375\n",
      "in 265600 euler: -200.65701 vol 0.8342965\n",
      "in 265728 euler: -200.66006 vol 0.83431494\n",
      "in 265856 euler: -200.67476 vol 0.83434206\n",
      "in 265984 euler: -200.65855 vol 0.8343317\n",
      "in 266112 euler: -200.6448 vol 0.83431697\n",
      "in 266240 euler: -200.65067 vol 0.8343221\n",
      "in 266368 euler: -200.65317 vol 0.83434093\n",
      "in 266496 euler: -200.63475 vol 0.83431304\n",
      "in 266624 euler: -200.62932 vol 0.8342894\n",
      "in 266752 euler: -200.6458 vol 0.83430636\n",
      "in 266880 euler: -200.66653 vol 0.8343284\n",
      "in 267008 euler: -200.6658 vol 0.83431286\n",
      "in 267136 euler: -200.66927 vol 0.8343231\n",
      "in 267264 euler: -200.62955 vol 0.8342699\n",
      "in 267392 euler: -200.66345 vol 0.8342976\n",
      "in 267520 euler: -200.69978 vol 0.83431435\n",
      "in 267648 euler: -200.66055 vol 0.8342876\n",
      "in 267776 euler: -200.63571 vol 0.8342392\n",
      "in 267904 euler: -200.64299 vol 0.8342475\n",
      "in 268032 euler: -200.61586 vol 0.8342216\n",
      "in 268160 euler: -200.6363 vol 0.83424836\n",
      "in 268288 euler: -200.63524 vol 0.834249\n",
      "in 268416 euler: -200.67154 vol 0.8342676\n",
      "in 268544 euler: -200.67728 vol 0.8342922\n",
      "in 268672 euler: -200.69086 vol 0.8343144\n",
      "in 268800 euler: -200.68488 vol 0.83431643\n",
      "in 268928 euler: -200.69127 vol 0.8343156\n",
      "in 269056 euler: -200.65747 vol 0.83428997\n",
      "in 269184 euler: -200.6345 vol 0.83426416\n",
      "in 269312 euler: -200.59802 vol 0.834223\n",
      "in 269440 euler: -200.60225 vol 0.8342224\n",
      "in 269568 euler: -200.58354 vol 0.83420855\n",
      "in 269696 euler: -200.5674 vol 0.83419955\n",
      "in 269824 euler: -200.56845 vol 0.8341923\n",
      "in 269952 euler: -200.58717 vol 0.834208\n",
      "in 270080 euler: -200.57368 vol 0.8341974\n",
      "in 270208 euler: -200.57446 vol 0.8341861\n",
      "in 270336 euler: -200.56267 vol 0.83416677\n",
      "in 270464 euler: -200.54881 vol 0.8341479\n",
      "in 270592 euler: -200.56583 vol 0.8341672\n",
      "in 270720 euler: -200.5535 vol 0.83416766\n",
      "in 270848 euler: -200.53574 vol 0.834141\n",
      "in 270976 euler: -200.5302 vol 0.8341427\n",
      "in 271104 euler: -200.52951 vol 0.83415014\n",
      "in 271232 euler: -200.51823 vol 0.8341413\n",
      "in 271360 euler: -200.4932 vol 0.8341357\n",
      "in 271488 euler: -200.49956 vol 0.8341348\n",
      "in 271616 euler: -200.48553 vol 0.83413893\n",
      "in 271744 euler: -200.47612 vol 0.8341397\n",
      "in 271872 euler: -200.47261 vol 0.83414257\n",
      "in 272000 euler: -200.48836 vol 0.83414775\n",
      "in 272128 euler: -200.46336 vol 0.8341268\n",
      "in 272256 euler: -200.46391 vol 0.83414245\n",
      "in 272384 euler: -200.49352 vol 0.8341703\n",
      "in 272512 euler: -200.47319 vol 0.8341494\n",
      "in 272640 euler: -200.49104 vol 0.83417016\n",
      "in 272768 euler: -200.47284 vol 0.8341576\n",
      "in 272896 euler: -200.48412 vol 0.8341772\n",
      "in 273024 euler: -200.46622 vol 0.8341585\n",
      "in 273152 euler: -200.49745 vol 0.8341762\n",
      "in 273280 euler: -200.50626 vol 0.8342075\n",
      "in 273408 euler: -200.47517 vol 0.83418566\n",
      "in 273536 euler: -200.47256 vol 0.83417755\n",
      "in 273664 euler: -200.50044 vol 0.8342148\n",
      "in 273792 euler: -200.49664 vol 0.8342089\n",
      "in 273920 euler: -200.46895 vol 0.83418334\n",
      "in 274048 euler: -200.46083 vol 0.8341769\n",
      "in 274176 euler: -200.45023 vol 0.83416635\n",
      "in 274304 euler: -200.47186 vol 0.8341886\n",
      "in 274432 euler: -200.48373 vol 0.834199\n",
      "in 274560 euler: -200.48436 vol 0.834188\n",
      "in 274688 euler: -200.47124 vol 0.83417296\n",
      "in 274816 euler: -200.49979 vol 0.8341763\n",
      "in 274944 euler: -200.47516 vol 0.83413893\n",
      "in 275072 euler: -200.45375 vol 0.8341371\n",
      "in 275200 euler: -200.41913 vol 0.8341025\n",
      "in 275328 euler: -200.44092 vol 0.83411324\n",
      "in 275456 euler: -200.40773 vol 0.83408344\n",
      "in 275584 euler: -200.3889 vol 0.8340884\n",
      "in 275712 euler: -200.41365 vol 0.8341316\n",
      "in 275840 euler: -200.4297 vol 0.83414227\n",
      "in 275968 euler: -200.42204 vol 0.83416444\n",
      "in 276096 euler: -200.41342 vol 0.83415145\n",
      "in 276224 euler: -200.4362 vol 0.8341625\n",
      "in 276352 euler: -200.45001 vol 0.834159\n",
      "in 276480 euler: -200.47116 vol 0.8341917\n",
      "in 276608 euler: -200.4521 vol 0.83416086\n",
      "in 276736 euler: -200.46451 vol 0.8341806\n",
      "in 276864 euler: -200.46962 vol 0.8341809\n",
      "in 276992 euler: -200.44252 vol 0.8341764\n",
      "in 277120 euler: -200.45924 vol 0.83421093\n",
      "in 277248 euler: -200.47008 vol 0.83421004\n",
      "in 277376 euler: -200.46577 vol 0.83417875\n",
      "in 277504 euler: -200.49965 vol 0.8342038\n",
      "in 277632 euler: -200.4879 vol 0.83419216\n",
      "in 277760 euler: -200.48344 vol 0.8341922\n",
      "in 277888 euler: -200.48601 vol 0.8341849\n",
      "in 278016 euler: -200.4926 vol 0.83420354\n",
      "in 278144 euler: -200.52823 vol 0.83422184\n",
      "in 278272 euler: -200.49442 vol 0.83418924\n",
      "in 278400 euler: -200.46571 vol 0.8341819\n",
      "in 278528 euler: -200.43756 vol 0.83416325\n",
      "in 278656 euler: -200.41759 vol 0.8341296\n",
      "in 278784 euler: -200.39986 vol 0.8341315\n",
      "in 278912 euler: -200.38805 vol 0.8341233\n",
      "in 279040 euler: -200.38406 vol 0.8341136\n",
      "in 279168 euler: -200.43028 vol 0.83414346\n",
      "in 279296 euler: -200.4731 vol 0.8341837\n",
      "in 279424 euler: -200.46756 vol 0.83417654\n",
      "in 279552 euler: -200.43501 vol 0.83415294\n",
      "in 279680 euler: -200.43164 vol 0.83417195\n",
      "in 279808 euler: -200.41783 vol 0.8341758\n",
      "in 279936 euler: -200.40587 vol 0.83417004\n",
      "in 280064 euler: -200.43051 vol 0.8341867\n",
      "in 280192 euler: -200.46031 vol 0.8342031\n",
      "in 280320 euler: -200.44466 vol 0.83418494\n",
      "in 280448 euler: -200.42581 vol 0.8341675\n",
      "in 280576 euler: -200.42133 vol 0.83417284\n",
      "in 280704 euler: -200.43607 vol 0.83418345\n",
      "in 280832 euler: -200.40408 vol 0.8341406\n",
      "in 280960 euler: -200.40518 vol 0.8341371\n",
      "in 281088 euler: -200.39162 vol 0.834113\n",
      "in 281216 euler: -200.41113 vol 0.83412564\n",
      "in 281344 euler: -200.41011 vol 0.8341193\n",
      "in 281472 euler: -200.4047 vol 0.8341182\n",
      "in 281600 euler: -200.41681 vol 0.83412325\n",
      "in 281728 euler: -200.41339 vol 0.8341105\n",
      "in 281856 euler: -200.41263 vol 0.83410347\n",
      "in 281984 euler: -200.46313 vol 0.8341453\n",
      "in 282112 euler: -200.45107 vol 0.8341452\n",
      "in 282240 euler: -200.43288 vol 0.83413094\n",
      "in 282368 euler: -200.43007 vol 0.8341425\n",
      "in 282496 euler: -200.38055 vol 0.8340912\n",
      "in 282624 euler: -200.37006 vol 0.8341074\n",
      "in 282752 euler: -200.36629 vol 0.8341279\n",
      "in 282880 euler: -200.42378 vol 0.8341747\n",
      "in 283008 euler: -200.41248 vol 0.83417445\n",
      "in 283136 euler: -200.45287 vol 0.8342006\n",
      "in 283264 euler: -200.43823 vol 0.8341788\n",
      "in 283392 euler: -200.44206 vol 0.8341815\n",
      "in 283520 euler: -200.4662 vol 0.83422124\n",
      "in 283648 euler: -200.47557 vol 0.8342102\n",
      "in 283776 euler: -200.48776 vol 0.83422345\n",
      "in 283904 euler: -200.4993 vol 0.83424324\n",
      "in 284032 euler: -200.48431 vol 0.8342493\n",
      "in 284160 euler: -200.46416 vol 0.8342197\n",
      "in 284288 euler: -200.482 vol 0.83424324\n",
      "in 284416 euler: -200.47081 vol 0.83424073\n",
      "in 284544 euler: -200.49222 vol 0.8342844\n",
      "in 284672 euler: -200.51088 vol 0.83430153\n",
      "in 284800 euler: -200.52321 vol 0.8343136\n",
      "in 284928 euler: -200.52174 vol 0.83431846\n",
      "in 285056 euler: -200.53337 vol 0.83432573\n",
      "in 285184 euler: -200.55702 vol 0.83436275\n",
      "in 285312 euler: -200.56966 vol 0.83437926\n",
      "in 285440 euler: -200.58119 vol 0.83439505\n",
      "in 285568 euler: -200.59404 vol 0.8344181\n",
      "in 285696 euler: -200.60799 vol 0.83442986\n",
      "in 285824 euler: -200.6004 vol 0.8344274\n",
      "in 285952 euler: -200.60503 vol 0.8344201\n",
      "in 286080 euler: -200.60425 vol 0.8344374\n",
      "in 286208 euler: -200.57971 vol 0.8343934\n",
      "in 286336 euler: -200.574 vol 0.8343904\n",
      "in 286464 euler: -200.55614 vol 0.83437216\n",
      "in 286592 euler: -200.53246 vol 0.8343441\n",
      "in 286720 euler: -200.52954 vol 0.8343532\n",
      "in 286848 euler: -200.50826 vol 0.83431906\n",
      "in 286976 euler: -200.49715 vol 0.834309\n",
      "in 287104 euler: -200.50504 vol 0.8343101\n",
      "in 287232 euler: -200.5179 vol 0.83431\n",
      "in 287360 euler: -200.5445 vol 0.8343364\n",
      "in 287488 euler: -200.59459 vol 0.83435905\n",
      "in 287616 euler: -200.58311 vol 0.83435917\n",
      "in 287744 euler: -200.56999 vol 0.8343549\n",
      "in 287872 euler: -200.55342 vol 0.83434653\n",
      "in 288000 euler: -200.56197 vol 0.83434504\n",
      "in 288128 euler: -200.55437 vol 0.8343275\n",
      "in 288256 euler: -200.54839 vol 0.83431685\n",
      "in 288384 euler: -200.52563 vol 0.83429104\n",
      "in 288512 euler: -200.52498 vol 0.834288\n",
      "in 288640 euler: -200.55139 vol 0.83431363\n",
      "in 288768 euler: -200.55602 vol 0.8343298\n",
      "in 288896 euler: -200.55373 vol 0.83431435\n",
      "in 289024 euler: -200.55922 vol 0.834333\n",
      "in 289152 euler: -200.54803 vol 0.8343258\n",
      "in 289280 euler: -200.52481 vol 0.8342966\n",
      "in 289408 euler: -200.5657 vol 0.8343164\n",
      "in 289536 euler: -200.58612 vol 0.83433044\n",
      "in 289664 euler: -200.5863 vol 0.8343241\n",
      "in 289792 euler: -200.60425 vol 0.8343386\n",
      "in 289920 euler: -200.592 vol 0.8343367\n",
      "in 290048 euler: -200.60994 vol 0.83436763\n",
      "in 290176 euler: -200.60248 vol 0.83435386\n",
      "in 290304 euler: -200.61261 vol 0.83435637\n",
      "in 290432 euler: -200.59123 vol 0.8343505\n",
      "in 290560 euler: -200.5903 vol 0.83434784\n",
      "in 290688 euler: -200.54681 vol 0.8343093\n",
      "in 290816 euler: -200.53094 vol 0.8343088\n",
      "in 290944 euler: -200.535 vol 0.83432144\n",
      "in 291072 euler: -200.53265 vol 0.83430976\n",
      "in 291200 euler: -200.51181 vol 0.8343052\n",
      "in 291328 euler: -200.52922 vol 0.8343297\n",
      "in 291456 euler: -200.50658 vol 0.83430934\n",
      "in 291584 euler: -200.47665 vol 0.83428544\n",
      "in 291712 euler: -200.48442 vol 0.8342917\n",
      "in 291840 euler: -200.4838 vol 0.8342937\n",
      "in 291968 euler: -200.50038 vol 0.834311\n",
      "in 292096 euler: -200.53006 vol 0.8343309\n",
      "in 292224 euler: -200.55357 vol 0.8343369\n",
      "in 292352 euler: -200.52736 vol 0.8343262\n",
      "in 292480 euler: -200.55057 vol 0.8343364\n",
      "in 292608 euler: -200.56123 vol 0.83435947\n",
      "in 292736 euler: -200.56279 vol 0.83436567\n",
      "in 292864 euler: -200.5667 vol 0.8343717\n",
      "in 292992 euler: -200.6 vol 0.83438617\n",
      "in 293120 euler: -200.63812 vol 0.8344112\n",
      "in 293248 euler: -200.66087 vol 0.83441556\n",
      "in 293376 euler: -200.64748 vol 0.8344129\n",
      "in 293504 euler: -200.6508 vol 0.83442634\n",
      "in 293632 euler: -200.62804 vol 0.83440614\n",
      "in 293760 euler: -200.66248 vol 0.83441776\n",
      "in 293888 euler: -200.69833 vol 0.834447\n",
      "in 294016 euler: -200.69472 vol 0.834431\n",
      "in 294144 euler: -200.67668 vol 0.83440596\n",
      "in 294272 euler: -200.67424 vol 0.8343959\n",
      "in 294400 euler: -200.69533 vol 0.8344128\n",
      "in 294528 euler: -200.70442 vol 0.8344177\n",
      "in 294656 euler: -200.72217 vol 0.83444583\n",
      "in 294784 euler: -200.73558 vol 0.83446175\n",
      "in 294912 euler: -200.73148 vol 0.8344485\n",
      "in 295040 euler: -200.7115 vol 0.83443487\n",
      "in 295168 euler: -200.70741 vol 0.83445096\n",
      "in 295296 euler: -200.70659 vol 0.8344449\n",
      "in 295424 euler: -200.70264 vol 0.83445466\n",
      "in 295552 euler: -200.67548 vol 0.8344385\n",
      "in 295680 euler: -200.68697 vol 0.8344375\n",
      "in 295808 euler: -200.70038 vol 0.8344381\n",
      "in 295936 euler: -200.68983 vol 0.83444816\n",
      "in 296064 euler: -200.70134 vol 0.8344623\n",
      "in 296192 euler: -200.6749 vol 0.8344481\n",
      "in 296320 euler: -200.67415 vol 0.83445853\n",
      "in 296448 euler: -200.6532 vol 0.8344402\n",
      "in 296576 euler: -200.61305 vol 0.83437866\n",
      "in 296704 euler: -200.64247 vol 0.8343982\n",
      "in 296832 euler: -200.63354 vol 0.83441114\n",
      "in 296960 euler: -200.6519 vol 0.8344173\n",
      "in 297088 euler: -200.64514 vol 0.8344131\n",
      "in 297216 euler: -200.63737 vol 0.83441126\n",
      "in 297344 euler: -200.63121 vol 0.8344276\n",
      "in 297472 euler: -200.66829 vol 0.8344518\n",
      "in 297600 euler: -200.68501 vol 0.8344568\n",
      "in 297728 euler: -200.65211 vol 0.8344353\n",
      "in 297856 euler: -200.65454 vol 0.83443594\n",
      "in 297984 euler: -200.67027 vol 0.83444756\n",
      "in 298112 euler: -200.66518 vol 0.834453\n",
      "in 298240 euler: -200.66446 vol 0.8344638\n",
      "in 298368 euler: -200.66278 vol 0.83446467\n",
      "in 298496 euler: -200.65036 vol 0.8344486\n",
      "in 298624 euler: -200.6326 vol 0.8344368\n",
      "in 298752 euler: -200.61638 vol 0.8344148\n",
      "in 298880 euler: -200.62042 vol 0.8344197\n",
      "in 299008 euler: -200.64307 vol 0.8344505\n",
      "in 299136 euler: -200.59192 vol 0.8343794\n",
      "in 299264 euler: -200.57533 vol 0.8343755\n",
      "in 299392 euler: -200.57959 vol 0.83437467\n",
      "in 299520 euler: -200.60919 vol 0.8344065\n",
      "in 299648 euler: -200.61143 vol 0.8344191\n",
      "in 299776 euler: -200.62213 vol 0.83443314\n",
      "in 299904 euler: -200.64345 vol 0.8344511\n",
      "in 300032 euler: -200.62885 vol 0.83441937\n",
      "in 300160 euler: -200.62695 vol 0.83442295\n",
      "in 300288 euler: -200.6535 vol 0.834438\n",
      "in 300416 euler: -200.645 vol 0.83442825\n",
      "in 300544 euler: -200.6546 vol 0.83444107\n",
      "in 300672 euler: -200.63751 vol 0.8344293\n",
      "in 300800 euler: -200.65013 vol 0.8344358\n",
      "in 300928 euler: -200.6674 vol 0.8344461\n",
      "in 301056 euler: -200.65677 vol 0.8344335\n",
      "in 301184 euler: -200.64903 vol 0.8344303\n",
      "in 301312 euler: -200.65436 vol 0.8344298\n",
      "in 301440 euler: -200.6125 vol 0.83439064\n",
      "in 301568 euler: -200.60147 vol 0.8343747\n",
      "in 301696 euler: -200.6176 vol 0.8343893\n",
      "in 301824 euler: -200.62256 vol 0.8344081\n",
      "in 301952 euler: -200.65446 vol 0.83443654\n",
      "in 302080 euler: -200.67076 vol 0.83445686\n",
      "in 302208 euler: -200.65388 vol 0.83443755\n",
      "in 302336 euler: -200.63603 vol 0.8344153\n",
      "in 302464 euler: -200.62698 vol 0.83440316\n",
      "in 302592 euler: -200.62868 vol 0.8343996\n",
      "in 302720 euler: -200.60637 vol 0.8343814\n",
      "in 302848 euler: -200.605 vol 0.83437186\n",
      "in 302976 euler: -200.59134 vol 0.8343705\n",
      "in 303104 euler: -200.58337 vol 0.8343756\n",
      "in 303232 euler: -200.5971 vol 0.8343745\n",
      "in 303360 euler: -200.59204 vol 0.83436674\n",
      "in 303488 euler: -200.5762 vol 0.83435774\n"
     ]
    }
   ],
   "source": [
    "generate_points_and_save_using_defaultsAlpha(free_coefficient,phimodel1,-200.,force_generate=False,seed_set=0,batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069331e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/f/fraser-talientec\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a058210a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataAlphaP/QuinticAlpha_pg_with_1e-14/dataset.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m train_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_alpha:\n\u001b[0;32m---> 10\u001b[0m     AlphaModel1,training_historyAlpha\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_and_save_nn_Alpha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfree_coefficient\u001b[49m\u001b[43m,\u001b[49m\u001b[43mphimodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43meuler_char\u001b[49m\u001b[43m,\u001b[49m\u001b[43malphaprime\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdepthAlpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwidthAlpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnEpochsAlpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbSizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstddev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlRate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43muse_zero_network\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mload_network\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     AlphaModel1,training_historyAlpha\u001b[38;5;241m=\u001b[39mload_nn_Alpha(free_coefficient,phimodel1,euler_char,alphaprime,depthPhi,widthPhi,nEpochsPhi,[\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m50000\u001b[39m],set_weights_to_zero\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[12], line 62\u001b[0m, in \u001b[0;36mtrain_and_save_nn_Alpha\u001b[0;34m(free_coefficient, phimodel, euler_char, alphaprime, nlayer, nHidden, nEpochs, bSizes, stddev, lRate, alpha, load_network, use_zero_network)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#data = np.load(os.path.join(dirname, 'dataset.npz'))\u001b[39;00m\n\u001b[1;32m     59\u001b[0m BASIS \u001b[38;5;241m=\u001b[39m prepare_tf_basis(np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirnameForMetric, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasis.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m), allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m---> 62\u001b[0m dataalpha \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirnameAlpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset.npz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m dataalpha_train\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mdict\u001b[39m(dataalpha)\u001b[38;5;241m.\u001b[39mitems())[:\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mdict\u001b[39m(dataalpha))\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m]))\n\u001b[1;32m     64\u001b[0m dataalpha_val_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mdict\u001b[39m(dataalpha)\u001b[38;5;241m.\u001b[39mitems())[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mdict\u001b[39m(dataalpha))\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n",
      "File \u001b[0;32m~/cymetricTESTING217/lib/python3.8/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataAlphaP/QuinticAlpha_pg_with_1e-14/dataset.npz'"
     ]
    }
   ],
   "source": [
    "alphaprime=1\n",
    "euler_char=-200\n",
    "depthAlpha=4\n",
    "widthAlpha=64\n",
    "nEpochsAlpha=30\n",
    "\n",
    "\n",
    "train_alpha=True\n",
    "if train_alpha:\n",
    "    AlphaModel1,training_historyAlpha=train_and_save_nn_Alpha(free_coefficient,phimodel1,euler_char,alphaprime,depthAlpha,widthAlpha,nEpochsAlpha,bSizes=[64,50000],stddev=0.05,lRate=0.1,use_zero_network=False,alpha=[1.,1.],load_network=False)\n",
    "else:\n",
    "    AlphaModel1,training_historyAlpha=load_nn_Alpha(free_coefficient,phimodel1,euler_char,alphaprime,depthPhi,widthPhi,nEpochsPhi,[64,50000],set_weights_to_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f584aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.load('dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) + '/dataset.npz')\n",
    "# AlphaModel1.model(data['X_train'][0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2954e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reimport custom_networks\n",
    "importlib.reload(sys.modules['custom_networks'])\n",
    "from custom_networks import *\n",
    "#reimport laplacian_funcs\n",
    "importlib.reload(sys.modules['laplacian_funcs'])\n",
    "from laplacian_funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bc49d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(self,kphi,linebundleindices,ambient_env_var,n_projective,kmoduli)\n",
    "n_0=0\n",
    "n_p= 500000*9//10\n",
    "ptdata=tf.cast(data['X_train'],tf.float32)[n_0:n_p]\n",
    "#cptdata=point_vec_to_complex(ptdata)\n",
    "#cptdata2=cptdata/cptdata[:,0:1]\n",
    "flat_measure_weights=tf.cast(((data['y_train'][:,0]/(data['y_train'][:,1]))*(1/(6)))[n_0:n_p],tf.float32)\n",
    "pullbacks=AlphaModel1.pullbacks(ptdata)#[n_0:n_p]\n",
    "kphi = np.array([3])\n",
    "#get_sections_cpts_func=get_degree_kphiandMmonomials_general(kphi,[0],ambientTQ,len(ambientTQ),kmoduliTQ)\n",
    "# get_sections_cpts_func(cptdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b5b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tf.function this, but for fixed arguments 0,2 and tensor arguments 1,3,4\n",
    "\n",
    "def compute_matrices_for_pts(rpoints,metric,flat_measure_weights,pullbacks,get_sections_cpts_func):\n",
    "    get_sections_rpts_func= lambda x: get_sections_cpts_func(point_vec_to_complex(x))\n",
    "    metrics = metric(rpoints)\n",
    "    inversemetrics=tf.linalg.inv(metrics)\n",
    "    dets = tf.linalg.det(metrics)\n",
    "    #k_phi=get_sections_rpts_func(cpoints)\n",
    "    djsigma=extder_j_for_sigma(rpoints,get_sections_rpts_func)\n",
    "    djbarsigma=extder_jbar_for_sigma(rpoints,get_sections_rpts_func)\n",
    "    dasigma = tf.einsum('xai,xCi->xCa',pullbacks,djsigma)\n",
    "    dbbarsigma = tf.einsum('xbj,xCj->xCb',tf.math.conj(pullbacks),djbarsigma)\n",
    "    #b is barred, a is unbarred\n",
    "    dbarfbar_df = tf.einsum('xba,xCb,xDa->xCD',inversemetrics,tf.math.conj(dasigma),dasigma)\n",
    "    dfbar_dbarf = tf.einsum('xba,xCa,xDb->xCD',inversemetrics,tf.math.conj(dbbarsigma),dbbarsigma)\n",
    "    #factor of 2 form the laplacian!\n",
    "    integrand_CD = tf.einsum('xCD,x->xCD',2*(dbarfbar_df + dfbar_dbarf),dets*tf.cast(flat_measure_weights,tf.complex64))\n",
    "    integrand_mean=tf.reduce_mean(integrand_CD,axis=0)\n",
    "    print(\"vol: \" + str(tf.reduce_mean(dets*tf.cast(flat_measure_weights,tf.complex64))))\n",
    "    return integrand_mean\n",
    "    \n",
    "get_sections_cpts_func=get_degree_kphiandMmonomials_general(kphi,kphi*0,ambientTQ,len(ambientTQ),kmoduliTQ)\n",
    "#mats=compute_matrices_for_pts(kphi,ptdata,AlphaModel1,flat_measure_weights,pullbacks,get_sections_cpts_func)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf56f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normalising_matrices_for_pts(rpoints,metric,flat_measure_weights,get_sections_cpts_func):\n",
    "    get_sections_rpts_func= lambda x: get_sections_cpts_func(point_vec_to_complex(x))\n",
    "    metrics = metric(rpoints)\n",
    "    dets = tf.linalg.det(metrics)\n",
    "    sectionsB    = get_sections_rpts_func(rpoints)\n",
    "    sectionsConjA   = tf.math.conj(sectionsB)\n",
    "\n",
    "    #k_phi=get_sections_rpts_func(cpoints)\n",
    "    integrand_CD = tf.einsum('xA,xB,x->AB',sectionsConjA,sectionsB,dets*tf.cast(flat_measure_weights,tf.complex64))/tf.cast(tf.shape(flat_measure_weights)[-1],tf.complex64)\n",
    "    print(\"vol: \" + str(tf.reduce_mean(dets*tf.cast(flat_measure_weights,tf.complex64))))\n",
    "    return integrand_CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47cc20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "metric_using=phimodel1\n",
    "\n",
    "\n",
    "#create compiled versions\n",
    "@tf.function\n",
    "def compute_matrices_for_pts_fixed(rpoints, flat_measure_weights, pullbacks):\n",
    "    #kphi = tf.constant([1])  # Fixed argument\n",
    "    return compute_matrices_for_pts(rpoints, metric_using, flat_measure_weights, pullbacks,get_sections_cpts_func)\n",
    "\n",
    "# Get the concrete function\n",
    "concrete_func = compute_matrices_for_pts_fixed.get_concrete_function(\n",
    "    tf.TensorSpec(shape=(batch_size,2*metric_using.ncoords), dtype=tf.float32),  # rpoints\n",
    "    tf.TensorSpec(shape=(batch_size), dtype=tf.float32),  # flat_measure_weights\n",
    "    tf.TensorSpec(shape=(batch_size, metric_using.nfold,metric_using.ncoords), dtype=tf.complex64)  # pullbacks\n",
    ")\n",
    "\n",
    "@tf.function\n",
    "def compute_normalising_matrices_for_pts_fixed(rpoints, flat_measure_weights):\n",
    "    #kphi = tf.constant([1])  # Fixed argument\n",
    "    return compute_normalising_matrices_for_pts(rpoints, metric_using, flat_measure_weights,get_sections_cpts_func)\n",
    "\n",
    "# Get the concrete function for the normalising matrix\n",
    "concrete_func_normalising = compute_normalising_matrices_for_pts_fixed.get_concrete_function(\n",
    "    tf.TensorSpec(shape=(batch_size,2*metric_using.ncoords), dtype=tf.float32),  # rpoints\n",
    "    tf.TensorSpec(shape=(batch_size), dtype=tf.float32),  # flat_measure_weights\n",
    ")\n",
    "\n",
    "#compute_matrices_for_pts_fixed(ptdata, flat_measure_weights, pullbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf64f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test runtime!\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((ptdata, flat_measure_weights, pullbacks))\n",
    "batched_dataset = dataset.batch(batch_size)\n",
    "import time\n",
    "start = time.time()\n",
    "for batch in batched_dataset.take(4):\n",
    "    batch_rpoints, batch_flat_measure_weights, batch_pullbacks = batch\n",
    "    batch_result = compute_matrices_for_pts_fixed(batch_rpoints, batch_flat_measure_weights, batch_pullbacks)\n",
    "    #oresults.append(batch_result)\n",
    "    print('done, timing: ' + str(time.time()-start))\n",
    "    start=time.time()\n",
    "\n",
    "print(\"\\n\\n concrete\" )\n",
    "import time\n",
    "start = time.time()\n",
    "for batch in batched_dataset.take(4):\n",
    "    batch_rpoints, batch_flat_measure_weights, batch_pullbacks = batch\n",
    "    batch_result = concrete_func(batch_rpoints, batch_flat_measure_weights, batch_pullbacks)\n",
    "    #oresults.append(batch_result)\n",
    "    print('done, timing: ' + str(time.time()-start))\n",
    "    start=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbb7f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the concrete_func is defined as in the previous example\n",
    "\n",
    "# Create a dataset from your large number of points\n",
    "dataset = tf.data.Dataset.from_tensor_slices((ptdata, flat_measure_weights, pullbacks))\n",
    "# Adjust based on your memory constraints and performance needs\n",
    "\n",
    "\n",
    "# Batch the dataset\n",
    "batched_dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Process batches\n",
    "results = []\n",
    "for batch in batched_dataset:\n",
    "    batch_rpoints, batch_flat_measure_weights, batch_pullbacks = batch\n",
    "    batch_result = concrete_func(batch_rpoints, batch_flat_measure_weights, batch_pullbacks)\n",
    "    results.append(batch_result)\n",
    "\n",
    "\n",
    "# Combine results if needed\n",
    "matrix_laplacian_requires_mean = np.array(results)\n",
    "\n",
    "print(f\"Processed {num_points} points. Shape of final result: {final_result.shape}\")\n",
    "\n",
    "results_norm = []\n",
    "for batch in batched_dataset:\n",
    "    batch_rpoints, batch_flat_measure_weights,_ = batch\n",
    "    batch_result_norm = concrete_func_normalising(batch_rpoints, batch_flat_measure_weights)\n",
    "    results_norm.append(batch_result_norm)\n",
    "\n",
    "matrix_normalising_requires_mean = np.array(results_norm)\n",
    "\n",
    "print(f\"Processed {num_points} points. Shape of final result: {final_result_norm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "#take eigs of final_result\n",
    "\n",
    "\n",
    "#log distribute over 1 to 100000\n",
    "number_approx=6\n",
    "\n",
    "numbers_to_take= np.unique((batch_size*np.round(np.logspace(np.log10(10000),np.log10(len(ptdata)),number_approx)/batch_size)).astype('int'))[0:]\n",
    "indices_to_take=tf.cast(numbers_to_take/batch_size,tf.int32).numpy()\n",
    "indices_to_take[0]\n",
    "print(numbers_to_take)\n",
    "\n",
    "eigvals_all=[]\n",
    "for i in indices_to_take:\n",
    "    matrix=tf.reduce_mean(np.array(results)[0:i],axis=0)\n",
    "    matrix_norm=tf.reduce_mean(np.array(results_norm)[0:i],axis=0)\n",
    "    eigvals=tf.math.real(scipy.linalg.eigvals(matrix,matrix_norm))\n",
    "    eigvals_all.append(np.sort(eigvals))\n",
    "    #round and print to 2dp\n",
    "    #eigvals_rounded=np.round(eigvals,2)\n",
    "    #print(tf.math.real(eigvals_rounded))\n",
    "eigvals_all=np.array(eigvals_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7793a9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#sort_eigvals_rounded=np.sort(eigvals_rounded)[0:200]\n",
    "#plt.scatter(range(len(sort_eigvals_rounded)),sort_eigvals_rounded)\n",
    "numbers_for_plotting=tf.repeat(tf.expand_dims(numbers_to_take,axis=-1),tf.shape(eigvals_all)[-1],axis=-1)\n",
    "#join each eigval as hte number of points increase with a dotted line\n",
    "#change the plot to a line plot\n",
    "#change all the line colours to red and thin, with a small red x at each point\n",
    "#fade the lines from blue to red corresponding to their index in the first axis of the array\n",
    "\n",
    "#plt.plot(numbers_for_plotting,eigvals_all,marker='x',linestyle='dotted',color='red',linewidth=0.5,markersize=1)\n",
    "\n",
    "\n",
    "#change the x axis to log scale, labelled at the endpoints with one point in the middle\n",
    "plt.xscale('log')\n",
    "middle_pt=((numbers_to_take[-1]-numbers_to_take[0])//(5000*2))*5000 + numbers_to_take[0]\n",
    "middle_pt2=((numbers_to_take[-1]-numbers_to_take[0])//(5000*5))*5000 + numbers_to_take[0]\n",
    "ticks = tf.cast(np.concatenate((numbers_to_take[[0,-1]],np.array([middle_pt,middle_pt2])),axis=0),tf.int32).numpy()\n",
    "plt.xticks(ticks,ticks)\n",
    "#label the x axis as number of points n_p\n",
    "#label the y axis as eigenvalue\n",
    "plt.xlabel('number of points n_p')\n",
    "plt.ylabel('eigenvalue')\n",
    "#title the plot as eigenvalues of the laplacian as number of points increases\n",
    "plt.title('eigenvalues of the laplacian vs integration dataset size')\n",
    "plt.ylim(0,300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d691905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Create a custom colormap from blue to red\n",
    "colors = [(0, 0, 1), (1, 0, 0)]  # R -> G -> B\n",
    "n_bins = eigvals_all.shape[1]  # Number of color bins (1225 eigenvalues)\n",
    "cmap = LinearSegmentedColormap.from_list('red_to_blue', colors, N=n_bins)\n",
    "\n",
    "# Plot setup\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot each eigenvalue evolution with a different color\n",
    "for i in range(n_bins):\n",
    "    color = cmap(i / (n_bins - 1))\n",
    "    ax.plot(numbers_for_plotting[:,i], eigvals_all[:,i],\n",
    "            marker='x', linestyle='dotted', color=color, linewidth=0.5, markersize=1)\n",
    "\n",
    "# Set x-axis to log scale\n",
    "ax.set_xscale('log')\n",
    "\n",
    "# Set x-axis ticks\n",
    "middle_pt = ((numbers_to_take[-1]-numbers_to_take[0])//(5000*2))*5000 + numbers_to_take[0]\n",
    "middle_pt2 = ((numbers_to_take[-1]-numbers_to_take[0])//(5000*5))*5000 + numbers_to_take[0]\n",
    "ticks = np.concatenate((numbers_to_take[[0,-1]], np.array([middle_pt, middle_pt2])))\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_xticklabels(ticks)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('number of points n_p')\n",
    "ax.set_ylabel('eigenvalue')\n",
    "ax.set_title('eigenvalues of the laplacian vs integration dataset size')\n",
    "\n",
    "# Set y-axis limit\n",
    "# `ax.set_ylim(0, 300)`\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c172119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_adam_optimizer_with_decay(initial_learning_rate, nEpochs, final_lr_factor=10):\n",
    "    \"\"\"\n",
    "    Creates an Adam optimizer with exponential learning rate decay.\n",
    "\n",
    "    Parameters:\n",
    "    - initial_learning_rate: The starting learning rate\n",
    "    - nEpochs: The number of epochs over which to decay the learning rate\n",
    "    - final_lr_factor: The factor by which to reduce the learning rate (default: 10)\n",
    "\n",
    "    Returns:\n",
    "    - An Adam optimizer with the specified learning rate decay\n",
    "    \"\"\"\n",
    "    decay_rate = (1/final_lr_factor) ** (1/nEpochs)\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=1,\n",
    "        decay_rate=decay_rate,\n",
    "        staircase=False\n",
    "    )\n",
    "\n",
    "    return tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "def generate_points_and_save_using_defaults(free_coefficient,number_points,force_generate=False,seed_set=0):\n",
    "   coefficients=coefficientsTQ\n",
    "   # coefficients=np.array([1, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, \\\n",
    "   # 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, free_coefficient, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 1, 0, 2, 0, 0, 0, 2, 0, 1])\n",
    "   monomials=monomialsTQ\n",
    "   kmoduli=kmoduliTQ\n",
    "   ambient=ambientTQ\n",
    "   pg = PointGenerator(monomials, coefficients, kmoduli, ambient)\n",
    "   pg._set_seed(seed_set)\n",
    "   #dirname = 'data/tetraquadric_pg_wit#h_'+str(free_coefficient)\n",
    "   dirname = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   print(\"dirname: \" + dirname)\n",
    "   #test if the directory exists, if not, create it\n",
    "   if force_generate or (not os.path.exists(dirname)):\n",
    "      print(\"Generating: forced? \" + str(force_generate))\n",
    "      kappa = pg.prepare_dataset(number_points, dirname)\n",
    "      pg.prepare_basis(dirname, kappa=kappa)\n",
    "   elif os.path.exists(dirname):\n",
    "      try:\n",
    "         print(\"loading prexisting dataset\")\n",
    "         data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "         if (len(data['X_train'])+len(data['X_val']))!=number_points:\n",
    "            print(\"wrong length - generating anyway\")\n",
    "            kappa = pg.prepare_dataset(number_points, dirname)\n",
    "            pg.prepare_basis(dirname, kappa=kappa)\n",
    "      except:\n",
    "         print(\"error loading - generating anyway\")\n",
    "         kappa = pg.prepare_dataset(number_points, dirname)\n",
    "         pg.prepare_basis(dirname, kappa=kappa)\n",
    "\n",
    "\n",
    "def getcallbacksandmetrics(data):\n",
    "   #rcb = RicciCallback((data['X_val'], data['y_val']), data['val_pullbacks'])\n",
    "   scb = SigmaCallback((data['X_val'], data['y_val']))\n",
    "   volkcb = VolkCallback((data['X_val'], data['y_val']))\n",
    "   kcb = KaehlerCallback((data['X_val'], data['y_val']))\n",
    "   tcb = TransitionCallback((data['X_val'], data['y_val']))\n",
    "   #cb_list = [rcb, scb, kcb, tcb, volkcb]\n",
    "   #cb_list = [ scb, kcb, tcb, volkcb]\n",
    "   #cmetrics = [TotalLoss(), SigmaLoss(), KaehlerLoss(), TransitionLoss(), VolkLoss()]#, RicciLoss()]\n",
    "   cb_list = [ scb ]\n",
    "   cmetrics = [TotalLoss(), SigmaLoss()]#, RicciLoss()]\n",
    "   return cb_list, cmetrics\n",
    "\n",
    "def train_and_save_nn(free_coefficient,nlayer=3,nHidden=128,nEpochs=50,stddev=0.1,bSizes=[192,50000],lRate=0.001,use_zero_network=False):\n",
    "   #dirname = 'data/tetraquadric_pg_with_'+str(free_coefficient)\n",
    "   dirname = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   name = 'phimodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(bSizes[1]) + 's' + str(nlayer) + 'x' +str(nHidden)\n",
    "   print('dirname: ' + dirname)\n",
    "   print('name: ' + name)\n",
    "\n",
    "   data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirname, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetrics(data)\n",
    "\n",
    "   #nlayer = 3\n",
    "   #nHidden = 128\n",
    "   act = 'gelu'\n",
    "   #nEpochs = 100\n",
    "   #bSizes = [192, 150000]\n",
    "   alpha = [1., 1., 30., 1., 2.] # 1 AND 3??\n",
    "   #nfold = 3\n",
    "   #n_in = 2*5\n",
    "   #n_out = 1\n",
    "   #lRate = 0.001\n",
    "   ambient=tf.cast(tf.math.abs(BASIS['AMBIENT']),tf.int32)\n",
    "\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   nn_phi = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_phi_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "   phimodel = PhiFSModel(nn_phi, BASIS, alpha=alpha)\n",
    "   phimodelzero = PhiFSModel(nn_phi_zero, BASIS, alpha=alpha)\n",
    "   #print('nn_phi ' )\n",
    "   #print('nn_phi ' + str(phimodel(data['X_train'][0:10])))\n",
    "\n",
    "   #Note, currently running legacy due to ongoing tf issue with M1/M2.\n",
    "   #Use the commented line instead if not on an M1/M2 machine\n",
    "   #opt = tfk.optimizers.Adam(learning_rate=lRate)\n",
    "   #opt = tfk.optimizers.legacy.Adam(learning_rate=lRate)\n",
    "   opt='adam'\n",
    "   #opt = create_adam_optimizer_with_decay(\n",
    "   # initial_learning_rate=lRate,\n",
    "   # nEpochs=nEpochs*(len(data['X_train'])//bSizes[0]),\n",
    "   # final_lr_factor=10  # This will decay to lRate/10\n",
    "   #)\n",
    "   # compile so we can test on validation set before training\n",
    "   phimodel.compile(custom_metrics=cmetrics)\n",
    "   phimodelzero.compile(custom_metrics=cmetrics)\n",
    "\n",
    "   ## compare validation loss before training for zero network and nonzero network\n",
    "   datacasted=[tf.cast(data['X_val'],tf.float32),tf.cast(data['y_val'],tf.float32)]\n",
    "   #need to re-enable learning, in case there's been a problem:\n",
    "   phimodel.learn_transition = False\n",
    "   phimodelzero.learn_transition = False\n",
    "   phimodel.learn_volk = True\n",
    "   phimodelzero.learn_volk = True\n",
    "   #phimodel.learn_ricci_val= True\n",
    "   #phimodelzero.learn_ricci_val= True\n",
    "   print(\"testing\")\n",
    "   valzero=phimodelzero.test_step(datacasted)\n",
    "   valraw=phimodel.test_step(datacasted)\n",
    "   print('tested')\n",
    "   # phimodel.learn_ricci_val=False\n",
    "   # phimodelzero.learn_ricci_val=False\n",
    "   valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   valraw = {key: value.numpy() for key, value in valraw.items()}\n",
    "\n",
    "   #### maybe remove\n",
    "   phimodel.learn_volk = False\n",
    "   phimodel.learn_transition=False\n",
    "   phimodel, training_history = train_model(phimodel, data, optimizer=opt, epochs=nEpochs, batch_sizes=bSizes,\n",
    "                                       verbose=1, custom_metrics=cmetrics, callbacks=cb_list)\n",
    "   print(\"finished training\\n\")\n",
    "   phimodel.model.save_weights(os.path.join(dirname, name) + '.weights.h5')\n",
    "   np.savez_compressed(os.path.join(dirname, 'trainingHistory-' + name),training_history)\n",
    "   #now print the initial losses and final losses for each metric\n",
    "   # first_metrics = {key: value[0] for key, value in training_history.items()}\n",
    "   # lastometrics = {key: value[-1] for key, value in training_history.items()}\n",
    "   phimodel.learn_transition = True\n",
    "   phimodel.learn_volk = True\n",
    "   #phimodel.learn_ricci_val= True\n",
    "   valfinal=phimodel.test_step(datacasted)\n",
    "   valfinal = {key: value.numpy() for key, value in valfinal.items()}\n",
    "   #phimodel.learn_ricci_val=False\n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for raw network: \")\n",
    "   print(valraw)\n",
    "   print(\"validation loss for final network: \")\n",
    "   print(valfinal)\n",
    "   print(\"ratio of final to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valfinal.items()}))\n",
    "   print(\"ratio of final to raw: \" + str({key + \" ratio\": value/(valraw[key]+1e-8) for key, value in valfinal.items()}))\n",
    "\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(phimodel,tf.cast(data[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   #IMPLEMENT THE FOLLOWING\n",
    "   #meanfailuretosolveequation,_,_=measure_laplacian_failure(phimodel,data)\n",
    "   print(\"\\n\\n\")\n",
    "   return phimodel,training_history\n",
    "\n",
    "def load_nn_phimodel(free_coefficient,nlayer=3,nHidden=128,nEpochs=50,bSizes=[192,50000],stddev=0.1,lRate=0.001,set_weights_to_zero=False):\n",
    "   dirname = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   name = 'phimodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(bSizes[1]) + 's' + str(nlayer) + 'x' +str(nHidden)\n",
    "   print(dirname)\n",
    "   print(name)\n",
    "\n",
    "   data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirname, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetrics(data)\n",
    "\n",
    "   #nlayer = 3\n",
    "   #nHidden = 128\n",
    "   act = 'gelu'\n",
    "   #nEpochs = 100\n",
    "   #bSizes = [192, 150000]\n",
    "   alpha = [1., 1., 30., 1., 2.] # 1 AND 3??\n",
    "   nfold = 3\n",
    "   n_in = 2*8\n",
    "   n_out = 1\n",
    "   #lRate = 0.001\n",
    "\n",
    "\n",
    "   ambient=tf.cast(BASIS['AMBIENT'],tf.int32)\n",
    "\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   nn_phi = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_phi_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "   print(\"nns made\")\n",
    "\n",
    "   datacasted=[tf.cast(data['X_val'],tf.float32),tf.cast(data['y_val'],tf.float32)]\n",
    "\n",
    "   #    nn_phi = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "   #    nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "   phimodel = PhiFSModel(nn_phi, BASIS, alpha=alpha)\n",
    "   phimodelzero = PhiFSModel(nn_phi_zero, BASIS, alpha=alpha)\n",
    "   #initialise weights\n",
    "   phimodel(datacasted[0][0:1])\n",
    "   phimodelzero(datacasted[0][0:1])\n",
    "\n",
    "\n",
    "   if set_weights_to_zero:\n",
    "      training_history=0\n",
    "   else:\n",
    "      #phimodel.model=tf.keras.layers.TFSMLayer(os.path.join(dirname,name),call_endpoint=\"serving_default\")\n",
    "      #phimodel.model=tf.keras.models.load_model(os.path.join(dirname, name) + \".keras\")\n",
    "      #print_sample_params(phimodel.model)\n",
    "      phimodel.model.load_weights(os.path.join(dirname, name) + '.weights.h5')\n",
    "      #print_sample_params(phimodel.model)\n",
    "      training_history=np.load(os.path.join(dirname, 'trainingHistory-' + name +'.npz'),allow_pickle=True)['arr_0'].item()\n",
    "\n",
    "   print(\"compiling\")\n",
    "   phimodel.compile(custom_metrics=cmetrics)\n",
    "   phimodelzero.compile(custom_metrics=cmetrics)\n",
    "\n",
    "   # compare validation loss before training for zero network and nonzero network\n",
    "   #need to re-enable learning, in case there's been a problem:\n",
    "   phimodel.learn_transition = True\n",
    "   phimodelzero.learn_transition = True\n",
    "   phimodel.learn_volk = True\n",
    "   phimodelzero.learn_volk = True\n",
    "\n",
    "   #problem - metricsnames aren't defined unless model has been trained, not just evaluated? SOlution - return_dict\n",
    "   valzero=phimodelzero.evaluate(datacasted[0],datacasted[1],return_dict=True)\n",
    "   valtrained=phimodel.evaluate(datacasted[0],datacasted[1],return_dict=True)\n",
    "\n",
    "\n",
    "   phimodel.learn_transition = True\n",
    "   phimodel.learn_volk = True\n",
    "\n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for final network: \")\n",
    "   print(valtrained)\n",
    "   print(\"ratio of trained to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valtrained.items()}))\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(phimodel,tf.cast(data[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   print(\"\\n\\n\")\n",
    "   #IMPLEMENT THE FOLLOWING\n",
    "   #meanfailuretosolveequation,_,_=measure_laplacian_failure(phimodel,data)\n",
    "   #print(\"\\n\\n\")\n",
    "   #print(\"mean of difference/mean of absolute value of source, weighted by sqrt(g): \" + str(meanfailuretosolveequation))\n",
    "   tf.keras.backend.clear_session()\n",
    "   return phimodel,training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42307dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_points_and_save_using_defaultsAlpha(free_coefficient,phimodel,euler_char,force_generate=False,seed_set=0,batch_size=128):\n",
    "   coefficients=coefficientsTQ\n",
    "   # coefficients=np.array([1, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, \\\n",
    "   # 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, free_coefficient, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 1, 0, 2, 0, 0, 0, 2, 0, 1])\n",
    "   monomials=monomialsTQ\n",
    "   kmoduli=kmoduliTQ\n",
    "   ambient=ambientTQ\n",
    "   # monomials = 5*np.eye(5, dtype=np.int64)\n",
    "   # coefficients = np.ones(5)\n",
    "   \n",
    "   # kmoduli = np.ones(1)\n",
    "   # ambient = np.array([4])\n",
    "   pg = PointGenerator(monomials, coefficients, kmoduli, ambient)\n",
    "   pg._set_seed(seed_set)\n",
    "\n",
    "\n",
    "   dirnameForMetric = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   dirnameAlpha= 'dataAlphaP/'+nameofmanifold+ \"Alpha_pg_with_\" + str(free_coefficient) \n",
    "   #dirnameAlpha = 'dataAlphaP/tetraquadricAlpha_pg_with_'+str(free_coefficient)+'forLB_'+lbstring\n",
    "   #dirnameForMetric = 'dataAlphaP/tetraquadric_pg_with_'+str(free_coefficient)\n",
    "   print(\"dirname for alpha: \" + dirnameForMetric)\n",
    "   print(\"dirname for alpha: \" + dirnameAlpha)\n",
    "\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirnameForMetric, 'basis.pickle'), allow_pickle=True))\n",
    "   \n",
    "   data=np.load(os.path.join(dirnameForMetric, 'dataset.npz'))\n",
    "\n",
    "   if force_generate or (not os.path.exists(dirnameAlpha)):\n",
    "      print(\"Generating: forced? \" + str(force_generate))\n",
    "      kappaAlpha = prepare_dataset_Alpha(pg,data,dirnameAlpha,phimodel,euler_char,BASIS,normalize_to_vol_j=True,batch_size=batch_size);\n",
    "   elif os.path.exists(dirnameAlpha):\n",
    "      try:\n",
    "         print(\"loading prexisting dataset\")\n",
    "         data = np.load(os.path.join(dirnameAlpha, 'dataset.npz'))\n",
    "      except:\n",
    "         print(\"problem loading data - generating anyway\")\n",
    "         kappaAlpha = prepare_dataset_Alpha(pg,data, dirnameAlpha,phimodel,euler_char,BASIS,normalize_to_vol_j=True,batch_size=batch_size);\n",
    "      \n",
    "   \n",
    "\n",
    "def getcallbacksandmetricsAlpha(dataalpha):\n",
    "   dataalpha_val_dict=dict(list(dict(dataalpha).items())[len(dict(dataalpha))//2:])\n",
    "   tcb = TransitionCallback((dataalpha['X_val'], dataalpha['y_val']))\n",
    "   lplcb = LaplacianCallback(dataalpha_val_dict)\n",
    "   # lplcb = LaplacianCallback(data_val)\n",
    "   cb_list = [lplcb,tcb]\n",
    "   cmetrics = [TotalLoss(), LaplacianLoss(), TransitionLoss()]\n",
    "   return cb_list, cmetrics\n",
    "\n",
    "   \n",
    "def train_and_save_nn_Alpha(free_coefficient,phimodel,euler_char,alphaprime,nlayer=3,nHidden=128,nEpochs=30,bSizes=[192,50000],stddev=0.1,lRate=0.001,alpha=[1,1],load_network=False,use_zero_network=False):\n",
    "   \n",
    "   dirnameForMetric = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   dirnameAlpha= 'dataAlphaP/'+nameofmanifold+ \"Alpha_pg_with_\" + str(free_coefficient) \n",
    "\n",
    "   #data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirnameForMetric, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "\n",
    "   dataalpha = np.load(os.path.join(dirnameAlpha, 'dataset.npz'))\n",
    "   dataalpha_train=tf.data.Dataset.from_tensor_slices(dict(list(dict(dataalpha).items())[:len(dict(dataalpha))//2]))\n",
    "   dataalpha_val_dict=dict(list(dict(dataalpha).items())[len(dict(dataalpha))//2:])\n",
    "   dataalpha_val=tf.data.Dataset.from_tensor_slices(dataalpha_val_dict)\n",
    "   # batch_sizes=[64,10000]\n",
    "   dataalpha_train=dataalpha_train.shuffle(buffer_size=1024).batch(bSizes[0])\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetricsAlpha(dataalpha)\n",
    "\n",
    "   #nlayer = 3\n",
    "   #nHidden = 128\n",
    "   act = 'gelu'\n",
    "   #nEpochs = 30\n",
    "   #bSizes = [192, 150000]\n",
    "   #alpha = [1., 1.] # 1 AND 3??\n",
    "   nfold = 3\n",
    "   n_in = 2*8\n",
    "   n_out = 1\n",
    "   #lRate = 0.001\n",
    "   name = 'alphamodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(nlayer) + 'x' +str(nHidden)\n",
    "   print(\"name: \" + name)\n",
    "\n",
    "   ambient=tf.cast(BASIS['AMBIENT'],tf.int32)\n",
    "\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   #nfirstlayer=tf.reduce_sum(((np.array(ambient)+1)**2)).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=stddev)\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   activ=tf.square\n",
    "   #activ=tfk.activations.gelu\n",
    "   #nn_alpha = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #copie from phi above\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "\n",
    "   #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.2)\n",
    "   #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)#note we don't need a last bias (flat direction)\n",
    "   #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network,kernel_initializer=initializer)#note we don't need a last bias (flat direction)\n",
    "   #nn_alpha_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)#note we don't need a last bias (flat direction)\n",
    "   \n",
    "   alphamodel= AlphaPrimeModel(nn_alpha,BASIS, phimodel,alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "   if load_network:\n",
    "      print(\"loading network\")\n",
    "      alphamodel.model=tf.keras.models.load_model(os.path.join(dirnameAlpha,name))\n",
    "      print(\"network loaded\")\n",
    "\n",
    "   alphamodelzero= AlphaPrimeModel(nn_alpha_zero,BASIS,phimodel, alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "\n",
    "   #Note, currently running legacy due to ongoing tf issue with M1/M2. \n",
    "   #Use the commented line instead if not on an M1/M2 machine\n",
    "   opt = tfk.optimizers.Adam(learning_rate=lRate)\n",
    "   #opt = tfk.optimizers.legacy.Adam(learning_rate=lRate)\n",
    "   # compile so we can test on validation set before training\n",
    "   alphamodel.compile(custom_metrics=cmetrics)\n",
    "   alphamodelzero.compile(custom_metrics=cmetrics)\n",
    "   \n",
    "   #datacasted=[tf.cast(data['X_val'],tf.float32),tf.cast(data['y_val'],tf.float32)]\n",
    "   valzero=alphamodelzero.test_step(dataalpha_val_dict)\n",
    "   valraw=alphamodel.test_step(dataalpha_val_dict)\n",
    "   valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   valraw = {key: value.numpy() for key, value in valraw.items()}\n",
    "   \n",
    "   training_historyAlpha={'transition_loss': [10**(-8)],'laplacian_loss': [1000000000000000]}\n",
    "   i=0\n",
    "   newLR=lRate\n",
    "   #while (training_historyAlpha['transition_loss'][-1]<10**(-5)) or (training_historyAlpha['laplacian_loss'][-1]>1.):\n",
    "   # continue looping if >10 or is nan\n",
    "   while i==0:#(training_historyAlpha['laplacian_loss'][-1]>10000000000000.) or (np.isnan( training_historyAlpha['laplacian_loss'][-1])):\n",
    "      print(\"trying iteration of training \"+str(i))\n",
    "      if i >0:\n",
    "\n",
    "         print('trying again laplacian_loss too big')\n",
    "         #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.2)\n",
    "         #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network,kernel_initializer=initializer)#note we don't need a last bias (flat direction)\n",
    "         #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         #nn_alpha = BiholoModelFuncGENERALforAlphainv2(shapeofnetwork,BASIS,activation=tfk.activations.gelu,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         #nn_alpha = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=False)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)#note we don't need a last bias (flat direction)\n",
    "         if newLR>0.0002:\n",
    "             newLR=newLR/2\n",
    "             print(\"new LR \" + str(newLR))\n",
    "         opt = tfk.optimizers.legacy.Adam(learning_rate=newLR)\n",
    "         alphamodel= AlphaPrimeModel(nn_alpha,BASIS, phimodel,alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "         cb_list, cmetrics = getcallbacksandmetricsAlpha(dataalpha)\n",
    "         alphamodel.compile(custom_metrics=cmetrics)\n",
    "      alphamodel, training_historyAlpha= train_modelalpha(alphamodel, dataalpha_train, optimizer=opt, epochs=nEpochs, batch_sizes=bSizes, \n",
    "                                        verbose=1, custom_metrics=cmetrics, callbacks=cb_list)\n",
    "      i+=1\n",
    "   print(\"finished training\\n\")\n",
    "   #alphamodel.model.save(os.path.join(dirnameAlpha, name))\n",
    "   phimodel.model.save_weights(os.path.join(dirname, name) + '.weights.h5')\n",
    "   np.savez_compressed(os.path.join(dirnameAlpha, 'trainingHistory-' + name),training_historyAlpha)\n",
    "   valfinal =alphamodel.test_step(dataalpha_val_dict)\n",
    "   valfinal = {key: value.numpy() for key, value in valfinal.items()}\n",
    "   #return training_historyAlpha\n",
    "   #now print the initial losses and final losses for each metric, by taking the first element of each key in the dictionary\n",
    "   #first_metrics = {key: value[0] for key, value in training_historyAlpha.items()}\n",
    "   #last_metrics = {key: value[-1] for key, value in training_historyAlpha.items()}\n",
    "\n",
    "   #print(\"initial losses\")\n",
    "   #print(first_metrics)\n",
    "   #print(\"final losses\")\n",
    "   #print(last_metrics)\n",
    "\n",
    "\n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for raw network: \")\n",
    "   print(valraw)\n",
    "   print(\"validation loss for final network: \")\n",
    "   print(valfinal)\n",
    "   print(\"ratio of final to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valfinal.items()}))\n",
    "   print(\"ratio of final to raw: \" + str({key + \" ratio\": value/(valraw[key]+1e-8) for key, value in valfinal.items()}))\n",
    "\n",
    "\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(alphamodel,tf.cast(dataalpha[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   meanfailuretosolveequation,_,_=HYM_measure_val(alphamodel,dataalpha)\n",
    "   print(\"mean of difference/mean of absolute value of source, weighted by sqrt(g): \" + str(meanfailuretosolveequation))\n",
    "   print(\"\\n\\n\")\n",
    "   tf.keras.backend.clear_session()\n",
    "   return alphamodel,training_historyAlpha\n",
    "\n",
    "def load_nn_Alpha(free_coefficient,phimodel,euler_char,alphaprime,nlayer=3,nHidden=128,nEpochs=30,bSizes=[192,50000],stddev=0.1,lRate=0.001,alpha=[1,1],set_weights_to_zero=False):\n",
    "   dirnameForMetric = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   dirnameAlpha= 'dataAlphaP/'+nameofmanifold+ \"Alpha_pg_with_\" + str(free_coefficient) \n",
    "\n",
    "   name = 'alphamodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(nlayer) + 'x' +str(nHidden)\n",
    "   print(\"name: \" + name)\n",
    "\n",
    "   #data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirnameForMetric, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "\n",
    "   dataalpha = np.load(os.path.join(dirnameAlpha, 'dataset.npz'))\n",
    "   dataalpha_train=tf.data.Dataset.from_tensor_slices(dict(list(dict(dataalpha).items())[:len(dict(dataalpha))//2]))\n",
    "   dataalpha_val_dict=dict(list(dict(dataalpha).items())[len(dict(dataalpha))//2:])\n",
    "   dataalpha_val=tf.data.Dataset.from_tensor_slices(dataalpha_val_dict)\n",
    "   # batch_sizes=[64,10000]\n",
    "   dataalpha_train=dataalpha_train.shuffle(buffer_size=1024).batch(bSizes[0])\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetricsAlpha(dataalpha)\n",
    "\n",
    "   #nlayer = 3\n",
    "   #nHidden = 128\n",
    "   act = 'gelu'\n",
    "   #nEpochs = 30\n",
    "   #bSizes = [192, 150000]\n",
    "   alpha = [1., 1.] # 1 AND 3??\n",
    "   nfold = 3\n",
    "   n_in = 2*8\n",
    "   n_out = 1\n",
    "   #lRate = 0.001\n",
    "   ambient=tf.cast(BASIS['AMBIENT'],tf.int32)\n",
    "\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=stddev)\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   activ=tf.square\n",
    "   nn_alpha = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,stddev=stddev,use_zero_network=set_weights_to_zero)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_alpha_zero = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #copie from phi above\n",
    "   #nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "   \n",
    "   #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)#note we don't need a last bias (flat direction)\n",
    "   #nn_alpha_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)#note we don't need a last bias (flat direction)\n",
    "   alphamodel= AlphaPrimeModel(nn_alpha,BASIS, phimodel,alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "   alphamodelzero= AlphaPrimeModel(nn_alpha_zero,BASIS,phimodel, alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "\n",
    "   alphamodel(dataalpha['X_train'][0:1])\n",
    "   alphamodelzero(dataalpha['X_train'][0:1])\n",
    "\n",
    "\n",
    "   if set_weights_to_zero:\n",
    "      training_historyAlpha=0\n",
    "   else:\n",
    "      #alphamodel.model=tf.keras.models.load_model(os.path.join(dirnameAlpha,name))\n",
    "      alphamodel.model.load_weights(os.path.join(dirnameAlpha, name) + '.weights.h5')\n",
    "      training_historyAlpha=np.load(os.path.join(dirnameAlpha, 'trainingHistory-' + name +'.npz'),allow_pickle=True)['arr_0'].item()\n",
    "\n",
    "   alphamodel.compile(custom_metrics=cmetrics)\n",
    "   alphamodelzero.compile(custom_metrics=cmetrics)\n",
    "   \n",
    "   valzero=alphamodelzero.evaluate(dataalpha_val_dict,return_dict=True)\n",
    "   valtrained=alphamodel.evaluate(dataalpha_val_dict,return_dict=True)\n",
    "   #valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   #valtrained= {key: value.numpy() for key, value in valtrained.items()}\n",
    "\n",
    "\n",
    "   #metricsnames=alphamodel.metrics_names\n",
    "\n",
    "   #valtrained= {metricsnames[i]: valtrained[i] for i in range(len(valtrained))}\n",
    "   ##valzero = {metricsnames[i]: valzero[i] for i in range(len(valzero))}\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for trained network: \")\n",
    "   print(valtrained)\n",
    "   print(\"ratio of trained to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valtrained.items()}))\n",
    "\n",
    "\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(alphamodel,tf.cast(dataalpha[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   meanfailuretosolveequation,_,_=HYM_measure_val(alphamodel,dataalpha)\n",
    "   print(\"mean of difference/mean of absolute value of source, weighted by sqrt(g): \" + str(meanfailuretosolveequation))\n",
    "   print(\"\\n\\n\")\n",
    "   return alphamodel,training_historyAlpha\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d277c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cymetricTESTING217",
   "language": "python",
   "name": "cymetrictesting217"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
