{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddabf73c",
   "metadata": {},
   "source": [
    "# Training a NN for metric on CICY with homog\n",
    "\n",
    "## Import the required packages/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a9cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import pickle\n",
    "import sys\n",
    "#sys.path.append(\"/Users/kit/Documents/Phys_Working/MF metric\")\n",
    "#sys.path.append(\"/home/f/fraser-talientec/PhysicalYukawas\")\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout)\n",
    "\n",
    "from cymetric.pointgen.pointgen import PointGenerator\n",
    "from cymetric.pointgen.nphelper import prepare_dataset, prepare_basis_pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "from cymetric.models.tfmodels import PhiFSModel, MultFSModel, FreeModel, MatrixFSModel, AddFSModel, PhiFSModelToric, MatrixFSModelToric\n",
    "from cymetric.models.tfhelper import prepare_tf_basis, train_model\n",
    "from cymetric.models.callbacks import SigmaCallback, KaehlerCallback, TransitionCallback, RicciCallback, VolkCallback, AlphaCallback\n",
    "from cymetric.models.metrics import SigmaLoss, KaehlerLoss, TransitionLoss, RicciLoss, VolkLoss, TotalLoss\n",
    "\n",
    "from NewCustomMetrics import *\n",
    "from HarmonicFormModel import *\n",
    "from BetaModel import *\n",
    "from laplacian_funcs import *\n",
    "from OneAndTwoFormsForLineBundles import *\n",
    "from generate_and_train_all_nnsHOLO import *\n",
    "from custom_networks import *\n",
    "import sys\n",
    "import importlib\n",
    "from AlphaPrimeModel import *\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d00ecc1-f6f4-47d4-a2bb-f7830e164093",
   "metadata": {},
   "source": [
    "## Point Cloud Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ba80cc",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d1dd93a",
   "metadata": {},
   "source": [
    "Set the properties of the defining polynomial. And the point in Kahler Moduli space\n",
    "\n",
    "If correct, this should be for the following defining polynomial\n",
    "$0.44 x_{1,0}^2 x_{3,0}^2 x_{4,0}^2 x_{2,0}^2+0.88 x_{1,1}^2 x_{3,0}^2 x_{4,0}^2 x_{2,0}^2+0.88 x_{1,0}^2 x_{3,1}^2 x_{4,0}^2 x_{2,0}^2+0.88 x_{1,1}^2 x_{3,1}^2 x_{4,0}^2 x_{2,0}^2-0.03 x_{1,0} x_{1,1} x_{3,0} x_{3,1} x_{4,0}^2 x_{2,0}^2+0.44 x_{1,0}^2 x_{3,0}^2 x_{4,1}^2 x_{2,0}^2+0.44 x_{1,1}^2 x_{3,0}^2 x_{4,1}^2 x_{2,0}^2+0.88 x_{1,0}^2 x_{3,1}^2 x_{4,1}^2 x_{2,0}^2+0.44 x_{1,1}^2 x_{3,1}^2 x_{4,1}^2 x_{2,0}^2-0.41 x_{1,0} x_{1,1} x_{3,0} x_{3,1} x_{4,1}^2 x_{2,0}^2-0.41 x_{1,0} x_{1,1} x_{3,0}^2 x_{4,0} x_{4,1} x_{2,0}^2-0.03 x_{1,0} x_{1,1} x_{3,1}^2 x_{4,0} x_{4,1} x_{2,0}^2+0.62 x_{1,0}^2 x_{3,0} x_{3,1} x_{4,0} x_{4,1} x_{2,0}^2+0.62 x_{1,1}^2 x_{3,0} x_{3,1} x_{4,0} x_{4,1} x_{2,0}^2-0.62 x_{1,0} x_{1,1} x_{2,1} x_{3,0}^2 x_{4,0}^2 x_{2,0}-0.62 x_{1,0} x_{1,1} x_{2,1} x_{3,1}^2 x_{4,0}^2 x_{2,0}+0.41 x_{1,0}^2 x_{2,1} x_{3,0} x_{3,1} x_{4,0}^2 x_{2,0}+0.03 x_{1,1}^2 x_{2,1} x_{3,0} x_{3,1} x_{4,0}^2 x_{2,0}-0.62 x_{1,0} x_{1,1} x_{2,1} x_{3,0}^2 x_{4,1}^2 x_{2,0}-0.62 x_{1,0} x_{1,1} x_{2,1} x_{3,1}^2 x_{4,1}^2 x_{2,0}+0.03 x_{1,0}^2 x_{2,1} x_{3,0} x_{3,1} x_{4,1}^2 x_{2,0}+0.41 x_{1,1}^2 x_{2,1} x_{3,0} x_{3,1} x_{4,1}^2 x_{2,0}+0.41 x_{1,0}^2 x_{2,1} x_{3,0}^2 x_{4,0} x_{4,1} x_{2,0}+0.03 x_{1,1}^2 x_{2,1} x_{3,0}^2 x_{4,0} x_{4,1} x_{2,0}+0.03 x_{1,0}^2 x_{2,1} x_{3,1}^2 x_{4,0} x_{4,1} x_{2,0}+0.41 x_{1,1}^2 x_{2,1} x_{3,1}^2 x_{4,0} x_{4,1} x_{2,0}+0.9 x_{1,0} x_{1,1} x_{2,1} x_{3,0} x_{3,1} x_{4,0} x_{4,1} x_{2,0}+0.44 x_{1,0}^2 x_{2,1}^2 x_{3,0}^2 x_{4,0}^2+0.88 x_{1,1}^2 x_{2,1}^2 x_{3,0}^2 x_{4,0}^2+0.44 x_{1,0}^2 x_{2,1}^2 x_{3,1}^2 x_{4,0}^2+0.44 x_{1,1}^2 x_{2,1}^2 x_{3,1}^2 x_{4,0}^2-0.41 x_{1,0} x_{1,1} x_{2,1}^2 x_{3,0} x_{3,1} x_{4,0}^2+0.88 x_{1,0}^2 x_{2,1}^2 x_{3,0}^2 x_{4,1}^2+0.88 x_{1,1}^2 x_{2,1}^2 x_{3,0}^2 x_{4,1}^2+0.88 x_{1,0}^2 x_{2,1}^2 x_{3,1}^2 x_{4,1}^2+0.44 x_{1,1}^2 x_{2,1}^2 x_{3,1}^2 x_{4,1}^2-0.03 x_{1,0} x_{1,1} x_{2,1}^2 x_{3,0} x_{3,1} x_{4,1}^2-0.03 x_{1,0} x_{1,1} x_{2,1}^2 x_{3,0}^2 x_{4,0} x_{4,1}-0.41 x_{1,0} x_{1,1} x_{2,1}^2 x_{3,1}^2 x_{4,0} x_{4,1}+0.62 x_{1,0}^2 x_{2,1}^2 x_{3,0} x_{3,1} x_{4,0} x_{4,1}+0.62 x_{1,1}^2 x_{2,1}^2 x_{3,0} x_{3,1} x_{4,0} x_{4,1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281a1979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe185432",
   "metadata": {},
   "outputs": [],
   "source": [
    "monomialsTQ = 5*np.eye(5, dtype=np.int64)\n",
    "coefficientsTQ = np.ones(5)\n",
    "kmoduliTQ = np.ones(1)\n",
    "ambientTQ = np.array([4])\n",
    "nameofmanifold=\"Quintic\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_points_and_save_using_defaults(free_coefficient,number_points,force_generate=False,seed_set=0):\n",
    "   coefficients=coefficientsTQ\n",
    "   # coefficients=np.array([1, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, \\\n",
    "   # 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, free_coefficient, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 1, 0, 2, 0, 0, 0, 2, 0, 1])\n",
    "   monomials=monomialsTQ\n",
    "   kmoduli=kmoduliTQ\n",
    "   ambient=ambientTQ\n",
    "   # monomials = 5*np.eye(5, dtype=np.int64)\n",
    "   # coefficients = np.ones(5)\n",
    "   \n",
    "   # kmoduli = np.ones(1)\n",
    "   # ambient = np.array([4])\n",
    "   pg = PointGenerator(monomials, coefficients, kmoduli, ambient)\n",
    "   pg._set_seed(seed_set)\n",
    "   dirname = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   print(\"dirname: \" + dirname)\n",
    "   #test if the directory exists, if not, create it\n",
    "   if force_generate or (not os.path.exists(dirname)):\n",
    "      print(\"Generating: forced? \" + str(force_generate))\n",
    "      kappa = pg.prepare_dataset(number_points, dirname)\n",
    "      pg.prepare_basis(dirname, kappa=kappa)\n",
    "   elif os.path.exists(dirname):\n",
    "      try:\n",
    "         print(\"loading prexisting dataset\")\n",
    "         data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "         if (len(data['X_train'])+len(data['X_val']))!=number_points:\n",
    "            print(\"wrong length - generating anyway\")\n",
    "            kappa = pg.prepare_dataset(number_points, dirname)\n",
    "            pg.prepare_basis(dirname, kappa=kappa)\n",
    "      except:\n",
    "         print(\"error loading - generating anyway\")\n",
    "         kappa = pg.prepare_dataset(number_points, dirname)\n",
    "         pg.prepare_basis(dirname, kappa=kappa)\n",
    "   \n",
    "\n",
    "def getcallbacksandmetrics(data):\n",
    "   #rcb = RicciCallback((data['X_val'], data['y_val']), data['val_pullbacks'])\n",
    "   scb = SigmaCallback((data['X_val'], data['y_val']))\n",
    "   volkcb = VolkCallback((data['X_val'], data['y_val']))\n",
    "   kcb = KaehlerCallback((data['X_val'], data['y_val']))\n",
    "   tcb = TransitionCallback((data['X_val'], data['y_val']))\n",
    "   #cb_list = [rcb, scb, kcb, tcb, volkcb]\n",
    "   cb_list = [ scb, kcb, tcb, volkcb]\n",
    "   cmetrics = [TotalLoss(), SigmaLoss(), KaehlerLoss(), TransitionLoss(), VolkLoss()]#, RicciLoss()]\n",
    "   return cb_list, cmetrics\n",
    "\n",
    "def train_and_save_nn(free_coefficient,nlayer=3,nHidden=128,nEpochs=50,stddev=0.1,bSizes=[192,50000],lRate=0.001,use_zero_network=False):\n",
    "   dirname = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   name = 'phimodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(bSizes[1]) + 's' + str(nlayer) + 'x' +str(nHidden)\n",
    "   print('dirname: ' + dirname)\n",
    "   print('name: ' + name)\n",
    "   \n",
    "   data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirname, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetrics(data)\n",
    "\n",
    "   act = 'gelu'\n",
    "   alpha = [1., 1., 30., 1., 2.] # 1 AND 3??\n",
    "   ambient=tf.cast(tf.math.abs(BASIS['AMBIENT']),tf.int32)\n",
    "\n",
    "   #nfirstlayer=tf.reduce_prod(2*(np.array(ambient)+1)).numpy().item()\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   nn_phi = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_phi=make_nn(10,1,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_phi_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   # print(nn_phi_zero(tf.cast(data['X_val'][0:2],tf.float32)))\n",
    "   #nn_phi_zero=make_nn(10,1,nlayer,nHidden,act,use_zero_network=True)\n",
    "   phimodel = PhiFSModel(nn_phi, BASIS, alpha=alpha)\n",
    "   phimodelzero = PhiFSModel(nn_phi_zero, BASIS, alpha=alpha)\n",
    "\n",
    "   #Note, currently running legacy due to ongoing tf issue with M1/M2. \n",
    "   #Use the commented line instead if not on an M1/M2 machine\n",
    "   opt = tfk.optimizers.Adam(learning_rate=lRate)\n",
    "   #opt = tfk.optimizers.legacy.Adam(learning_rate=lRate)\n",
    "   # compile so we can test on validation set before training\n",
    "   phimodel.compile(custom_metrics=cmetrics)\n",
    "   phimodelzero.compile(custom_metrics=cmetrics)\n",
    "\n",
    "   ## compare validation loss before training for zero network and nonzero network\n",
    "   datacasted=[tf.cast(data['X_val'],tf.float32),tf.cast(data['y_val'],tf.float32)]\n",
    "   #need to re-enable learning, in case there's been a problem:\n",
    "   phimodel.learn_transition = False\n",
    "   phimodelzero.learn_transition = False\n",
    "   phimodel.learn_volk = True\n",
    "   phimodelzero.learn_volk = True\n",
    "   #phimodel.learn_ricci_val= True\n",
    "   #phimodelzero.learn_ricci_val= True\n",
    "   valzero=phimodelzero.test_step(datacasted)\n",
    "   valraw=phimodel.test_step(datacasted)\n",
    "   # phimodel.learn_ricci_val=False \n",
    "   # phimodelzero.learn_ricci_val=False \n",
    "   valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   valraw = {key: value.numpy() for key, value in valraw.items()}\n",
    "\n",
    "   phimodel, training_history = train_model(phimodel, data, optimizer=opt, epochs=nEpochs, batch_sizes=bSizes, \n",
    "                                       verbose=1, custom_metrics=cmetrics, callbacks=cb_list)\n",
    "   print(\"finished training\\n\")\n",
    "   phimodel.model.save(os.path.join(dirname, name))\n",
    "   np.savez_compressed(os.path.join(dirname, 'trainingHistory-' + name),training_history)\n",
    "   #now print the initial losses and final losses for each metric\n",
    "   # first_metrics = {key: value[0] for key, value in training_history.items()}\n",
    "   # lastometrics = {key: value[-1] for key, value in training_history.items()}\n",
    "   phimodel.learn_transition = True\n",
    "   phimodel.learn_volk = True\n",
    "   #phimodel.learn_ricci_val= True\n",
    "   valfinal=phimodel.test_step(datacasted)\n",
    "   valfinal = {key: value.numpy() for key, value in valfinal.items()}\n",
    "   #phimodel.learn_ricci_val=False \n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for raw network: \")\n",
    "   print(valraw)\n",
    "   print(\"validation loss for final network: \")\n",
    "   print(valfinal)\n",
    "   print(\"ratio of final to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valfinal.items()}))\n",
    "   print(\"ratio of final to raw: \" + str({key + \" ratio\": value/(valraw[key]+1e-8) for key, value in valfinal.items()}))\n",
    "\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(phimodel,tf.cast(data[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   #IMPLEMENT THE FOLLOWING\n",
    "   #meanfailuretosolveequation,_,_=measure_laplacian_failure(phimodel,data)\n",
    "   print(\"\\n\\n\")\n",
    "   return phimodel,training_history\n",
    "\n",
    "def load_nn_phimodel(free_coefficient,nlayer=3,nHidden=128,nEpochs=50,bSizes=[192,50000],stddev=0.1,lRate=0.001,set_weights_to_zero=False):\n",
    "   dirname = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   name = 'phimodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(bSizes[1]) + 's' + str(nlayer) + 'x' +str(nHidden)\n",
    "   print(dirname)\n",
    "   print(name)\n",
    "   \n",
    "   data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirname, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetrics(data)\n",
    "\n",
    "\n",
    "   act = 'gelu'\n",
    "\n",
    "   alpha = [1., 1., 30., 1., 2.] # 1 AND 3??\n",
    " \n",
    "\n",
    "\n",
    "   ambient=tf.cast(BASIS['AMBIENT'],tf.int32)\n",
    "\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   nn_phi = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_phi_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "\n",
    "\n",
    "#    nn_phi = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "#    nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "   phimodel = PhiFSModel(nn_phi, BASIS, alpha=alpha)\n",
    "   phimodelzero = PhiFSModel(nn_phi_zero, BASIS, alpha=alpha)\n",
    "\n",
    "   if set_weights_to_zero:\n",
    "      training_history=0\n",
    "   else:\n",
    "      phimodel.model=tf.keras.models.load_model(os.path.join(dirname,name))\n",
    "      training_history=np.load(os.path.join(dirname, 'trainingHistory-' + name +'.npz'),allow_pickle=True)['arr_0'].item()\n",
    "\n",
    "   phimodel.compile(custom_metrics=cmetrics)\n",
    "   phimodelzero.compile(custom_metrics=cmetrics)\n",
    "\n",
    "   # compare validation loss before training for zero network and nonzero network\n",
    "   datacasted=[tf.cast(data['X_val'],tf.float32),tf.cast(data['y_val'],tf.float32)]\n",
    "   #need to re-enable learning, in case there's been a problem:\n",
    "   phimodel.learn_transition = True\n",
    "   phimodelzero.learn_transition = True\n",
    "   phimodel.learn_volk = True\n",
    "   phimodelzero.learn_volk = True\n",
    "   #phimodel.learn_ricci_val= True\n",
    "   #phimodelzero.learn_ricci_val= True\n",
    "   valzero=phimodelzero.evaluate(datacasted[0],datacasted[1])\n",
    "   valtrained=phimodel.evaluate(datacasted[0],datacasted[1])\n",
    "   metricsnames=phimodel.metrics_names\n",
    "   # phimodel.learn_ricci_val=False \n",
    "   # phimodelzero.learn_ricci_val=False \n",
    "   valzero = {metricsnames[i]: valzero[i] for i in range(len(valzero))}\n",
    "   valtrained= {metricsnames[i]: valtrained[i] for i in range(len(valtrained))}\n",
    "\n",
    "   #valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   #valtrained = {key: value.numpy() for key, value in valtrained.items()}\n",
    "\n",
    "   #valtrained = {key: value.numpy() for key, value in valtrained.items()}\n",
    "\n",
    "   phimodel.learn_transition = True\n",
    "   phimodel.learn_volk = True\n",
    "\n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for final network: \")\n",
    "   print(valtrained)\n",
    "   print(\"ratio of trained to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valtrained.items()}))\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(phimodel,tf.cast(data[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   print(\"\\n\\n\")\n",
    "   #IMPLEMENT THE FOLLOWING\n",
    "   #meanfailuretosolveequation,_,_=measure_laplacian_failure(phimodel,data)\n",
    "   #print(\"\\n\\n\")\n",
    "   #print(\"mean of difference/mean of absolute value of source, weighted by sqrt(g): \" + str(meanfailuretosolveequation))\n",
    "   return phimodel,training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f55f1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "848e584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_points_and_save_using_defaultsAlpha(free_coefficient,phimodel,euler_char,force_generate=False,seed_set=0):\n",
    "   coefficients=coefficientsTQ\n",
    "   # coefficients=np.array([1, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, \\\n",
    "   # 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, free_coefficient, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 1, 0, 2, 0, 0, 0, 2, 0, 1])\n",
    "   monomials=monomialsTQ\n",
    "   kmoduli=kmoduliTQ\n",
    "   ambient=ambientTQ\n",
    "   # monomials = 5*np.eye(5, dtype=np.int64)\n",
    "   # coefficients = np.ones(5)\n",
    "   \n",
    "   # kmoduli = np.ones(1)\n",
    "   # ambient = np.array([4])\n",
    "   pg = PointGenerator(monomials, coefficients, kmoduli, ambient)\n",
    "   pg._set_seed(seed_set)\n",
    "\n",
    "\n",
    "   dirnameForMetric = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   dirnameAlpha= 'dataAlphaP/'+nameofmanifold+ \"Alpha_pg_with_\" + str(free_coefficient) \n",
    "   #dirnameAlpha = 'dataAlphaP/tetraquadricAlpha_pg_with_'+str(free_coefficient)+'forLB_'+lbstring\n",
    "   #dirnameForMetric = 'dataAlphaP/tetraquadric_pg_with_'+str(free_coefficient)\n",
    "   print(\"dirname for alpha: \" + dirnameForMetric)\n",
    "   print(\"dirname for alpha: \" + dirnameAlpha)\n",
    "\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirnameForMetric, 'basis.pickle'), allow_pickle=True))\n",
    "   \n",
    "   data=np.load(os.path.join(dirnameForMetric, 'dataset.npz'))\n",
    "\n",
    "   if force_generate or (not os.path.exists(dirnameAlpha)):\n",
    "      print(\"Generating: forced? \" + str(force_generate))\n",
    "      kappaAlpha = prepare_dataset_Alpha(pg,data,dirnameAlpha,phimodel,euler_char,BASIS,normalize_to_vol_j=True);\n",
    "   elif os.path.exists(dirnameAlpha):\n",
    "      try:\n",
    "         print(\"loading prexisting dataset\")\n",
    "         data = np.load(os.path.join(dirnameAlpha, 'dataset.npz'))\n",
    "      except:\n",
    "         print(\"problem loading data - generating anyway\")\n",
    "         kappaAlpha = prepare_dataset_Alpha(pg,data, dirnameAlpha,phimodel,euler_char,BASIS,normalize_to_vol_j=True);\n",
    "      \n",
    "   \n",
    "\n",
    "def getcallbacksandmetricsAlpha(dataalpha):\n",
    "   dataalpha_val_dict=dict(list(dict(dataalpha).items())[len(dict(dataalpha))//2:])\n",
    "   tcb = TransitionCallback((dataalpha['X_val'], dataalpha['y_val']))\n",
    "   lplcb = LaplacianCallback(dataalpha_val_dict)\n",
    "   # lplcb = LaplacianCallback(data_val)\n",
    "   cb_list = [lplcb,tcb]\n",
    "   cmetrics = [TotalLoss(), LaplacianLoss(), TransitionLoss()]\n",
    "   return cb_list, cmetrics\n",
    "\n",
    "   \n",
    "def train_and_save_nn_Alpha(free_coefficient,phimodel,euler_char,alphaprime,nlayer=3,nHidden=128,nEpochs=30,bSizes=[192,50000],stddev=0.1,lRate=0.001,use_zero_network=False,alpha=[1,1],load_network=False):\n",
    "   \n",
    "   dirnameForMetric = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   dirnameAlpha= 'dataAlphaP/'+nameofmanifold+ \"Alpha_pg_with_\" + str(free_coefficient) \n",
    "\n",
    "   #data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirnameForMetric, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "\n",
    "   dataalpha = np.load(os.path.join(dirnameAlpha, 'dataset.npz'))\n",
    "   dataalpha_train=tf.data.Dataset.from_tensor_slices(dict(list(dict(dataalpha).items())[:len(dict(dataalpha))//2]))\n",
    "   dataalpha_val_dict=dict(list(dict(dataalpha).items())[len(dict(dataalpha))//2:])\n",
    "   dataalpha_val=tf.data.Dataset.from_tensor_slices(dataalpha_val_dict)\n",
    "   # batch_sizes=[64,10000]\n",
    "   dataalpha_train=dataalpha_train.shuffle(buffer_size=1024).batch(bSizes[0])\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetricsAlpha(dataalpha)\n",
    "\n",
    "   #nlayer = 3\n",
    "   #nHidden = 128\n",
    "   act = 'gelu'\n",
    "   #nEpochs = 30\n",
    "   #bSizes = [192, 150000]\n",
    "   #alpha = [1., 1.] # 1 AND 3??\n",
    "   nfold = 3\n",
    "   n_in = 2*8\n",
    "   n_out = 1\n",
    "   #lRate = 0.001\n",
    "   name = 'alphamodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(nlayer) + 'x' +str(nHidden)\n",
    "   print(\"name: \" + name)\n",
    "\n",
    "   ambient=tf.cast(BASIS['AMBIENT'],tf.int32)\n",
    "\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   #nfirstlayer=tf.reduce_sum(((np.array(ambient)+1)**2)).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=stddev)\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   activ=tf.square\n",
    "   #activ=tfk.activations.gelu\n",
    "   #nn_alpha = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=False)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_alpha_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #copie from phi above\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "\n",
    "   #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.2)\n",
    "   #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)#note we don't need a last bias (flat direction)\n",
    "   #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network,kernel_initializer=initializer)#note we don't need a last bias (flat direction)\n",
    "   #nn_alpha_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)#note we don't need a last bias (flat direction)\n",
    "   \n",
    "   alphamodel= AlphaPrimeModel(nn_alpha,BASIS, phimodel,alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "   if load_network:\n",
    "      print(\"loading network\")\n",
    "      alphamodel.model=tf.keras.models.load_model(os.path.join(dirnameAlpha,name))\n",
    "      print(\"network loaded\")\n",
    "\n",
    "   alphamodelzero= AlphaPrimeModel(nn_alpha_zero,BASIS,phimodel, alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "\n",
    "   #Note, currently running legacy due to ongoing tf issue with M1/M2. \n",
    "   #Use the commented line instead if not on an M1/M2 machine\n",
    "   #opt = tfk.optimizers.Adam(learning_rate=lRate)\n",
    "   opt = tfk.optimizers.legacy.Adam(learning_rate=lRate)\n",
    "   # compile so we can test on validation set before training\n",
    "   alphamodel.compile(custom_metrics=cmetrics)\n",
    "   alphamodelzero.compile(custom_metrics=cmetrics)\n",
    "   \n",
    "   #datacasted=[tf.cast(data['X_val'],tf.float32),tf.cast(data['y_val'],tf.float32)]\n",
    "   valzero=alphamodelzero.test_step(dataalpha_val_dict)\n",
    "   valraw=alphamodel.test_step(dataalpha_val_dict)\n",
    "   valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   valraw = {key: value.numpy() for key, value in valraw.items()}\n",
    "   \n",
    "   training_historyAlpha={'transition_loss': [10**(-8)],'laplacian_loss': [1000000000000000]}\n",
    "   i=0\n",
    "   newLR=lRate\n",
    "   #while (training_historyAlpha['transition_loss'][-1]<10**(-5)) or (training_historyAlpha['laplacian_loss'][-1]>1.):\n",
    "   # continue looping if >10 or is nan\n",
    "   while i==0:#(training_historyAlpha['laplacian_loss'][-1]>10000000000000.) or (np.isnan( training_historyAlpha['laplacian_loss'][-1])):\n",
    "      print(\"trying iteration of training \"+str(i))\n",
    "      if i >0:\n",
    "\n",
    "         print('trying again laplacian_loss too big')\n",
    "         #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.2)\n",
    "         #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network,kernel_initializer=initializer)#note we don't need a last bias (flat direction)\n",
    "         #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         #nn_alpha = BiholoModelFuncGENERALforAlphainv2(shapeofnetwork,BASIS,activation=tfk.activations.gelu,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         #nn_alpha = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=False)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)#note we don't need a last bias (flat direction)\n",
    "         if newLR>0.0002:\n",
    "             newLR=newLR/2\n",
    "             print(\"new LR \" + str(newLR))\n",
    "         opt = tfk.optimizers.legacy.Adam(learning_rate=newLR)\n",
    "         alphamodel= AlphaPrimeModel(nn_alpha,BASIS, phimodel,alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "         cb_list, cmetrics = getcallbacksandmetricsAlpha(dataalpha)\n",
    "         alphamodel.compile(custom_metrics=cmetrics)\n",
    "      alphamodel, training_historyAlpha= train_modelalpha(alphamodel, dataalpha_train, optimizer=opt, epochs=nEpochs, batch_sizes=bSizes, \n",
    "                                        verbose=1, custom_metrics=cmetrics, callbacks=cb_list)\n",
    "      i+=1\n",
    "   print(\"finished training\\n\")\n",
    "   alphamodel.model.save(os.path.join(dirnameAlpha, name))\n",
    "   np.savez_compressed(os.path.join(dirnameAlpha, 'trainingHistory-' + name),training_historyAlpha)\n",
    "   valfinal =alphamodel.test_step(dataalpha_val_dict)\n",
    "   valfinal = {key: value.numpy() for key, value in valfinal.items()}\n",
    "   #return training_historyAlpha\n",
    "   #now print the initial losses and final losses for each metric, by taking the first element of each key in the dictionary\n",
    "   #first_metrics = {key: value[0] for key, value in training_historyAlpha.items()}\n",
    "   #last_metrics = {key: value[-1] for key, value in training_historyAlpha.items()}\n",
    "\n",
    "   #print(\"initial losses\")\n",
    "   #print(first_metrics)\n",
    "   #print(\"final losses\")\n",
    "   #print(last_metrics)\n",
    "\n",
    "\n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for raw network: \")\n",
    "   print(valraw)\n",
    "   print(\"validation loss for final network: \")\n",
    "   print(valfinal)\n",
    "   print(\"ratio of final to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valfinal.items()}))\n",
    "   print(\"ratio of final to raw: \" + str({key + \" ratio\": value/(valraw[key]+1e-8) for key, value in valfinal.items()}))\n",
    "\n",
    "\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(alphamodel,tf.cast(dataalpha[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   meanfailuretosolveequation,_,_=HYM_measure_val(alphamodel,dataalpha)\n",
    "   print(\"mean of difference/mean of absolute value of source, weighted by sqrt(g): \" + str(meanfailuretosolveequation))\n",
    "   print(\"\\n\\n\")\n",
    "   tf.keras.backend.clear_session()\n",
    "   return alphamodel,training_historyAlpha\n",
    "\n",
    "def load_nn_Alpha(free_coefficient,phimodel,euler_char,alphaprime,nlayer=3,nHidden=128,nEpochs=30,bSizes=[192,50000],stddev=0.1,lRate=0.001,use_zero_network=False,alpha=[1,1],load_network=False):\n",
    "   dirnameForMetric = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   dirnameAlpha= 'dataAlphaP/'+nameofmanifold+ \"Alpha_pg_with_\" + str(free_coefficient) \n",
    "\n",
    "   name = 'alphamodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(nlayer) + 'x' +str(nHidden)\n",
    "   print(\"name: \" + name)\n",
    "\n",
    "   #data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirnameForMetric, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "\n",
    "   dataalpha = np.load(os.path.join(dirnameAlpha, 'dataset.npz'))\n",
    "   dataalpha_train=tf.data.Dataset.from_tensor_slices(dict(list(dict(dataalpha).items())[:len(dict(dataalpha))//2]))\n",
    "   dataalpha_val_dict=dict(list(dict(dataalpha).items())[len(dict(dataalpha))//2:])\n",
    "   dataalpha_val=tf.data.Dataset.from_tensor_slices(dataalpha_val_dict)\n",
    "   # batch_sizes=[64,10000]\n",
    "   dataalpha_train=dataalpha_train.shuffle(buffer_size=1024).batch(bSizes[0])\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetricsAlpha(dataalpha)\n",
    "\n",
    "   #nlayer = 3\n",
    "   #nHidden = 128\n",
    "   act = 'gelu'\n",
    "   #nEpochs = 30\n",
    "   #bSizes = [192, 150000]\n",
    "   alpha = [1., 1.] # 1 AND 3??\n",
    "   nfold = 3\n",
    "   n_in = 2*8\n",
    "   n_out = 1\n",
    "   #lRate = 0.001\n",
    "   ambient=tf.cast(BASIS['AMBIENT'],tf.int32)\n",
    "\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=stddev)\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   activ=tf.square\n",
    "   nn_alpha = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,stddev=stddev)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_alpha_zero = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #copie from phi above\n",
    "   #nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "   \n",
    "   #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)#note we don't need a last bias (flat direction)\n",
    "   #nn_alpha_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)#note we don't need a last bias (flat direction)\n",
    "   alphamodel= AlphaPrimeModel(nn_alpha,BASIS, phimodel,alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "   alphamodelzero= AlphaPrimeModel(nn_alpha_zero,BASIS,phimodel, alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "\n",
    "   if set_weights_to_zero:\n",
    "      training_historyAlpha=0\n",
    "   else:\n",
    "      alphamodel.model=tf.keras.models.load_model(os.path.join(dirnameAlpha,name))\n",
    "      training_historyAlpha=np.load(os.path.join(dirnameAlpha, 'trainingHistory-' + name +'.npz'),allow_pickle=True)['arr_0'].item()\n",
    "\n",
    "   alphamodel.compile(custom_metrics=cmetrics)\n",
    "   alphamodelzero.compile(custom_metrics=cmetrics)\n",
    "   \n",
    "   valzero=alphamodelzero.evaluate(dataalpha_val_dict)\n",
    "   valtrained=alphamodel.evaluate(dataalpha_val_dict)\n",
    "   #valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   #valtrained= {key: value.numpy() for key, value in valtrained.items()}\n",
    "\n",
    "\n",
    "   metricsnames=alphamodel.metrics_names\n",
    "\n",
    "   valzero = {metricsnames[i]: valzero[i] for i in range(len(valzero))}\n",
    "   valtrained= {metricsnames[i]: valtrained[i] for i in range(len(valtrained))}\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for trained network: \")\n",
    "   print(valtrained)\n",
    "   print(\"ratio of trained to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valtrained.items()}))\n",
    "\n",
    "\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(alphamodel,tf.cast(dataalpha[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   meanfailuretosolveequation,_,_=HYM_measure_val(alphamodel,dataalpha)\n",
    "   print(\"mean of difference/mean of absolute value of source, weighted by sqrt(g): \" + str(meanfailuretosolveequation))\n",
    "   print(\"\\n\\n\")\n",
    "   return alphamodel,training_historyAlpha\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb10fbce",
   "metadata": {},
   "source": [
    "Now generate example points with a point generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a2a6fe4",
   "metadata": {},
   "source": [
    "Geneate the point cloud for our NN training - note that this will take a few mins\n",
    "\n",
    "\n",
    "Note that \"free_coefficient\" is just a label for this particular quintic - for the TQ it was psi. Here, it just lets you have different runs not overwrite each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a21f4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirname: dataAlphaP/Quintic_pg_with_2.342343234\n",
      "loading prexisting dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nPoints=100000\n",
    "\n",
    "free_coefficient = 1.9#float(sys.argv[1])\n",
    "free_coefficient=2.342351\n",
    "free_coefficient=2.342343234\n",
    "#free_coefficient=1.# when the coefficient is 1, ensure that it's 1., not 1 for the sake of the filename\n",
    "#nEpochsPhi=100\n",
    "nEpochsPhi=10\n",
    "\n",
    "depthPhi=3\n",
    "widthPhi=64#128 4 in the 1.0s\n",
    "\n",
    "\n",
    "train_phi=False\n",
    "generate_points_and_save_using_defaults(free_coefficient,nPoints)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01658c4c",
   "metadata": {},
   "source": [
    "## Training the NN\n",
    "\n",
    "Now we can start preperation for training the NN\n",
    "\n",
    "Begin by loading in the required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd5dbc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataAlphaP/Quintic_pg_with_2.342343234\n",
      "phimodel_for_10_64_50000s3x64\n",
      "network shape: [25, 64, 64, 64, 1]\n",
      "313/313 [==============================] - 4s 3ms/step - loss: 0.5024 - sigma_loss: 0.5024 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - volk_loss: 2.8117e-07\n",
      "313/313 [==============================] - 4s 3ms/step - loss: 0.5453 - sigma_loss: 0.0067 - kaehler_loss: 0.0000e+00 - transition_loss: 8.1156e-10 - volk_loss: 0.2693\n",
      "zero network validation loss: \n",
      "{'loss': 0.5023784041404724, 'sigma_loss': 0.502377986907959, 'kaehler_loss': 0.0, 'transition_loss': 0.0, 'volk_loss': 2.811732144891721e-07}\n",
      "validation loss for final network: \n",
      "{'loss': 0.5453054904937744, 'sigma_loss': 0.00666390173137188, 'kaehler_loss': 0.0, 'transition_loss': 8.115599303692989e-10, 'volk_loss': 0.2693207859992981}\n",
      "ratio of trained to zero: {'loss ratio': 1.0854476927054013, 'sigma_loss ratio': 0.013264716552848506, 'kaehler_loss ratio': 0.0, 'transition_loss ratio': 0.08115599303692989, 'volk_loss ratio': 924950.4164446876}\n",
      "average transition discrepancy in standard deviations: tf.Tensor(2.3270998e-06, shape=(), dtype=float32)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if False:\n",
    "    phimodel1,training_history=train_and_save_nn(free_coefficient,depthPhi,widthPhi,nEpochsPhi,stddev=0.05,bSizes=[64,50000],lRate=0.001) \n",
    "else:\n",
    "    phimodel1,training_history=load_nn_phimodel(free_coefficient,depthPhi,widthPhi,nEpochsPhi,[64,50000],set_weights_to_zero=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1eb5fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirname for alpha: dataAlphaP/Quintic_pg_with_2.342343234\n",
      "dirname for alpha: dataAlphaP/QuinticAlpha_pg_with_2.342343234\n",
      "loading prexisting dataset\n"
     ]
    }
   ],
   "source": [
    "generate_points_and_save_using_defaultsAlpha(free_coefficient,phimodel1,-200.,force_generate=False,seed_set=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a058210a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: alphamodel_for_100_64_3x128\n",
      "network shape: [25, 128, 128, 128, 1]\n",
      "trying iteration of training 0\n",
      "\n",
      "Epoch  1/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 1194.3726 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 60.0688\n",
      " - Transition measure val: 4.1232e-07\n",
      "1407/1407 [==============================] - 17s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 1192.8094 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 60.0688 - transition_val: 4.1232e-07\n",
      "\n",
      "Epoch  2/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 43.2649 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 39.3161\n",
      " - Transition measure val: 5.4331e-07\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 43.2469 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 39.3161 - transition_val: 5.4331e-07\n",
      "\n",
      "Epoch  3/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 39.7710 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 38.7680\n",
      " - Transition measure val: 5.2767e-07\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 39.7810 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 38.7680 - transition_val: 5.2767e-07\n",
      "\n",
      "Epoch  4/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 38.9425 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 38.2296\n",
      " - Transition measure val: 5.2376e-07\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 38.9452 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 38.2296 - transition_val: 5.2376e-07\n",
      "\n",
      "Epoch  5/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 38.6760 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 38.1248\n",
      " - Transition measure val: 5.4407e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 38.6760 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 38.1248 - transition_val: 5.4407e-07\n",
      "\n",
      "Epoch  6/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 38.2059 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 37.1304\n",
      " - Transition measure val: 5.3329e-07\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 38.2230 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 37.1304 - transition_val: 5.3329e-07\n",
      "\n",
      "Epoch  7/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 38.1079 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 36.8252\n",
      " - Transition measure val: 5.3635e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 38.1037 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 36.8252 - transition_val: 5.3635e-07\n",
      "\n",
      "Epoch  8/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 37.9002 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 37.1113\n",
      " - Transition measure val: 5.1861e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 37.9143 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 37.1113 - transition_val: 5.1861e-07\n",
      "\n",
      "Epoch  9/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 37.6442 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 36.4164\n",
      " - Transition measure val: 5.5342e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 37.6266 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 36.4164 - transition_val: 5.5342e-07\n",
      "\n",
      "Epoch 10/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 37.5376 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 36.4851\n",
      " - Transition measure val: 5.5180e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 37.5184 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 36.4851 - transition_val: 5.5180e-07\n",
      "\n",
      "Epoch 11/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 37.4302 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 36.9850\n",
      " - Transition measure val: 5.4312e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 37.4149 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 36.9850 - transition_val: 5.4312e-07\n",
      "cutting LR, multiplying by 0.1 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.010000001>\n",
      "\n",
      "Epoch 12/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 35.5868 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5914\n",
      " - Transition measure val: 5.0907e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 35.6174 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5914 - transition_val: 5.0907e-07\n",
      "\n",
      "Epoch 13/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 35.1380 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5058\n",
      " - Transition measure val: 5.2481e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 35.1270 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5058 - transition_val: 5.2481e-07\n",
      "\n",
      "Epoch 14/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 35.0545 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.3880\n",
      " - Transition measure val: 5.3854e-07\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 35.0717 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.3880 - transition_val: 5.3854e-07\n",
      "\n",
      "Epoch 15/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.9726 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.4474\n",
      " - Transition measure val: 5.2948e-07\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 35.0310 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.4474 - transition_val: 5.2948e-07\n",
      "\n",
      "Epoch 16/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.9413 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.4761\n",
      " - Transition measure val: 5.2032e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.9628 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.4761 - transition_val: 5.2032e-07\n",
      "\n",
      "Epoch 17/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.9239 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.4593\n",
      " - Transition measure val: 5.2099e-07\n",
      "1407/1407 [==============================] - 14s 9ms/step - loss: 0.0000e+00 - laplacian_loss: 34.9077 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.4593 - transition_val: 5.2099e-07\n",
      "\n",
      "Epoch 18/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.8378 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5509\n",
      " - Transition measure val: 5.2757e-07\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.8260 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5509 - transition_val: 5.2757e-07\n",
      "\n",
      "Epoch 19/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.7944 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5710\n",
      " - Transition measure val: 5.3511e-07\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.7679 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5710 - transition_val: 5.3511e-07\n",
      "\n",
      "Epoch 20/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.7454 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.6484\n",
      " - Transition measure val: 5.2872e-07\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.7337 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.6484 - transition_val: 5.2872e-07\n",
      "\n",
      "Epoch 21/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.6672 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.6334\n",
      " - Transition measure val: 5.3444e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.7112 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.6334 - transition_val: 5.3444e-07\n",
      "cutting LR, multiplying by 0.1 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "\n",
      "Epoch 22/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.3926 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5442\n",
      " - Transition measure val: 5.3740e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.3784 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5442 - transition_val: 5.3740e-07\n",
      "\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.3459 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5079\n",
      " - Transition measure val: 5.1575e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.3459 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5079 - transition_val: 5.1575e-07\n",
      "\n",
      "Epoch 24/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.3326 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.4923\n",
      " - Transition measure val: 5.2958e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.3284 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.4923 - transition_val: 5.2958e-07\n",
      "\n",
      "Epoch 25/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.3092 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.4823\n",
      " - Transition measure val: 5.2996e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.3127 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.4823 - transition_val: 5.2996e-07\n",
      "\n",
      "Epoch 26/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.3121 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.4805\n",
      " - Transition measure val: 5.3682e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2971 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.4805 - transition_val: 5.3682e-07\n",
      "\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2853 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.4934\n",
      " - Transition measure val: 5.3425e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2853 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.4934 - transition_val: 5.3425e-07\n",
      "\n",
      "Epoch 28/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2778 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.4992\n",
      " - Transition measure val: 5.2557e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2713 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.4992 - transition_val: 5.2557e-07\n",
      "\n",
      "Epoch 29/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2865 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5074\n",
      " - Transition measure val: 5.1708e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2659 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5074 - transition_val: 5.1708e-07\n",
      "\n",
      "Epoch 30/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2712 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5123\n",
      " - Transition measure val: 5.1098e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2567 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5123 - transition_val: 5.1098e-07\n",
      "\n",
      "Epoch 31/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2643 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5145\n",
      " - Transition measure val: 5.3902e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2549 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5145 - transition_val: 5.3902e-07\n",
      "cutting LR, multiplying by 0.1 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.000100000005>\n",
      "\n",
      "Epoch 32/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2176 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5104\n",
      " - Transition measure val: 5.3139e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2123 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5104 - transition_val: 5.3139e-07\n",
      "\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2137 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5064\n",
      " - Transition measure val: 5.1813e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2137 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5064 - transition_val: 5.1813e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=9e-05>\n",
      "\n",
      "Epoch 34/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2032 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5033\n",
      " - Transition measure val: 5.3396e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2596 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5033 - transition_val: 5.3396e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=8.1e-05>\n",
      "\n",
      "Epoch 35/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2027 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5024\n",
      " - Transition measure val: 5.2881e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1982 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5024 - transition_val: 5.2881e-07\n",
      "\n",
      "Epoch 36/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2002 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5019\n",
      " - Transition measure val: 5.2319e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2125 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5019 - transition_val: 5.2319e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=7.2899995e-05>\n",
      "\n",
      "Epoch 37/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2278 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5022\n",
      " - Transition measure val: 5.2023e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1954 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5022 - transition_val: 5.2023e-07\n",
      "\n",
      "Epoch 38/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2071 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5016\n",
      " - Transition measure val: 5.3692e-07\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1954 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5016 - transition_val: 5.3692e-07\n",
      "\n",
      "Epoch 39/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1776 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5012\n",
      " - Transition measure val: 5.4293e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1938 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5012 - transition_val: 5.4293e-07\n",
      "\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1955 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5020\n",
      " - Transition measure val: 5.4741e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1955 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5020 - transition_val: 5.4741e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=6.5609995e-05>\n",
      "\n",
      "Epoch 41/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1768 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5019\n",
      " - Transition measure val: 5.2032e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1898 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5019 - transition_val: 5.2032e-07\n",
      "\n",
      "Epoch 42/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2021 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5030\n",
      " - Transition measure val: 5.3263e-07\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1904 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5030 - transition_val: 5.3263e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=5.9048994e-05>\n",
      "\n",
      "Epoch 43/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1570 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5023\n",
      " - Transition measure val: 5.2814e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2676 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5023 - transition_val: 5.2814e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=5.3144093e-05>\n",
      "\n",
      "Epoch 44/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2098 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5019\n",
      " - Transition measure val: 5.2166e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1914 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5019 - transition_val: 5.2166e-07\n",
      "\n",
      "Epoch 45/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1731 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5020\n",
      " - Transition measure val: 5.2681e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2052 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5020 - transition_val: 5.2681e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=4.7829682e-05>\n",
      "\n",
      "Epoch 46/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2112 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5025\n",
      " - Transition measure val: 5.3883e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1908 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5025 - transition_val: 5.3883e-07\n",
      "\n",
      "Epoch 47/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1446 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5025\n",
      " - Transition measure val: 5.2614e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1860 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5025 - transition_val: 5.2614e-07\n",
      "\n",
      "Epoch 48/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1956 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5025\n",
      " - Transition measure val: 5.2509e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1877 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5025 - transition_val: 5.2509e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=4.304671e-05>\n",
      "\n",
      "Epoch 49/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1866 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5027\n",
      " - Transition measure val: 5.2385e-07\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1841 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5027 - transition_val: 5.2385e-07\n",
      "\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1916 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5032\n",
      " - Transition measure val: 5.0335e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1916 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5032 - transition_val: 5.0335e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=3.874204e-05>\n",
      "\n",
      "Epoch 51/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2169 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5038\n",
      " - Transition measure val: 5.3892e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1854 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5038 - transition_val: 5.3892e-07\n",
      "\n",
      "Epoch 52/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1937 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5037\n",
      " - Transition measure val: 5.3301e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1878 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5037 - transition_val: 5.3301e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=3.4867837e-05>\n",
      "\n",
      "Epoch 53/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2134 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5037\n",
      " - Transition measure val: 5.2605e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1840 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5037 - transition_val: 5.2605e-07\n",
      "\n",
      "Epoch 54/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1828 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5041\n",
      " - Transition measure val: 5.2700e-07\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1828 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5041 - transition_val: 5.2700e-07\n",
      "\n",
      "Epoch 55/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1921 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5044\n",
      " - Transition measure val: 5.2795e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1875 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5044 - transition_val: 5.2795e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=3.1381052e-05>\n",
      "\n",
      "Epoch 56/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1936 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5042\n",
      " - Transition measure val: 5.3511e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1814 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5042 - transition_val: 5.3511e-07\n",
      "\n",
      "Epoch 57/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1806 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5047\n",
      " - Transition measure val: 5.1708e-07\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2207 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5047 - transition_val: 5.1708e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=2.8242946e-05>\n",
      "\n",
      "Epoch 58/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1758 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5042\n",
      " - Transition measure val: 5.3072e-07\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2319 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5042 - transition_val: 5.3072e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=2.541865e-05>\n",
      "\n",
      "Epoch 59/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1968 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5044\n",
      " - Transition measure val: 5.1994e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1968 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5044 - transition_val: 5.1994e-07\n",
      "\n",
      "Epoch 60/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1920 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5045\n",
      " - Transition measure val: 5.3549e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1801 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5045 - transition_val: 5.3549e-07\n",
      "\n",
      "Epoch 61/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1760 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5044\n",
      " - Transition measure val: 5.3558e-07\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2044 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5044 - transition_val: 5.3558e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=2.2876786e-05>\n",
      "\n",
      "Epoch 62/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1638 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5051\n",
      " - Transition measure val: 5.3997e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1918 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5051 - transition_val: 5.3997e-07\n",
      "\n",
      "Epoch 63/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1906 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5050\n",
      " - Transition measure val: 5.3997e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1797 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5050 - transition_val: 5.3997e-07\n",
      "\n",
      "Epoch 64/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1689 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5049\n",
      " - Transition measure val: 5.1241e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1876 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5049 - transition_val: 5.1241e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=2.0589106e-05>\n",
      "\n",
      "Epoch 65/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1700 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5050\n",
      " - Transition measure val: 5.3778e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1776 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5050 - transition_val: 5.3778e-07\n",
      "\n",
      "Epoch 66/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1830 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5048\n",
      " - Transition measure val: 5.5208e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2621 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5048 - transition_val: 5.5208e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=1.8530196e-05>\n",
      "\n",
      "Epoch 67/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1525 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5048\n",
      " - Transition measure val: 5.1966e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1766 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5048 - transition_val: 5.1966e-07\n",
      "\n",
      "Epoch 68/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1899 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5051\n",
      " - Transition measure val: 5.3139e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1760 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5051 - transition_val: 5.3139e-07\n",
      "\n",
      "Epoch 69/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1849 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5052\n",
      " - Transition measure val: 5.3883e-07\n",
      "1407/1407 [==============================] - 19s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1778 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5052 - transition_val: 5.3883e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=1.6677175e-05>\n",
      "\n",
      "Epoch 70/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1852 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5052\n",
      " - Transition measure val: 5.2929e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1893 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5052 - transition_val: 5.2929e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=1.50094575e-05>\n",
      "\n",
      "Epoch 71/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1858 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5053\n",
      " - Transition measure val: 5.3320e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1868 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5053 - transition_val: 5.3320e-07\n",
      "\n",
      "Epoch 72/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1796 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5053\n",
      " - Transition measure val: 5.0678e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1764 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5053 - transition_val: 5.0678e-07\n",
      "\n",
      "Epoch 73/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1557 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5054\n",
      " - Transition measure val: 5.2824e-07\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1781 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5054 - transition_val: 5.2824e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=1.3508511e-05>\n",
      "\n",
      "Epoch 74/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1772 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5052\n",
      " - Transition measure val: 5.3406e-07\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1973 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5052 - transition_val: 5.3406e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=1.21576595e-05>\n",
      "\n",
      "Epoch 75/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1743 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5052\n",
      " - Transition measure val: 5.2900e-07\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1743 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5052 - transition_val: 5.2900e-07\n",
      "\n",
      "Epoch 76/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1719 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5052\n",
      " - Transition measure val: 5.2128e-07\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2235 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5052 - transition_val: 5.2128e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=1.0941893e-05>\n",
      "\n",
      "Epoch 77/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1951 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5053\n",
      " - Transition measure val: 5.3177e-07\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1754 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5053 - transition_val: 5.3177e-07\n",
      "\n",
      "Epoch 78/100\n",
      "  61/1407 [>.............................] - ETA: 9s - loss: 0.0000e+00 - laplacian_loss: 30.2790 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node gradients/AddN_2 defined at (most recent call last):\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n\n  File \"/var/folders/n5/4xlf9f2j41z1r42htkz4vw0m0000gn/T/ipykernel_91769/2454447764.py\", line 7, in <module>\n\n  File \"/var/folders/n5/4xlf9f2j41z1r42htkz4vw0m0000gn/T/ipykernel_91769/2703670363.py\", line 157, in train_and_save_nn_Alpha\n\n  File \"/Users/kit/Documents/Phys_Working/PhysicalYukawas/AlphaPrimeModel.py\", line 587, in train_modelalpha\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/Users/kit/Documents/Phys_Working/PhysicalYukawas/AlphaPrimeModel.py\", line 330, in train_step\n\nInputs to operation PartitionedCall_5/gradients/AddN_2 of type AddN must have the same size and shape.  Input 0: [10,64,5] != input 1: [0]\n\t [[{{node gradients/AddN_2}}]] [Op:__inference_train_function_1724362]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m nEpochsAlpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     AlphaModel1,training_historyAlpha\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_and_save_nn_Alpha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfree_coefficient\u001b[49m\u001b[43m,\u001b[49m\u001b[43mphimodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43meuler_char\u001b[49m\u001b[43m,\u001b[49m\u001b[43malphaprime\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdepthAlpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwidthAlpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnEpochsAlpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbSizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstddev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlRate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43muse_zero_network\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mload_network\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     AlphaModel1,training_historyAlpha\u001b[38;5;241m=\u001b[39mload_nn_Alpha(free_coefficient,phimodel1,euler_char,alphaprime,depthPhi,widthPhi,nEpochsPhi,[\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m50000\u001b[39m],set_weights_to_zero\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[4], line 157\u001b[0m, in \u001b[0;36mtrain_and_save_nn_Alpha\u001b[0;34m(free_coefficient, phimodel, euler_char, alphaprime, nlayer, nHidden, nEpochs, bSizes, stddev, lRate, use_zero_network, alpha, load_network)\u001b[0m\n\u001b[1;32m    155\u001b[0m       cb_list, cmetrics \u001b[38;5;241m=\u001b[39m getcallbacksandmetricsAlpha(dataalpha)\n\u001b[1;32m    156\u001b[0m       alphamodel\u001b[38;5;241m.\u001b[39mcompile(custom_metrics\u001b[38;5;241m=\u001b[39mcmetrics)\n\u001b[0;32m--> 157\u001b[0m    alphamodel, training_historyAlpha\u001b[38;5;241m=\u001b[39m \u001b[43mtrain_modelalpha\u001b[49m\u001b[43m(\u001b[49m\u001b[43malphamodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataalpha_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnEpochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbSizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m    i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinished training\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Phys_Working/PhysicalYukawas/AlphaPrimeModel.py:587\u001b[0m, in \u001b[0;36mtrain_modelalpha\u001b[0;34m(alphaprimemodel, data_train, optimizer, epochs, batch_sizes, verbose, custom_metrics, callbacks, sw)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{:2d}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{:d}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, epochs))\n\u001b[0;32m--> 587\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43malphaprimemodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m#print(history)\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m history\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node gradients/AddN_2 defined at (most recent call last):\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n\n  File \"/var/folders/n5/4xlf9f2j41z1r42htkz4vw0m0000gn/T/ipykernel_91769/2454447764.py\", line 7, in <module>\n\n  File \"/var/folders/n5/4xlf9f2j41z1r42htkz4vw0m0000gn/T/ipykernel_91769/2703670363.py\", line 157, in train_and_save_nn_Alpha\n\n  File \"/Users/kit/Documents/Phys_Working/PhysicalYukawas/AlphaPrimeModel.py\", line 587, in train_modelalpha\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/Users/kit/Documents/Phys_Working/PhysicalYukawas/AlphaPrimeModel.py\", line 330, in train_step\n\nInputs to operation PartitionedCall_5/gradients/AddN_2 of type AddN must have the same size and shape.  Input 0: [10,64,5] != input 1: [0]\n\t [[{{node gradients/AddN_2}}]] [Op:__inference_train_function_1724362]"
     ]
    }
   ],
   "source": [
    "alphaprime=1\n",
    "euler_char=-200\n",
    "depthAlpha=3\n",
    "widthAlpha=128\n",
    "nEpochsAlpha=100\n",
    "if True:\n",
    "    AlphaModel1,training_historyAlpha=train_and_save_nn_Alpha(free_coefficient,phimodel1,euler_char,alphaprime,depthAlpha,widthAlpha,nEpochsAlpha,bSizes=[64,50000],stddev=0.05,lRate=0.1,use_zero_network=False,alpha=[1.,1.],load_network=False)\n",
    "else:\n",
    "    AlphaModel1,training_historyAlpha=load_nn_Alpha(free_coefficient,phimodel1,euler_char,alphaprime,depthPhi,widthPhi,nEpochsPhi,[64,50000],set_weights_to_zero=False)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5dfed2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sigma_val', 'volk_loss', 'transition_val', 'kaehler_val', 'kaehler_loss', 'loss', 'transition_loss', 'sigma_loss', 'volk_val', 'epochs'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0132c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with learning rate:  0.0001\n",
      "name: alphamodel_for_10_64_3x64\n",
      "network shape: [25, 64, 64, 64, 1]\n",
      "trying iteration of training 0\n",
      "\n",
      "Epoch  1/10\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 18978.3809 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 78906.5938\n",
      " - Transition measure val: 6.3086e-08\n",
      "1407/1407 [==============================] - 14s 8ms/step - loss: 0.0000e+00 - laplacian_loss: 18951.6562 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 78906.5938 - transition_val: 6.3086e-08\n",
      "\n",
      "Epoch  2/10\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 63096.2930 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 4991.8018\n",
      " - Transition measure val: 2.7872e-08\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0000e+00 - laplacian_loss: 62917.4336 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 4991.8018 - transition_val: 2.7872e-08\n",
      "\n",
      "Epoch  3/10\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 20944.3496 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 972.5402\n",
      " - Transition measure val: 1.5282e-08\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0000e+00 - laplacian_loss: 20929.5762 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 972.5402 - transition_val: 1.5282e-08\n",
      "\n",
      "Epoch  4/10\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 4136.0605 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 2239.6484\n",
      " - Transition measure val: 1.7444e-08\n",
      "1407/1407 [==============================] - 11s 7ms/step - loss: 0.0000e+00 - laplacian_loss: 5517.8691 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 2239.6484 - transition_val: 1.7444e-08\n",
      "\n",
      "Epoch  5/10\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 21254.4629 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 163449.2344\n",
      " - Transition measure val: 3.7340e-08\n",
      "1407/1407 [==============================] - 11s 7ms/step - loss: 0.0000e+00 - laplacian_loss: 21179.6641 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 163449.2344 - transition_val: 3.7340e-08\n",
      "\n",
      "Epoch  6/10\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 40529.4414 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 12359.4170\n",
      " - Transition measure val: 2.1039e-08\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0000e+00 - laplacian_loss: 40529.4414 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 12359.4170 - transition_val: 2.1039e-08\n",
      "\n",
      "Epoch  7/10\n",
      "1401/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 3732.3740 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 338.0861\n",
      " - Transition measure val: 8.8781e-09\n",
      "1407/1407 [==============================] - 11s 7ms/step - loss: 0.0000e+00 - laplacian_loss: 3717.2019 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 338.0861 - transition_val: 8.8781e-09\n",
      "\n",
      "Epoch  8/10\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 6101.1030 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 1138.8033\n",
      " - Transition measure val: 1.6047e-08\n",
      "1407/1407 [==============================] - 11s 7ms/step - loss: 0.0000e+00 - laplacian_loss: 6096.9790 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 1138.8033 - transition_val: 1.6047e-08\n",
      "\n",
      "Epoch  9/10\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 13376.0176 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 2057.8040\n",
      " - Transition measure val: 1.8965e-08\n",
      "1407/1407 [==============================] - 11s 7ms/step - loss: 0.0000e+00 - laplacian_loss: 13366.6221 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 2057.8040 - transition_val: 1.8965e-08\n",
      "\n",
      "Epoch 10/10\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 111362.5078 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 2807.0554\n",
      " - Transition measure val: 1.8771e-08\n",
      "1407/1407 [==============================] - 11s 7ms/step - loss: 0.0000e+00 - laplacian_loss: 110967.5391 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 2807.0554 - transition_val: 1.8771e-08\n",
      "finished training\n",
      "\n",
      "zero network validation loss: \n",
      "{'loss': 0.0, 'laplacian_loss': 176.30818, 'transition_loss': 0.0, 'sigma_loss': 0.0, 'kaehler_loss': 0.0, 'volk_loss': 0.0}\n",
      "validation loss for raw network: \n",
      "{'loss': 0.0, 'laplacian_loss': 176.29546, 'transition_loss': 0.0, 'sigma_loss': 0.0, 'kaehler_loss': 0.0, 'volk_loss': 0.0}\n",
      "validation loss for final network: \n",
      "{'loss': 0.0, 'laplacian_loss': 110890.72, 'transition_loss': 0.0, 'sigma_loss': 0.0, 'kaehler_loss': 0.0, 'volk_loss': 0.0}\n",
      "ratio of final to zero: {'loss ratio': 0.0, 'laplacian_loss ratio': 628.9595731465569, 'transition_loss ratio': 0.0, 'sigma_loss ratio': 0.0, 'kaehler_loss ratio': 0.0, 'volk_loss ratio': 0.0}\n",
      "ratio of final to raw: {'loss ratio': 0.0, 'laplacian_loss ratio': 629.0049743885291, 'transition_loss ratio': 0.0, 'sigma_loss ratio': 0.0, 'kaehler_loss ratio': 0.0, 'volk_loss ratio': 0.0}\n",
      "average transition discrepancy in standard deviations: tf.Tensor(1.8704974e-06, shape=(), dtype=float32)\n",
      "mean of difference/mean of absolute value of source, weighted by sqrt(g): tf.Tensor(18.071758, shape=(), dtype=float32)\n",
      "\n",
      "\n",
      "\n",
      "Training with learning rate:  0.00021544346900318845\n",
      "name: alphamodel_for_10_64_3x64\n",
      "network shape: [25, 64, 64, 64, 1]\n",
      "trying iteration of training 0\n",
      "\n",
      "Epoch  1/10\n",
      "1401/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 50377.0547 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 806.8136\n",
      " - Transition measure val: 1.3115e-08\n",
      "1407/1407 [==============================] - 14s 7ms/step - loss: 0.0000e+00 - laplacian_loss: 50171.9062 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 806.8136 - transition_val: 1.3115e-08\n",
      "\n",
      "Epoch  2/10\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 23507.0977 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 28880.7656\n",
      " - Transition measure val: 1.9322e-08\n",
      "1407/1407 [==============================] - 11s 7ms/step - loss: 0.0000e+00 - laplacian_loss: 23457.3301 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 28880.7656 - transition_val: 1.9322e-08\n",
      "\n",
      "Epoch  3/10\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 2123637.2500 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 1772.1104\n",
      " - Transition measure val: 1.7574e-08\n",
      "1407/1407 [==============================] - 11s 7ms/step - loss: 0.0000e+00 - laplacian_loss: 2117600.7500 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 1772.1104 - transition_val: 1.7574e-08\n",
      "\n",
      "Epoch  4/10\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 15510.7939 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 18543.3438\n",
      " - Transition measure val: 2.9765e-08\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0000e+00 - laplacian_loss: 15469.0625 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 18543.3438 - transition_val: 2.9765e-08\n",
      "\n",
      "Epoch  5/10\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 37819.3047 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 21472.9688\n",
      " - Transition measure val: 2.6493e-08\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0000e+00 - laplacian_loss: 37766.0391 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 21472.9688 - transition_val: 2.6493e-08\n",
      "\n",
      "Epoch  6/10\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 551714.8750 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 964.8328\n",
      " - Transition measure val: 9.3520e-09\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0000e+00 - laplacian_loss: 549755.0625 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 964.8328 - transition_val: 9.3520e-09\n",
      "\n",
      "Epoch  7/10\n",
      "1400/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 6050.3643 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 22792.5312\n",
      " - Transition measure val: 1.2467e-08\n",
      "1407/1407 [==============================] - 11s 7ms/step - loss: 0.0000e+00 - laplacian_loss: 6034.3901 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 22792.5312 - transition_val: 1.2467e-08\n",
      "\n",
      "Epoch  8/10\n",
      "  14/1407 [..............................] - ETA: 5s - loss: 0.0000e+00 - laplacian_loss: 1655.8901 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00  "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "alphaprime=1\n",
    "euler_char=-200\n",
    "depthAlpha=3\n",
    "widthAlpha=64\n",
    "nEpochsAlpha=10\n",
    "\n",
    "def train_model_with_lr(lr):\n",
    "    print(\"Training with learning rate: \", lr)\n",
    "    model, history = train_and_save_nn_Alpha(\n",
    "        free_coefficient,\n",
    "        phimodel1,\n",
    "        euler_char,\n",
    "        alphaprime,\n",
    "        depthAlpha,\n",
    "        widthAlpha,\n",
    "        nEpochsAlpha,\n",
    "        bSizes=[64, 50000],\n",
    "        stddev=0.05,\n",
    "        lRate=lr,\n",
    "        use_zero_network=False,\n",
    "        alpha=[1., 1.],\n",
    "        load_network=False\n",
    "    )\n",
    "    # Return the final validation loss or any other metric you want to optimize\n",
    "    return history['laplacian_loss'][-1]\n",
    "\n",
    "lr_space = {\n",
    "    'lr': np.logspace(-4, -1, 10)  # 20 log-spaced values between 10^-4 and 10^-1\n",
    "}\n",
    "\n",
    "best_lr = None\n",
    "best_performance = float('inf')\n",
    "\n",
    "results = []\n",
    "for params in ParameterGrid(lr_space):\n",
    "    lr = params['lr']\n",
    "    performance = train_model_with_lr(lr)\n",
    "    results.append((lr, performance))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e14f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort results by learning rate\n",
    "results.sort(key=lambda x: x[0])\n",
    "\n",
    "# Unpack the results\n",
    "learning_rates, performances = zip(*results)\n",
    "\n",
    "# Plot learning rate vs performance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(learning_rates, performances, 'bo-')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Learning Rate vs Validation Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot learning rate vs performance (scatter plot)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(learning_rates, performances)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Learning Rate vs Validation Loss (Scatter)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot performance distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(performances, bins=20)\n",
    "plt.xlabel('Validation Loss')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Validation Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the best performance\n",
    "best_lr = min(results, key=lambda x: x[1])[0]\n",
    "best_performance = min(results, key=lambda x: x[1])[1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(learning_rates, performances, 'bo-')\n",
    "plt.plot(best_lr, best_performance, 'r*', markersize=15)\n",
    "plt.annotate(f'Best: {best_lr:.2e}', (best_lr, best_performance), xytext=(5, 5),\n",
    "             textcoords='offset points')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Learning Rate vs Validation Loss (Best Highlighted)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3860bc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(sys.modules['AlphaPrimeModel'])\n",
    "from AlphaPrimeModel import *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cymetric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
